{
 "metadata": {
  "name": "",
  "signature": "sha256:079c231a4ed39e85cecd2c7e7d70bdb743db42e65ace16300ab1434e80679a72"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How to make predictions about real-world quantities.\n",
      "\n",
      "+  How does sales volume change with changes in price? How is it affected by weather?\n",
      "+  How does affect the title of a book to sales?\n",
      "+  How does the amount of a drug absorbed vary with body weight of patient? Does it depend on blood pressure?\n",
      "+  How many customers can I expect today?\n",
      "+  At what time should I go home to avoid the traffic jam?\n",
      "+  What is the chance of rain for next two Mondays? What is the expected temperature?\n",
      "\n",
      "<center><img src=\"files/images/crystal_ball_s.jpg\"></center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example: Connected Car.\n",
      "#### How to Beat the Traffic. Driving the Future of Smart Cities.\n",
      "\n",
      "Car as a sensor with information feedback.\n",
      "\n",
      "Possible Data Science Use-Cases:\n",
      "+ Predictive Car Maintenance (predict part failure, optimize replacement schedule).\n",
      "+ Leveraging Driving Behavior (differenciate insurance pricing, optimize car design).\n",
      "+ Improving GPS Systems (establish baseline for traffic congestion, create meaningful metrics for routing).\n",
      "+ Predictive Power for Assistance Systems (optimize fuel efficiency, predict start/stops/braking in the next two minutes)\n",
      "+ Traffic Light Assistance (signal timing of traffic lights, crowd sourcing of traffic lights).\n",
      "\n",
      "<small>(from http://www.slideshare.net/ihuston/driving-the-future-of-smart-cities-how-to-beat-the-traffic-pivotal-talk-at-strata-2014)</small>\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Notation\n",
      "\n",
      "$x_i$ element of a vector, $\\textbf{x}$ column vector, $\\textbf{x'}$ (transpose of $\\textbf{x}$) row vector, $X$ matrix."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### From Data to Models\n",
      "\n",
      "All these questions have a common structure: we are asking about one variable $\\textbf{y}$ (*response*) that can be expressed as a combination of one or more (independent) variables $\\textbf{x}_i$ (commonly called *covariates* or *predictors*).\n",
      "\n",
      "The role of regression is to build a model (formula) to predict the response from the covariates.\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Linear Model\n",
      "\n",
      "The simplest model we can think of is the **linear model**, where the response $\\textbf{y}$ depends linearly from the covariates $\\textbf{x}_i$:\n",
      "\n",
      "$$ \\textbf{y}  =  a_1 \\textbf{x}_1  + \\dots + a_m \\textbf{x}_{m} $$\n",
      "\n",
      "The $a_i$ are termed the *parameters* of the model or the coefficients.\n",
      "\n",
      "This equation can be rewritten in a more compact form as\n",
      "\n",
      "$$ \\textbf{y}  = X \\textbf{w}$$\n",
      "\n",
      "where $$ \\textbf{y} = \\left( \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array} \\right), \n",
      " X = \\left( \\begin{array}{c} x_{11}  \\dots x_{1m} \\\\ x_{21}  \\dots x_{2m}\\\\ \\vdots \\\\ x_{n1}  \\dots x_{nm} \\end{array} \\right),\n",
      " \\textbf{w} = \\left( \\begin{array}{c} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_m \\end{array} \\right) $$\n",
      " \n",
      " **Linear regression** is the technique for creating linear models.\n",
      " \n",
      "In the **simple** linear regression, with a single variable, we described the relationship between the predictor and the response with a straight line. \n",
      "\n",
      "The model is:\n",
      "$$ \\textbf{y}  =  a_0+ a_1 \\textbf{x}_1 $$\n",
      "\n",
      "The parameter $a_0$ is called the constant term or the *intercept*.\n",
      "\n",
      "*Example*: Does the insurance price depend on the driving experience?\n",
      "Given the following information, the monthly auto insurance prices ($\\textbf{y}$) and driving experiences in years ($\\textbf{x}_{1}$) of a set of n subjects, we can build a linear model to answer this question.\n",
      "We can also predict the monthly auto insurance price for a driver with 10 years of driving experience.\n",
      "\n",
      "<div class=\"alert alert-error\">This is apparently a very simple model, but even when the response depends in non-linear ways from the covariates, this model can still be used by considering non-linear transformations $\\phi(\\cdot)$ of the covariates: $ \\textbf{y} = a_1 \\phi(\\textbf{x}_{1}) + \\dots + a_m \\phi(\\textbf{x}_{m}) $ </div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Example of simple linear regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline \n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "X1 = np.random.randn(300, 2)  #  random floats sampled from a univariate \u201cnormal\u201d (Gaussian) distribution\n",
      "A = np.array([[0.6, .4], [.4, 0.6]])\n",
      "X2 = np.dot(X1, A)\n",
      "plt.plot(X2[:, 0], X2[:, 1], \"o\", alpha=0.3) # alpha, blending value, between 0 (transparent) and 1 (opaque)."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a linear model to explain the data\n",
      "model=[0+1*x for x in np.arange(-2,3)]\n",
      "plt.plot(X2[:, 0], X2[:, 1], \"o\", alpha=0.3);\n",
      "plt.plot(np.arange(-2,3), model,'r');\n",
      "plt.show()\n",
      "# The red line gives the predicted values of this model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Which is the best model for a set of samples?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(X2[:, 0], X2[:, 1], \"o\", alpha=0.3);\n",
      "# We can use several parameters and we do not know which is the best model\n",
      "model1=[0+1*x for x in np.arange(-2,3)]\n",
      "model2=[0.3+0.9*x for x in np.arange(-2,3)]\n",
      "model3=[0-0.1*x for x in np.arange(-2,3)]\n",
      "plt.plot(np.arange(-2,3), model1,'r');\n",
      "plt.plot(np.arange(-2,3), model2,'g');\n",
      "plt.plot(np.arange(-2,3), model3,'y');\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Linear Regression and Ordinary Least Squares\n",
      "\n",
      "$$\\textbf{y} = a_0+a_1 \\textbf{x}$$\n",
      "\n",
      "Ordinary Least Squares (OLS) is the simplest and most common **estimator** in which the two $a$'s are chosen to minimize the **square of the distance between the predicted values and the actual values**. \n",
      "\n",
      "Given the set of samples $(\\textbf{x},\\textbf{y})$, the objetive is to minimize:\n",
      "\n",
      "$$ ||a_0 + a_1 \\textbf{x} -  \\textbf{y} ||^2_2 = \\sum_{j=1}^n (a_0+a_1 x_{j} -  y_j )^2,$$ with respect to $a_0, a_1$.\n",
      "\n",
      "This expression is often called **sum of squared errors of prediction (SSE)**.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### How to compute the OLS: Scipy.optimize"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To illustrate\n",
      "zip([2,3,4,5,6],[40,50,60,70,80])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from scipy.optimize import fmin\n",
      "%matplotlib inline\n",
      "import numpy as np\n",
      "from scipy.optimize import fmin\n",
      "import matplotlib.pylab as pl\n",
      "\n",
      "x = np.array([2.2, 4.3, 5.1, 5.8, 6.4, 8.0])\n",
      "y = np.array([0.4, 10.1, 14.0, 10.9, 15.4, 18.5])\n",
      "\n",
      "ss = lambda theta, x, y: np.sum((y - theta[0] - theta[1]*x) ** 2) # define the lambda function\n",
      "\n",
      "b0,b1 = fmin(ss, [0,1], args=(x,y));   # Minimizing the sum of squares\n",
      "\n",
      "pl.plot(x, y, 'ro')\n",
      "pl.plot([0,10], [b0, b0+b1*10], alpha=0.8) # Add the regression line, colored in blue\n",
      "for xi, yi in zip(x,y):\n",
      "    pl.plot([xi]*2, [yi, b0+b1*xi], \"k:\")\n",
      "pl.xlim(2, 9); pl.ylim(0, 20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can minimize other criteria, such as the **sum of absolute differences between the predicted values and the actual values**. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sabs = lambda theta, x, y: np.sum(np.abs(y - theta[0] - theta[1]*x))\n",
      "b0,b1 = fmin(sabs, [0,1], args=(x,y))   # minimize the sum of absolute differences\n",
      "pl.plot(x, y, 'ro')\n",
      "pl.plot([0,10], [b0, b0+b1*10]) # Add the regression line, colored in blue\n",
      "pl.xlim(2, 9); pl.ylim(0, 20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OLS is a popular approach for several reasons. \n",
      "\n",
      "+ For one, it is computationally cheap to calculate the coefficients. \n",
      "+ It is also easier to interpret than more sophisticated models, and in situations where the goal is understanding a simple model in detail, rather than estimating the response well, they can provide insight into what the model captures. \n",
      "+ Finally, in situations where there is a lot of noise, it may be hard to find the true functional form, so a constrained model can perform quite well compared to a complex model which is more affected by noise.\n",
      "\n",
      "The resulting model is represented as follows:\n",
      "\n",
      "$$\\textbf{y} = \\hat{a}_0+\\hat{a}_1 \\textbf{x}$$\n",
      "\n",
      "Here the hats on the variables represent the fact that they are estimated from the data we have available."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How to compute the OLS: StatsModel\n",
      "\n",
      "The ``statsmodels`` package provides several different classes that provide different options for linear regression. Getting started with linear regression is quite straightforward with the OLS module.\n",
      "\n",
      "To start with we load the Longley dataset of US macroeconomic data from the Rdatasets website."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import statsmodels.api as sm\n",
      "\n",
      "df = pd.read_csv('http://vincentarelbundock.github.io/Rdatasets/csv/datasets/longley.csv', index_col=0)\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will use the variable Total Derived Employment ('Employed') as our response $\\textbf{y}$ and Gross National Product ('GNP') as our predictor $\\textbf{x}$.\n",
      "\n",
      "We take the single response variable $y$ and store it separately. \n",
      "\n",
      "We also add a constant term so that we fit the intercept of our linear model: $X=(\\textbf{1},\\textbf{x})$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = df.Employed  # response\n",
      "X = df.GNP  # predictor\n",
      "X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
      "print X.shape\n",
      "X.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we perform the regression of the predictor on the response, using the ``sm.OLS`` class and its initialization ``OLS(y, X)`` method. This method takes as an input two array-like objects: $X$ and $\\textbf{y}$. In general, $X$ will either be a numpy array or a pandas data frame with shape ``(n, p)`` where $n$ is the number of data points and $p$ is the number of predictors. $\\textbf{y}$ is either a one-dimensional numpy array or a pandas series of length $n$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "est = sm.OLS(y, X) # Creates an object OLS estimator"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We then need to fit the model by calling the OLS object\u2019s ``fit()`` method. Ignore the warning about the kurtosis test if it appears, we have only 16 examples in our dataset and the test of the kurtosis is valid only if there are more than 20 examples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "est = est.fit()\n",
      "est.summary()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can recover the coefficients of the fit."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "est.params"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Information in the summary:\n",
      "\n",
      "The left part of the first table provides basic information about the model fit:\n",
      "\n",
      "+ Dep. Variable:\tWhich variable is the response in the model\n",
      "+ Model:\tWhat model you are using in the fit\n",
      "+ Method:\tHow the parameters of the model were calculated\n",
      "+ No. Observations:\tThe number of observations (examples)\n",
      "+ DF Residuals:\tDegrees of freedom of the residuals. Number of observations - number of parameters\n",
      "+ DF Model:\tNumber of parameters in the model (not including the constant term if present)\n",
      "\n",
      "The right part of the first table shows the goodness of fit:\n",
      "\n",
      "+ R-squared:\tThe coefficient of determination. A statistical measure of how well the regression line approximates the real data points\n",
      "+ Adj. R-squared:\tThe above value adjusted based on the number of observations and the degrees-of-freedom of the residuals\n",
      "+ F-statistic:\tA measure how significant the fit is. The mean squared error of the model divided by the mean squared error of the residuals\n",
      "+ Prob (F-statistic):\tThe probability that you would get the above statistic, given the null hypothesis that they are unrelated\n",
      "+ Log-likelihood:\tThe log of the likelihood function.\n",
      "+ AIC:\tThe Akaike Information Criterion. Adjusts the log-likelihood based on the number of observations and the complexity of the model.\n",
      "+ BIC:\tThe Bayesian Information Criterion. Similar to the AIC, but has a higher penalty for models with more parameters.\n",
      "\n",
      "The second table reports for each of the coefficients:\n",
      "\n",
      "+ coef:\tThe estimated value of the coefficient\n",
      "+ std err:\tThe basic standard error of the estimate of the coefficient. More sophisticated errors are also available.\n",
      "+ t:\tThe t-statistic value. This is a measure of how statistically significant the coefficient is.\n",
      "+ P > |t|:\tP-value that the null-hypothesis that the coefficient = 0 is true. If it is less than the confidence level, often 0.05, it indicates that there is a statistically significant relationship between the term and the response.\n",
      "+ [95.0% Conf. Interval]:\tThe lower and upper values of the 95% confidence interval\n",
      "\n",
      "Finally, there are several statistical tests to assess the distribution of the residuals"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's make predictions!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We pick 100 points equally spaced from the min to the max\n",
      "X_prime = np.linspace(X.GNP.min(), X.GNP.max(), 100)[:, np.newaxis]\n",
      "X_prime = sm.add_constant(X_prime)  # add constant as we did before\n",
      "\n",
      "# Now we calculate the predicted values\n",
      "y_hat = est.predict(X_prime)\n",
      "\n",
      "plt.scatter(X.GNP, y, alpha=0.5)  # Plot the raw data\n",
      "plt.xlabel(\"Gross National Product\")\n",
      "plt.ylabel(\"Total Employment\")\n",
      "plt.plot(X_prime[:, 1], y_hat, 'r', alpha=0.9)  # Add the regression line, colored in red"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Seaborn visualization\n",
      "\n",
      "The ``lmplot()`` function from the Seaborn module is intended for exploring linear relationships of different forms in multidimensional datesets. Input data must be in a Pandas ``DataFrame``. To plot, provide the predictor and response variable names along with the dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import seaborn as sns\n",
      "sns.lmplot(\"Employed\", \"GNP\", df);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This plot has two main components. \n",
      "\n",
      "+ The first is a scatterplot, showing the observed datapoints. \n",
      "+ The second is a regression line, showing the estimated linear model relating the two variables. \n",
      "\n",
      "Because the regression line is only an estimate, it is plotted with a 95% confidence band to give an impression of the certainty in the model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sns.lmplot(\"Armed.Forces\", \"Unemployed\", df);\n",
      "sns.lmplot(\"GNP\", \"Population\", df, order=1);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sns.lmplot(\"GNP\", \"Population\", df, order=2); # We can change the order of the model, if the relationship follows a higher-order trend."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sns.corrplot(df);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How to compute the OLS: Scikit-learn\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scikit-learn is a library that provides a variety of both supervised and unsupervised machine learning techniques.\n",
      "\n",
      "Scikit-learn provides an object-oriented interface centered around the concept of an Estimator. \n",
      "\n",
      "The <code>Estimator.fit</code> method sets the state of the estimator based on the training data. Usually, the data is comprised of a two-dimensional numpy array $X$ of shape <code>(n_samples, n_predictors)</code> that holds the so-called feature matrix and a one-dimensional numpy array $\\textbf{y}$ that holds the responses. Some estimators allow the user to control the fitting behavior. \n",
      "\n",
      "For example, the <code>sklearn.linear_model.LinearRegression</code> estimator allows the user to specify whether or not to fit an intercept term. This is done by setting the corresponding constructor arguments of the estimator object:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.linear_model import LinearRegression\n",
      "est = LinearRegression(fit_intercept=True) # Call the constructor of the estimator LinearRegression"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LinearRegression?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "During the fitting process, the state of the estimator is stored in instance attributes that have a trailing underscore (``'_'``). For example, the coefficients of a ``LinearRegression`` estimator are stored in the attribute ``coef_``:\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "clf = linear_model.LinearRegression() \n",
      "clf.fit ([[0, 1], [1, 1], [2, 1]], [0, 1, 2])  # Perform the fitting\n",
      "clf.coef_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Predict and Transformer\n",
      "Estimators that can generate predictions provide a ``Estimator.predict`` method. \n",
      "In the case of regression, ``Estimator.predict`` will return the predicted regression values. \n",
      "\n",
      "Moreover, there is a special type of ``Estimator`` called ``Transformer`` which transforms the input data -- e.g. selects a subset of the features or extracts new features based on the original ones.\n",
      "\n",
      "One transformer that we will use in this posting is ``sklearn.preprocessing.StandardScaler``. This transformer centers each predictor in ``X`` to have zero mean and unit variance:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import StandardScaler\n",
      "scaler = StandardScaler(copy=True)  # always copy input data (don't modify in-place)\n",
      "# X was equal to df.GNP  \n",
      "print 'X mean:', X.mean()\n",
      "X_centered = scaler.fit(X).transform(X)\n",
      "print 'Scaler mean:',scaler.mean_  # mean that will be subtracted upon transform\n",
      "print 'X_centered mean:', X_centered.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example Diabetes\n",
      "\n",
      "The diabetes dataset consists of 10 physiological variables (age, sex, weight, blood pressure) measure on 442 patients, and an indication of disease progression after one year:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "diabetes = datasets.load_diabetes()\n",
      "X,y = diabetes.data, diabetes.target\n",
      "print X.shape, y.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.utils import shuffle\n",
      "X,y = shuffle(X,y,random_state=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's use column 2 to perform a regression:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X2 = X[:,2:3]\n",
      "X2.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will split the data into training set and test set:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_size = 250\n",
      "X_train = X2[:train_size]\n",
      "X_test = X2[train_size:]\n",
      "y_train = y[:train_size]\n",
      "y_test = y[train_size:]\n",
      "print X_train.shape, X_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Data visualization:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(X_train, y_train, color='red', alpha=0.5)\n",
      "plt.scatter(X_test, y_test, color='blue', alpha=0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "regr1 = LinearRegression()\n",
      "regr1.fit(X_train, y_train) \n",
      "print regr1.coef_, regr1.intercept_\n",
      "print 'Score:', regr1.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "``Score`` returns the **coefficient of determination $R^2$ of the prediction.**\n",
      "\n",
      "The coefficient $R^2$ is defined as $(1 - u/v)$, where $u$ is the residual sum of squares $\\sum (y - \\hat{y})^2$ and $v$ is the regression sum of squares $\\sum (y - \\bar{y})^2$, where $\\bar{y}$ is the mean.\n",
      "\n",
      "Best possible score is 1.0, lower values are worse.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can calculate the Mean Squared Error on the test set:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Training MSE: ', np.mean((regr1.predict(X_train) - y_train)**2)\n",
      "print 'Test MSE: ', np.mean((regr1.predict(X_test) - y_test)**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualization:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(X_train, y_train, color='black')\n",
      "plt.plot(X_train, regr1.predict(X_train), color='red')\n",
      "plt.xlabel('Data')\n",
      "plt.ylabel('Target')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(X_test, y_test, color='red')\n",
      "plt.plot(X_test, regr1.predict(X_test), color='blue')\n",
      "plt.xlabel('Data')\n",
      "plt.ylabel('Target')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Using all variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "from sklearn.utils import shuffle\n",
      "\n",
      "diabetes = datasets.load_diabetes()\n",
      "X,y = diabetes.data, diabetes.target\n",
      "print X.shape, y.shape\n",
      "X,y = shuffle(X,y,random_state=1)\n",
      "\n",
      "train_size = 250\n",
      "X_train = X[:train_size]\n",
      "X_test = X[train_size:]\n",
      "y_train = y[:train_size]\n",
      "y_test = y[train_size:]\n",
      "print X_train.shape, y_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "regr2 = LinearRegression()\n",
      "regr2.fit(X_train, y_train) \n",
      "print regr2.coef_, regr2.intercept_\n",
      "print 'Score:', regr2.score(X_test, y_test) # Best possible score is 1.0, lower values are worse."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Training MSE: ', np.mean((regr2.predict(X_train) - y_train)**2)\n",
      "print 'Test MSE: ', np.mean((regr2.predict(X_test) - y_test)**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Understanding Multiple Regression (StatsModel)\n",
      "In Ordinary Least Squares Regression with a single variable we described the relationship between the predictor and the response with a straight line. This case is called *simple* linear regression. In the case of *multiple* linear regression we extend this idea by fitting a p-dimensional hyperplane to our p predictors.\n",
      "\n",
      "$$ \\textbf{y} = a_1 \\textbf{x}_1 + \\dots + a_p \\textbf{x}_p = X \\textbf{w} $$\n",
      "\n",
      "We can show this for two predictor variables in a three dimensional plot. \n",
      "\n",
      "####Example: \n",
      "In the following example we will use the *advertising dataset* which consists of the sales of products and their advertising budget in three different media TV, radio, newspaper."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "df_adv = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\n",
      "X = df_adv[['TV', 'Radio']]\n",
      "y = df_adv['Sales']\n",
      "df_adv.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "## fit a OLS model with intercept on TV and Radio\n",
      "X = sm.add_constant(X)\n",
      "est = sm.OLS(y, X).fit()\n",
      "\n",
      "## Create the 3d plot \n",
      "xx1, xx2 = np.meshgrid(np.linspace(X.TV.min(), X.TV.max(), 100), \n",
      "                       np.linspace(X.Radio.min(), X.Radio.max(), 100))\n",
      "# plot the hyperplane by evaluating the parameters on the grid\n",
      "Z = est.params[0] + est.params[1] * xx1 + est.params[2] * xx2\n",
      "\n",
      "# create matplotlib 3d axes\n",
      "fig = plt.figure(figsize=(12, 8))\n",
      "ax = Axes3D(fig, azim=-115, elev=15) # azim, stores the azimuth angle in the x,y plane; elev, stores the elevation angle in the z plane\u00a0\n",
      "\n",
      "# plot hyperplane\n",
      "surf = ax.plot_surface(xx1, xx2, Z, cmap=plt.cm.RdBu_r, alpha=0.6, linewidth=0) # cmap, a colormap for the surface patches.\n",
      "# linewidth=0, allows not drawing lines on the surface\n",
      "\n",
      "# plot data points - points over the HP are white, points below are black\n",
      "resid = y - est.predict(X)\n",
      "ax.scatter(X[resid >= 0].TV, X[resid >= 0].Radio, y[resid >= 0], color='black', alpha=1.0, facecolor='white')\n",
      "ax.scatter(X[resid < 0].TV, X[resid < 0].Radio, y[resid < 0], color='black', alpha=1.0)\n",
      "\n",
      "# set axis labels\n",
      "ax.set_xlabel('TV')\n",
      "ax.set_ylabel('Radio')\n",
      "ax.set_zlabel('Sales')\n",
      "plt.figure()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exploring multiple variables: Scatter plot\n",
      "\n",
      "The scatter plot is a grid of plots of multiple varaibles one against the other, showing the relationship of each variable to the others."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = pd.scatter_matrix(df_adv, figsize=(10.0,10.0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sparse models (Scikit-learn)\n",
      "\n",
      "To improve the conditioning of the problem (uninformative variables, mitigate the curse of dimensionality, as a feature selection preprocessing, etc.), sometimes it is interesting to select only the informative features and set non-informative ones to 0. This penalization approach, called **Lasso**, can set some coefficients to zero. \n",
      "\n",
      "Such methods are called sparse methods, and sparsity can be seen as an application of Occam\u2019s razor: prefer simpler models to complex ones."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given the set of samples $(X,\\textbf{y})$, the objetive is to minimize the SSE with a restriction:\n",
      "\n",
      "\n",
      "$$ \\textrm{$argmin$}_{\\textbf{w}} \\left( \\frac{1}{2n}  || X \\textbf{w} -  \\textbf{y} ||^2_2 + \\alpha || \\textbf{w}||_1 \\right)$$\n",
      "\n",
      "where $||\\textbf{w}||_1$ is the $\\ell_1$-norm of the parameter vector."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Geometric interpretation of regularization\n",
      "\n",
      "The left panel shows L2 regularization (ridge regularization) and the right panel L1 regularization (LASSO regression). The ellipses indicate the distribution for no regularization. The blue lines show the constraints due to regularization (limiting $\\theta^2$ for ridge regression and $|\\theta|$ for LASSO regression). The corners of the L1 regularization create more opportunities for the solution to have zeros for some of the weights.\n",
      "\n",
      "\n",
      "<center><img src=\"files/images/regularization_ridge_lasso.png\"></center>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Continue the example of Diabetes dataset\n",
      "# Create a Lasso regressor\n",
      "regr3 = linear_model.Lasso(alpha=.3)\n",
      "regr3.fit(X_train, y_train) \n",
      "print regr3.coef_ # very sparse coefficients\n",
      "print 'Score:', regr3.score(X_test, y_test) # Best possible score is 1.0, lower values are worse\n",
      "# The coeficients now are sparse\n",
      "# But, the score is almost the same"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Training MSE: ', np.mean((regr3.predict(X_train) - y_train)**2)\n",
      "print 'Test MSE: ', np.mean((regr3.predict(X_test) - y_test)**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example: Regression of Housing Data with multiple variables.\n",
      "\n",
      "We will use the simple Boston house prices set, available in scikit-learn. This records measurements of 13 attributes of housing markets around Boston, as well as the median price. The question is: can you predict the price of a new market given its attributes?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_boston\n",
      "data = load_boston()\n",
      "print data.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print data.data.shape\n",
      "print data.target.shape\n",
      "print data.feature_names\n",
      "print np.max(data.target), np.min(data.target), np.mean(data.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print data.DESCR"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import matplotlib.pyplot as plt\n",
      "fig = plt.figure(figsize=(6, 4))\n",
      "plt.hist(data.target) # plot the histogram of the prices\n",
      "plt.xlabel('price ($1000s)')\n",
      "plt.ylabel('count')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's make predictions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LinearRegression\n",
      "clf = LinearRegression()\n",
      "clf.fit(data.data, data.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.coef_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted = clf.predict(data.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Visualization of target and predicted responses\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "plt.scatter(data.target, predicted, alpha=0.3)\n",
      "plt.plot([0, 50], [0, 50], '--k')\n",
      "plt.axis('tight')\n",
      "plt.xlabel('True price ($1000s)')\n",
      "plt.ylabel('Predicted price ($1000s)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Model evaluation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.score(data.data, data.target)  # Best possible score is 1.0, lower values are worse."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Separate train and test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=33)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can select the most important features with sklearn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_selection import *\n",
      "fs=SelectKBest(score_func=f_regression,k=5)\n",
      "X_new=fs.fit_transform(X_train,y_train)\n",
      "print X_train.shape\n",
      "print X_new.shape\n",
      "print zip(fs.get_support(),data.feature_names) # Show the selected features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Data normalization:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import StandardScaler\n",
      "# Create the transformer StandardScaler for data and target\n",
      "scalerX = StandardScaler().fit(X_train)\n",
      "scalery = StandardScaler().fit(y_train)\n",
      "\n",
      "print np.max(y_train), np.min(y_train), np.mean(y_train), \n",
      "\n",
      "# Normalization of train and test data using mean and variance of the training\n",
      "X_train = scalerX.transform(X_train)\n",
      "y_train = scalery.transform(y_train)\n",
      "X_test = scalerX.transform(X_test)\n",
      "y_test = scalery.transform(y_test)\n",
      "\n",
      "print np.max(y_train), np.min(y_train), np.mean(y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Definition of a training & testing function for cross-validation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import *\n",
      "def train_and_evaluate(clf, X_train, y_train):\n",
      "    \n",
      "    clf.fit(X_train, y_train)\n",
      "    \n",
      "    print \"Coefficient of determination on training set:\",clf.score(X_train, y_train)\n",
      "    \n",
      "    # create a k-fold cross-validation iterator of k=5 folds\n",
      "    cv = KFold(X_train.shape[0], 5, shuffle=True, random_state=33)\n",
      "    scores = cross_val_score(clf, X_train, y_train, cv=cv)\n",
      "    print \"Average coefficient of determination using 5-fold crossvalidation:\",np.mean(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fitting a linear SGD regressor with squared loss:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "clf_sgd = linear_model.SGDRegressor(loss='squared_loss', penalty=None, random_state=42)\n",
      "train_and_evaluate(clf_sgd,X_train,y_train)\n",
      "print clf_sgd.coef_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fitting a linear SGD regressor with squared loss and $\\ell_2$ penalty:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_sgd1 = linear_model.SGDRegressor(loss='squared_loss', penalty='l2',  random_state=42)\n",
      "train_and_evaluate(clf_sgd1,X_train,y_train)\n",
      "print clf_sgd1.coef_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Metrics of the sklearn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "def measure_performance(X,y,clf, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True, show_r2_score=False):\n",
      "    y_pred=clf.predict(X)   \n",
      "    if show_accuracy:\n",
      "        print \"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y,y_pred)),\"\\n\"\n",
      "\n",
      "    if show_classification_report:\n",
      "        print \"Classification report\"\n",
      "        print metrics.classification_report(y,y_pred),\"\\n\"\n",
      "        \n",
      "    if show_confusion_matrix:\n",
      "        print \"Confusion matrix\"\n",
      "        print metrics.confusion_matrix(y,y_pred),\"\\n\"\n",
      "        \n",
      "    if show_r2_score:\n",
      "        print \"Coefficient of determination:{0:.3f}\".format(metrics.r2_score(y,y_pred)),\"\\n\"\n",
      "        \n",
      "# Evaluate the test set:        \n",
      "measure_performance(X_test,y_test,clf_sgd1, show_accuracy=False, \\\n",
      "                    show_classification_report=False,show_confusion_matrix=False, show_r2_score=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Logistic Regresion (Scikit-learn)\n",
      "\n",
      "**Logistic regression** or logit regression is a type of probabilistic statistical classification model. It is also used to predict a binary response from a binary predictor, used for predicting the outcome of a categorical dependent variable (i.e., a class label) based on one or more predictor variables (features). \n",
      "\n",
      "(Source: *Wikipedia*)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The logistic function is:\n",
      "\n",
      "$$ f(x) = \\frac{1}{1+e^{- \\lambda x}}$$\n",
      "\n",
      "The logistic function is useful because it can take an input with any value from negative infinity to positive infinity, whereas the output\u00a0\u00a0is confined to values between 0 and 1 and hence is interpretable as a probability."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logist(x,l):\n",
      "    return 1/(1+np.exp(-l*x))\n",
      "\n",
      "x = np.linspace(-10,10) # 50 points equally spaced from -10 to 10\n",
      "t = logist(x,0.5)\n",
      "y = logist(x,1)\n",
      "z = logist(x,3)\n",
      "plt.plot(x,t, label='lambda=0.5')\n",
      "plt.plot(x,y, label='lambda=1')\n",
      "plt.plot(x,z, label='lambda=3')\n",
      "plt.legend(loc='upper left')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Code source: Gael Varoquaux\n",
      "# License: BSD 3 clause\n",
      "import numpy as np\n",
      "import pylab as pl\n",
      "from sklearn import linear_model\n",
      "\n",
      "# this is our test set, it's just a straight line with some gaussian noise\n",
      "xmin, xmax = -5, 5\n",
      "n_samples = 100\n",
      "np.random.seed(0)\n",
      "X = np.random.normal(size=n_samples) # Creates 100 random numbers from a normal (Gaussian) distribution.\n",
      "y = (X > 0).astype(np.float) # astype transforms the True Fals to 1 and 0.\n",
      "X[X > 0] *= 4 \n",
      "X += .3 * np.random.normal(size=n_samples)\n",
      "X = X[:, np.newaxis] \n",
      "\n",
      "# run the classifier\n",
      "clf = linear_model.LogisticRegression(C=1e5)\n",
      "clf.fit(X, y)\n",
      "\n",
      "# Plot the result\n",
      "pl.figure(1, figsize=(8, 6))\n",
      "pl.clf() # Clear the current figure.\n",
      "pl.scatter(X, y, color='black', zorder=20)\n",
      "\n",
      "X_test = np.linspace(-5, 10, 300)\n",
      "\n",
      "def model(x):\n",
      "    return 1 / (1 + np.exp(-x))\n",
      "loss = model(X_test * clf.coef_ + clf.intercept_).ravel() # in column array\n",
      "pl.plot(X_test, loss, color='blue', linewidth=3, label='loss')\n",
      "\n",
      "# run a linear regression\n",
      "ols = linear_model.LinearRegression()\n",
      "ols.fit(X, y)\n",
      "\n",
      "# and plot the result\n",
      "pl.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=1, label='linear model')\n",
      "\n",
      "pl.axhline(0.5, color='0.5') # Plot horizontal axis in 0.5\n",
      "\n",
      "plt.legend(loc='upper left')\n",
      "pl.ylabel('y')\n",
      "pl.xlabel('X')\n",
      "\n",
      "pl.ylim(-.25, 1.25)\n",
      "pl.xlim(-4, 10)\n",
      "\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise**: Can we predict a win or loss in a soccer game if we are given the score of a team?\n",
      "\n",
      "+ Read this file in a pandas DataFrame: http://www.football-data.co.uk/mmz4281/1213/SP1.csv\n",
      "+ Select this columns in a new DataFrame: 'HomeTeam','AwayTeam', 'FTHG', 'FTAG', 'FTR'. (FTHG: Home team goals, FTAG: Away team goals, FTR: H=Home Win, D=Draw, A=Away Win)\n",
      "+ Visualize a scatter plot of FTHG versus FTAG.\n",
      "+ Built a $X$ 1-d predictor with all scores and a $y$ binary variable indicating win or loss.\n",
      "+ Compute and visualize a logistic regression. \n",
      "+ Which is the cut value?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your solution here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Polynomial regression (StatsModels)\n",
      "Despite its name, linear regression can be used to fit non-linear functions. A linear regression model is linear in the model parameters, not necessarily in the predictors. If you add non-linear transformations of your predictors to the linear regression model, the model will be non-linear in the predictors.\n",
      "\n",
      "$$ \\textbf{y} = a_1 \\phi(\\textbf{x}_1) + \\dots + a_m \\phi(\\textbf{x}_m) $$\n",
      "\n",
      "A very popular non-linear regression technique is *Polynomial Regression*, a technique which models the relationship between the response and the predictors as an n-th order polynomial. The higher the order of the polynomial the more \"wigglier\" functions you can fit. \n",
      "\n",
      "Using higher order polynomial comes at a price: **computational complexity** and **overfitting**. Overfitting refers to a situation in which the model fits the idiosyncrasies of the training data and loses the ability to generalize from the seen to predict the unseen."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To illustrate polynomial regression we will consider the Boston housing dataset. We'll look into the task to predict median house values in the Boston area using the predictor ``lstat``, defined as the \"proportion of the adults without some high school education and proportion of male workes classified as laborers\" (see *Hedonic House Prices and the Demand for Clean Air, Harrison & Rubinfeld, 1978*).\n",
      "\n",
      "We can clearly see that the relationship between ``medv`` and ``lstat`` is non-linear: the straight line is a poor fit; a better fit can be obtained by including higher order terms.\n",
      "\n",
      "We can represent a curved relationship between our variables by introducing **polynomial** terms. For example, a cubic model:\n",
      "\n",
      "\n",
      "$$y_i \\approx a_0 + a_1 x_i + a_2 x_i^2 + a_3 x_i^3$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example: Boston housing dataset\n",
      "# load the Boston housing dataset - median house values in the Boston area\n",
      "df = pd.read_csv('http://vincentarelbundock.github.io/Rdatasets/csv/MASS/Boston.csv')\n",
      "\n",
      "# plot lstat (% lower status of the population) against median value\n",
      "plt.figure(figsize=(6 * 1.618, 6))\n",
      "plt.scatter(df.lstat, df.medv, s=10, alpha=0.3)\n",
      "plt.xlabel('lstat')\n",
      "plt.ylabel('medv')\n",
      "\n",
      "# points linearlyd space on lstats\n",
      "x = pd.DataFrame({'lstat': np.linspace(df.lstat.min(), df.lstat.max(), 100)})\n",
      "\n",
      "# 1-st order polynomial\n",
      "poly_1 = smf.ols(formula='medv ~ 1 + lstat', data=df).fit()\n",
      "plt.plot(x.lstat, poly_1.predict(x), 'b-', label='Poly n=1 $R^2$=%.2f' % poly_1.rsquared, alpha=0.9)\n",
      "\n",
      "# 2-nd order polynomial\n",
      "poly_2 = smf.ols(formula='medv ~ 1 + lstat + I(lstat ** 2.0)', data=df).fit()\n",
      "plt.plot(x.lstat, poly_2.predict(x), 'g-', label='Poly n=2 $R^2$=%.2f' % poly_2.rsquared, alpha=0.9)\n",
      "\n",
      "# 3-rd order polynomial\n",
      "poly_3 = smf.ols(formula='medv ~ 1 + lstat + I(lstat ** 2.0) + I(lstat ** 3.0)', data=df).fit()\n",
      "plt.plot(x.lstat, poly_3.predict(x), 'r-', alpha=0.9,label='Poly n=3 $R^2$=%.2f' % poly_3.rsquared)\n",
      "\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the legend of the above figure, the $R^2$ value for each of the fits is given. The fact that the $R^2$ value is higher for the quadratic model shows that it fits the model better than the Ordinary Least Squares model. These $R^2$ values have a major flaw, however, in that they rely exclusively on the same data that was used to train the model. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise: Climate Change and Sea Ice Extent\n",
      "\n",
      "> Sea ice area refers to the total area covered by ice, whereas sea ice extent is the area of ocean with at least 15% sea ice, while the volume is the total amount of ice in the Arctic.\n",
      "\n",
      "> Reliable measurement of sea ice edges began with the satellite era in the late 1970s. Before this time, sea ice area and extent were monitored less precisely by a combination of ships, buoys and aircraft. The data show a long-term negative trend in recent years, attributed to global warming, although there is also a considerable amount of variation from year to year. Some of this variation may be related to effects such as the arctic oscillation, which may itself be related to global warming and some of the variation is essentially random \"weather noise\".\n",
      "\n",
      "> *Source: Wikipedia*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to check if there is an anomaly in the evolution of sea ice extent during the last years, we can perform the following processing steps:\n",
      "\n",
      "+ We must read and clean the data from <code>'files/SeaIce.txt'</code>. This is a ``Tab`` separated file. \n",
      "    + Year:\t4-digit year\n",
      "    + mo:\t1- or 2-digit month\n",
      "    + data_type:\tInput data set (Goddard/NRTSI-G)\n",
      "    + region:\tHemisphere that this data covers (N: Northern; S: Southern)\n",
      "    + extent:\tSea ice extent in millions of square km\n",
      "    + area:\tSea ice area in millions of square km\n",
      "\n",
      "+ To compute the anomaly for a given interval of time (month), we can compute the mean for that interval of time (using the period 1981 through 2010 for the mean extend) and we subtract it from the mean extend for that interval. This value must be converted to percent difference by dividing it by the 1981-2010 average and then multiplying by 100.\n",
      "\n",
      "+ This values can be plotted for the entire time series. \n",
      "+ We can also compute the trend as a simple linear regression."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your solution here\n",
      "import pandas as pd\n",
      "ice = pd.read_csv('files/SeaIce.txt',delim_whitespace=True)\n",
      "ice.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Notebook Visualization \n",
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style>\r\n",
        "    @font-face {\r\n",
        "        font-family: \"Computer Modern\";\r\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\r\n",
        "    }\r\n",
        "    @font-face {\r\n",
        "        font-family: \"Computer Modern\";\r\n",
        "        font-weight: bold;\r\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\r\n",
        "    }\r\n",
        "    @font-face {\r\n",
        "        font-family: \"Computer Modern\";\r\n",
        "        font-style: oblique;\r\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\r\n",
        "    }\r\n",
        "    @font-face {\r\n",
        "        font-family: \"Computer Modern\";\r\n",
        "        font-weight: bold;\r\n",
        "        font-style: oblique;\r\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\r\n",
        "    }\r\n",
        "    div.cell{\r\n",
        "        width:800px;\r\n",
        "        margin-left:16% !important;\r\n",
        "        margin-right:auto;\r\n",
        "    }\r\n",
        "    h1 {\r\n",
        "        font-family: Helvetica, serif;\r\n",
        "    }\r\n",
        "    h4{\r\n",
        "        margin-top:12px;\r\n",
        "        margin-bottom: 3px;\r\n",
        "       }\r\n",
        "    div.text_cell_render{\r\n",
        "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\r\n",
        "        line-height: 145%;\r\n",
        "        font-size: 130%;\r\n",
        "        width:800px;\r\n",
        "        margin-left:auto;\r\n",
        "        margin-right:auto;\r\n",
        "    }\r\n",
        "    .CodeMirror{\r\n",
        "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\r\n",
        "    }\r\n",
        "    .prompt{\r\n",
        "        display: None;\r\n",
        "    }\r\n",
        "    .text_cell_render h5 {\r\n",
        "        font-weight: 300;\r\n",
        "        font-size: 22pt;\r\n",
        "        color: #4057A1;\r\n",
        "        font-style: italic;\r\n",
        "        margin-bottom: .5em;\r\n",
        "        margin-top: 0.5em;\r\n",
        "        display: block;\r\n",
        "    }\r\n",
        "    \r\n",
        "    .warning{\r\n",
        "        color: rgb( 240, 20, 20 )\r\n",
        "        }  \r\n",
        "</style>\r\n",
        "<script>\r\n",
        "    MathJax.Hub.Config({\r\n",
        "                        TeX: {\r\n",
        "                           extensions: [\"AMSmath.js\"]\r\n",
        "                           },\r\n",
        "                tex2jax: {\r\n",
        "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\r\n",
        "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\r\n",
        "                },\r\n",
        "                displayAlign: 'center', // Change this to 'center' to center equations.\r\n",
        "                \"HTML-CSS\": {\r\n",
        "                    styles: {'.MathJax_Display': {\"margin\": 4}}\r\n",
        "                }\r\n",
        "        });\r\n",
        "</script>\r\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.HTML at 0x104450810>"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}