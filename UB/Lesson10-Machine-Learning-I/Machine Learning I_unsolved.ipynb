{
 "metadata": {
  "name": "",
  "signature": "sha256:58a54fc6336604032637cee67fabceb0453a0177dde1ad6a7e7aee2c0bef1c98"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>May 2014 - This notebook was created by [Oriol Pujol Vila](http://www.maia.ub.es/~oriol). Source and license info are in the folder.</i></small>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#OFFLINE MATHJAX INSTALLATION\n",
      "from IPython.external import mathjax;\n",
      "mathjax.install_mathjax()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Machine Learning I: Introduction to Supervised Classification Methods\n",
      "\n",
      "CONCEPTS\n",
      "\n",
      "##PART 1: A practical approach to machine learning\n",
      "\n",
      "1. About the software.\n",
      "\n",
      "2. What is Machine Learning?\n",
      "\n",
      "3. Modeling the machine learning problem\n",
      "\n",
      "4. The supervised classification problem. A basic guided programatic example\n",
      "\n",
      "    4.1 Representing the problem in sklearn\n",
      "    \n",
      "    4.2 Learning and predicting\n",
      "    \n",
      "    4.3 More about the feature space\n",
      "    \n",
      "    4.4 Training and testing\n",
      "    \n",
      "    4.5 Model selection (I)\n",
      "\n",
      "##PART 2: Learning concepts and theory\n",
      "\n",
      "5. What is learning?\n",
      "\n",
      "    * PAC-learning\n",
      "      \n",
      "6. Inside the learning model\n",
      "\n",
      "    * The human machine learning algorithm\n",
      "    * Model class and hypothesis space\n",
      "    * Objective function\n",
      "    * Searching/Optimization/Learning algorithm\n",
      "    \n",
      "7. Learning curves and overfitting\n",
      "\n",
      "    * Learning curves\n",
      "    * Overfitting\n",
      "        \n",
      "8. Cures to overfitting\n",
      "\n",
      "    * Model selection (II)\n",
      "    * Regularization\n",
      "    * Ensemble\n",
      "\n",
      "##PART 3: First models\n",
      "\n",
      "9. Generative and discriminative models\n",
      "    1.\tBayesian models (Naive Bayes) and some applications.\n",
      "    2.\tSupport Vector Machines."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#PART 1: A practical introduction to Machine Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1. About the software: Scikit-Learn\n",
      "\n",
      "+ Scikit-Learn is a Machine learning library writen in Python.\n",
      "+ Simple and efficient, for both experts and non-experts.\n",
      "+ Classical, well-established machine learning algorithms.\n",
      "+ BSD 3 license."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.1 Integration in the scientific Python ecosystem\n",
      "\n",
      "The open source Python ecosystem provides a standalone, versatile and powerful scientific working environment, including:\n",
      "\n",
      "+ NumPy (for efficient manipulation of multi-dimensional arrays);\n",
      "+ SciPy (for specialized data structures (e.g., sparse matrices) and lower-level scientific algorithms),\n",
      "+ IPython (for interactive exploration),\n",
      "+ Matplotlib (for vizualization)\n",
      "+ Pandas (for data management and data analysis)\n",
      "+ (and many others...)\n",
      "\n",
      "Scikit-Learn builds upon NumPy and SciPy and complements this scientific environment with machine learning algorithms; By design, Scikit-Learn is non-intrusive, easy to use and easy to combine with other libraries. We will use Scikit-Learn as a tool for understanding machine learning."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2. What is Machine Learning?\n",
      "\n",
      "**Machine Learning** (ML) is about coding programs that automatically adjust their performance from exposure to information encoded in data. This learning is achieved via a parameterized model with tunable parameters automatically adjusted according to a performance criteria.\n",
      "\n",
      "Machine Learning can be considered a subfield of Artificial Intelligence (AI).\n",
      "\n",
      "There are three major classes of ML:\n",
      "\n",
      "   1. Supervised learning : Algorithms which learn from a training set of labeled examples (exemplars) to generalize to the set of all possible inputs. Examples of techniques in supervised learning include regression and support vector machines.\n",
      "    \n",
      "   2. Unsupervised learning : Algorithms which learn from a training set of unlableled examples, using the features of the inputs to categorize inputs together according to some statistical criteria. Examples of unsupervised learning include k-means clustering and kernel density estimation.\n",
      "    \n",
      "   3. Reinforcement learning : Algorithms that learn via reinforcement from a critic that provides information on the quality of a solution, but not on how to improve it. Improved solutions are achieved by iteratively exploring the solution space. We will not cover RL in this course."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3. Modeling the machine learning problem\n",
      "\n",
      "The first step to apply data science and machine learning is identifying an interesting question to answer. According to the type of answer we are seeking we are directly aiming for a certain set of techniques.\n",
      "\n",
      "+ If our question is answered by *YES/NO*, we are in front of a **classification** problem. Classifiers are also the techniques to use if our question admits only a discrete set of answers, i.e. we want to select among a finite number of choices.  \n",
      "\n",
      "    + Given a client profile and past activity, which are the financial products she would be most interested in?\n",
      "    \n",
      "    + Given the results of a clinical test, does this patient suffers from diabetes?\n",
      "    \n",
      "    + Given an Magnetic Resonance Image, is there a tumor in it?\n",
      "    \n",
      "    + Given the past activity associated to a credit card, is the current operation a fraud?\n",
      "    \n",
      "    + Given my skills and marks in computer science and maths, will I pass the data science course?\n",
      "\n",
      "+ If our question is a prediction of a (usually real valued) quantity, we are in front of a **regression** problem.\n",
      "\n",
      "    + Given the description of an appartment, which is the expected market value of the flat? What would the value be if the appartment has a elevator?\n",
      "    \n",
      "    + Given the past records of user activities on Apps, how long is a certain client be hocked to our App?\n",
      "    \n",
      "    + Given my skills and marks in computer science and maths, what mark will I achive?\n",
      "    \n",
      "    \n",
      "Observe that some problems can be solved using both regression and classification. As we will see later many classification algorithms are thresholded regressors. There is a certain skill in designing the correct question and this dramatically change the solution we obtain. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**TAKE HOME PRINCIPLE:** Our first designing principle to keep in mind is that in general if a problem can be solved using a simpler question do not use a more complex one. This is an instantiation of the famous KISS principle (*Keep It Simple, Stupid!*). "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\">**QUIZ:** Which of the following questions correspond to a classification problem?\n",
      "\n",
      "<li> Weather forecast. \n",
      "<li> Is this behavior normal?\n",
      "<li> Where are my keys in this picture? \n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 4. The supervised classification problem. A basic and guided programatic example.\n",
      "\n",
      " \n",
      "In a supervised classification problem, given a set of examples with their corresponding label, our goal is to predict the membership of a given instance to one of a predefined discrete set of classes. \n",
      "\n",
      "Formally, we can describe the problem as follows: Consider a set *training set* composed of $N$ data sample pairs $\\{(x_i,y_i)\\}, \\quad i =1,\\dots,N$ where $x_i \\in {\\bf R}^d$ is described by $d$ features, and its corresponding supervised label, e.g. in the simplest binary case $y_i = \\{-1,1\\}$. Our goal is to find a model $h:{\\bf R}^d \\rightarrow {\\bf R}$ such that given a new data sample $x$ it correctly predicts its label $y$, i.e. $h(x) = y$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have seen **supervised learning** before ... *regression*. Let us refresh the basic pipeline."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "In machine learning we usually talk about two different steps:\n",
      "\n",
      "+ **Training**. Given a set of data instances $x$ and their corresponding label $y$ we want to learn/<span style=\"color:red\">fit</span> a model.\n",
      "\n",
      "+ **Testing or exploitation**. Given a model we want to apply it to new unseen data in order to <span style=\"color:red\">predict</span> its label.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check the following example on how to handle **basic training** and **persistence**. We consider a visual problem in order to build up our intuition on the process. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**The problem:** Consider the problem of handrwiten digits recognition. Given an image of a handwriten digit we want to build a classifier that recognizes the correct label."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us start loading the data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load data set.\n",
      "from sklearn import datasets\n",
      "digits = datasets.load_digits()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, check the data just loaded."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Check the data format.\n",
      "X, y = digits.data, digits.target\n",
      "\n",
      "print X.shape\n",
      "print y.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.1 Representing a machine learning problem in Scikit-Learn\n",
      "\n",
      "Recall the formalization of the problem where the training data set consists of $N$ data pairs $S = \\{(x_i,y_i)\\},\\; i = 1\\dots N$ where $x_i\u00a0\\in {\\bf R}^d$ is composed of $d$ features/descriptors and $y_i \\in \\{1,\\dots,K\\}$ is a discrete target label. In our current problem, we have $N = 1797$ data examples of handwritten numbers. Each sample is an $8\\times 8$ image. The representation of each data sample is encoded in vector. For this reason we flatten the image and reshape it to a vector with $d=64$ corresponding to the gray values/brightness of each pixel of the image. $y_i$ is the value of the target class the number belongs to."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us visualize the first digit."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "# The original digit has been flattened, so we reshape it back to its original form\n",
      "# Check the dimensionality of the data, e.g. the first element in the data set X[0]\n",
      "print X[0].shape\n",
      "print X[0]\n",
      "\n",
      "# Reshape it to 8x8 to recover the original image\n",
      "print X[0].reshape((8,8))\n",
      "\n",
      "\n",
      "# Show the image using scikit.image package\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.imshow(X[0].reshape((8,8)),cmap=\"gray\",interpolation=\"nearest\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us check some of the examples we have in our data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Visualize some of the data.\n",
      "import matplotlib.pyplot as plt\n",
      "fig, ax = plt.subplots(8, 12, subplot_kw={'xticks':[], 'yticks':[]})\n",
      "for i in range(ax.size):\n",
      "    ax.flat[i].imshow(digits.data[i].reshape(8, 8),\n",
      "                      cmap=plt.cm.binary)\n",
      "fig.set_size_inches((10,6))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A problem in Scikit-Learn is modeled as follows:\n",
      "\n",
      "+ Input data is structured in Numpy arrays. The size of the array is expected to be [n_samples, n_features]:\n",
      "\n",
      "    + *n_samples*: The number of samples ($N$): each sample is an item to process (e.g. classify). A sample can be a document, a picture, a sound, a video, an astronomical object, a row in database or CSV file, or whatever you can describe with a fixed set of quantitative traits.\n",
      "  \n",
      "    + *n_features*: The number of features ($d$) or distinct traits that can be used to describe each item in a quantitative manner. Features are generally real-valued, but may be boolean, discrete-valued or even cathegorical.\n",
      "\n",
      "$${\\rm feature~matrix:~~~} {\\bf X}~=~\\left[\n",
      "\\begin{matrix}\n",
      "x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
      "x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
      "x_{31} & x_{32} & \\cdots & x_{3d}\\\\\n",
      "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
      "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
      "x_{N1} & x_{N2} & \\cdots & x_{Nd}\\\\\n",
      "\\end{matrix}\n",
      "\\right]$$\n",
      "\n",
      "$${\\rm label~vector:~~~} {\\bf y}~=~ [y_1, y_2, y_3, \\cdots y_N]$$\n",
      "    \n",
      "\n",
      "The number of features must be fixed in advance. However it can be very high dimensional (e.g. millions of features) with most of them being zeros for a given sample. \n",
      "\n",
      "**Example** *Consider a text document representation. Given a text document we want to build a representation for it. In this case we could use as a description of the document a dictionary with all possible words in our language and create a description of the document as the number of times a certain word appears in the document. Each document is a sample and each value counting the number of times a word appear in the text is a feature. Observe that a single document will use just a handful of words. Thus there are many words not used and their representation will be zero. This is a case where scipy.sparse matrices can be useful, in that they are much more memory-efficient than numpy arrays.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Data set jargon\n",
      "\n",
      "Considering data arranged as in the previous section we refer to:\n",
      "\n",
      "+ the **columns** as features, attributes, dimensions, regressors, covariates, predictors, independent variables,\n",
      "+ the **rows** as instances, examples, samples.\n",
      "+ the **target** as label, outcome, response, dependent variable."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\">**QUESTION: ** Consider the following problem: *We are asked to develop a product similar to Shazzam(tm). This is, recognize the name of a song given a small sample of the music.*\n",
      "<p>\n",
      "Discuss and describe a posible feature vector for this problem with your partner.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.2 Learning and predicting with Scikit-Learn\n",
      "\n",
      "All objects in scikit-learn share a uniform and limited API consisting of three complementary interfaces:\n",
      "\n",
      "+ an estimator interface for building and fitting models (.fit());\n",
      "+ a predictor interface for making predictions (.predict());\n",
      "+ a transformer interface for converting data.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us choose a model and fit the training data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Train a classifier using .fit()\n",
      "from sklearn import neighbors\n",
      "knn = neighbors.KNeighborsClassifier(n_neighbors=10)\n",
      "knn.fit(X,y) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Save the model to disk (it can alternatively be stored in a string)\n",
      "import pickle\n",
      "ofname = open('my_classifier.pkl', 'wb')\n",
      "s = pickle.dump(knn,ofname)\n",
      "ofname.close()\n",
      "print s\n",
      "\n",
      "#Clear the namespace\n",
      "%reset -f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Check we don't have the variable in the namespace. This should give a NameError\n",
      "print knn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let us do some **exploitation** of the model. In this example we use the same data but in general new and unseen data is to be provided to the trained classifier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import neighbors\n",
      "from sklearn import datasets\n",
      "import pickle\n",
      "ofname = open('my_classifier.pkl','rb') #Open in binary format. You never know how it was saved.\n",
      "digits = datasets.load_digits()\n",
      "X = digits.data\n",
      "knn = pickle.load(ofname)\n",
      "\n",
      "#Compute the prediction according to the model\n",
      "print knn.predict(X[0])\n",
      "ofname.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Check the target value.\n",
      "y = digits.target\n",
      "y[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to evaluate the performance of the classifier, prediction accuracy can used:\n",
      "$$acc  = \\frac{\\mbox{# of correct predictions}}{N}$$\n",
      "\n",
      "Each estimator has a *.score()* method that invokes the default scoring metric. In the case of k-Nearest Neighbors this is classification accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Check the performance on the training set \n",
      "# - IF YOU KNOW WHAT YOU ARE DOING YOU WILL NEVER DO THIS AGAIN!\n",
      "knn.score(X,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\"> **EXERCISE: ** Put all the problem steps in one cell and check it runs.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#EXERCISE#Fill this cell with the exercise solution.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.3 More intuition about the data: The feature space\n",
      "\n",
      "Data is usually gathered as raw values. In the case of the digits dataset the gray values of the image. However, we can use domain knowledge we may consider important in order to discriminate the different classes. Take for instance two very simple derived features: horizontal, vertical symmetry and area. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from skimage import io as io\n",
      "\n",
      "tmp = X[7].reshape((8,8))    \n",
      "sym = tmp*tmp[:,::-1]\n",
      "io.imshow(tmp)\n",
      "io.show()\n",
      "io.imshow(tmp[:,::-1])\n",
      "io.show()\n",
      "\n",
      "import numpy as np\n",
      "Xnew = np.zeros((y.shape[0],3))\n",
      "for i in xrange(y.shape[0]):\n",
      "    area = sum(X[i])\n",
      "    tmp = X[i].reshape((8,8))    \n",
      "    symH = tmp*tmp[:,::-1]\n",
      "    symV = tmp*tmp[::-1,:]\n",
      "    \n",
      "    Xnew[i,:]=[sum(symH.flatten()), area, sum(symV.flatten())]\n",
      "\n",
      "print Xnew\n",
      "print Xnew.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Save this dataset for later use\n",
      "import pickle\n",
      "ofname = open('my_digits_data.pkl', 'wb')\n",
      "s = pickle.dump([Xnew,y],ofname)\n",
      "ofname.close()\n",
      "print 'DONE'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a three dimensional problem so we can easily visualize the feature space we have just created using matplotlib tools. We will select just two features and see how number 0 and number 6 differ."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "idxA = y==0\n",
      "idxB = y==6\n",
      "\n",
      "feature1 = 1\n",
      "feature2 = 0\n",
      "\n",
      "plt.figure()\n",
      "plt.scatter(Xnew[idxA, feature1], Xnew[idxA,feature2], c='blue')\n",
      "plt.scatter(Xnew[idxB, feature1], Xnew[idxB,feature2], c='red')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\"> **Exercise** Change feature1 and feature2 axis $\\in \\{0,1,2\\}$ and select the most suitable view for classification purposes. Why did you select that view?\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The process of using knowledge domain information in order to create discriminant features is called <span style=\"color:red\">feature extraction</span>."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Raw data vs feature extraction\n",
      "\n",
      "**Raw data**\n",
      "\n",
      "Advantages:\n",
      "\n",
      "+ No domain specific knowledge is required.\n",
      "\n",
      "Drawbacks:\n",
      "\n",
      "+ Highly redundant in many cases and usually span very large dimensional spaces.\n",
      "+ Unknown discriminability.\n",
      "\n",
      "**Feature extraction**\n",
      "\n",
      "Advantages:\n",
      "\n",
      "+ Attempt to capture discriminant information in the data.\n",
      "+ Lower dimensionality and complexity.\n",
      "\n",
      "Drawbacks: \n",
      "\n",
      "+ Domain specific knowledge is required."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\">**Exercise:** Train a new classifier on the new training set and check its training performance.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#EXERCISE#TODO NOW!!!\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.4 Measuring performance\n",
      "\n",
      "There are different criteria for measuring performance of a classifier and the most adequate metric is usually problem dependent. When no prior information on the problem in given, we usually use classification accuracy. When we are in front of a multi-class problem (there are many classes to choose from) we may use the confusion matrix. The elements of the confusion matrix $M$ are defined as follows,\n",
      "$$M(i,j) = \\mbox{# of samples from class j predicted as class i}$$\n",
      "\n",
      "Let us check these values:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "\n",
      "def plot_confusion_matrix(y, y_pred):\n",
      "    plt.imshow(metrics.confusion_matrix(y, y_pred),\n",
      "               cmap=plt.cm.jet, interpolation='nearest')\n",
      "    plt.colorbar()\n",
      "    plt.ylabel('true value')\n",
      "    plt.xlabel('predicted value')\n",
      "    \n",
      "print \"classification accuracy:\", metrics.accuracy_score(y, y_pred)\n",
      "plot_confusion_matrix(y, y_pred)\n",
      "#print metrics.classification_report(yhat,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\">**QUESTION:** Which are the classes with more confusion?\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.4 Training and testing. More intuition behind the learning process\n",
      "\n",
      "Check the following code and its result:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Train a classifier using .fit()\n",
      "from sklearn import neighbors\n",
      "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
      "knn.fit(Xnew,y)\n",
      "yhat=knn.predict(Xnew)\n",
      "\n",
      "print \"classification accuracy:\", metrics.accuracy_score(yhat, y)\n",
      "plot_confusion_matrix(y, yhat)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\">**QUESTION :** \n",
      "\n",
      "<li> Which is the accuracy of this classifier on the training set?\n",
      "<li> Do we expect to work better than the former one on new data?\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Up to this point we have checked the classifier performance on the same data it has been trained with. However in real applications data has not been previously seen. Let us simulate this effect splitting training data in two sets: One will be used for learning and the other for testing the accuracy performance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Reset workspace\n",
      "%reset -f\n",
      "\n",
      "#Load our old digits dataset \n",
      "import pickle\n",
      "ofname = open('my_digits_data.pkl', 'rb')\n",
      "data = pickle.load(ofname)\n",
      "X = data[0]\n",
      "y = data[1]\n",
      "ofname.close()\n",
      "\n",
      "\n",
      "# Simulate a real case: Randomize and split data in two subsets PRC*100% for training and \n",
      "# the rest (1-PRC)*100% for testing\n",
      "import numpy as np\n",
      "perm = np.random.permutation(y.size)\n",
      "print perm\n",
      "PRC = 0.7\n",
      "split_point = int(np.ceil(y.shape[0]*PRC))\n",
      "\n",
      "X_train = X[perm[:split_point].ravel(),:]\n",
      "y_train = y[perm[:split_point].ravel()]\n",
      "\n",
      "X_test = X[perm[split_point:].ravel(),:]\n",
      "y_test = y[perm[split_point:].ravel()]\n",
      "\n",
      "print X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
      "\n",
      "#Train a classifier on training data\n",
      "from sklearn import neighbors\n",
      "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
      "knn.fit(X_train,y_train)\n",
      "\n",
      "#Check on the training set and visualize performance\n",
      "yhat=knn.predict(X_train)\n",
      "from sklearn import metrics\n",
      "import matplotlib.pyplot as plt\n",
      "print \"\\nTRAINING STATS:\"\n",
      "print \"classification accuracy:\", metrics.accuracy_score(yhat, y_train)\n",
      "plt.imshow(metrics.confusion_matrix(y_train, yhat),\n",
      "               cmap=plt.cm.binary, interpolation='nearest')\n",
      "plt.xlabel('Training confusion matrix')\n",
      "plt.colorbar()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Check on the test set and visualize performance\n",
      "yhat=knn.predict(X_test)\n",
      "print \"TESTING STATS:\"\n",
      "print \"classification accuracy:\", metrics.accuracy_score(yhat, y_test)\n",
      "plt.figure()\n",
      "plt.imshow(metrics.confusion_matrix(y_test, yhat),\n",
      "               cmap=plt.cm.binary, interpolation='nearest')\n",
      "plt.xlabel('Testing confusion matrix')\n",
      "plt.colorbar()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\">**QUESTION : ** Describe what is going on.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Observe that each time we run this process we reach a different performance. A good simulation for approximating the test error is to run multiple times this process and average the performances. Let us do this!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#The splitting can be done using the tools provided by sklearn:\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn import neighbors\n",
      "from sklearn import metrics\n",
      "\n",
      "PRC = 0.3\n",
      "acc=np.zeros((10,))\n",
      "for i in xrange(10):\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=PRC)\n",
      "    knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
      "    knn.fit(X_train,y_train)\n",
      "    yhat=knn.predict(X_test)\n",
      "    acc[i] = metrics.accuracy_score(yhat, y_test)\n",
      "acc.shape=(1,10)\n",
      "print \"Mean expected error: \"+str(np.mean(acc[0]))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us introduce the nomenclature for the quantities we have just computed. We define:\n",
      "\n",
      "+ **In-sample error** $E_{\\text{in}}$: The in-sample error or training error is the error measured over all the observed data samples in the training set, i.e.\n",
      "\n",
      "$$E_{\\text{in}} = \\frac{1}{N}\\sum\\limits_{i=1}^{N} e(x_i,y_i)$$\n",
      "\n",
      "+ **Out-of-sample error** $E_{\\text{out}}$: The out-of-sample error or generalization error measure the expected error on unseen data. We can approximate this quantity holding out some training data for testing purposes.\n",
      "\n",
      "$$E_{\\text{out}}=\\mathbb{E}_{x,y}(e(x,y))$$\n",
      "\n",
      "Observe that there is still missing the definition of the instantaneous error $e(x_i,y_i)$. For example, in classification we could use the indicator fucntion to account for a correctly classified sample as follows\n",
      "\n",
      "$$e(x_i,y_i) = I[h(x_i)=y_i] =\\left\\{\\begin{align} 1 & \\text{if}\\; h(x_i)=y_i\\\\\n",
      "0 & \\text{otherwise} \\\\ \\end{align}\n",
      "\\right.$$\n",
      "\n",
      "\n",
      "Note that\n",
      "\n",
      "$$E_{\\text{out}} \\geq E_{\\text{in}}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.5 Model Selection (I)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the expected error on the test set, we can select the best classifier for our application. This is called **model selection**. In this example we cover the most simplistic setting. Suppose we have a set of different classifiers and want to select the \"best\" one. We may use the one that yields the lowest error rate. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#The splitting can be done using the tools provided by sklearn:\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn import neighbors\n",
      "from sklearn import tree\n",
      "from sklearn import svm\n",
      "from sklearn import metrics\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "PRC = 0.1\n",
      "acc_r=np.zeros((10,4))\n",
      "for i in xrange(10):\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=PRC)\n",
      "    nn1 = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
      "    nn3 = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
      "    svc = svm.SVC()\n",
      "    dt = tree.DecisionTreeClassifier()\n",
      "    \n",
      "    nn1.fit(X_train,y_train)\n",
      "    nn3.fit(X_train,y_train)\n",
      "    svc.fit(X_train,y_train)\n",
      "    dt.fit(X_train,y_train)\n",
      "    \n",
      "    yhat_nn1=nn1.predict(X_test)\n",
      "    yhat_nn3=nn3.predict(X_test)\n",
      "    yhat_svc=svc.predict(X_test)\n",
      "    yhat_dt=dt.predict(X_test)\n",
      "    \n",
      "    acc_r[i][0] = metrics.accuracy_score(yhat_nn1, y_test)\n",
      "    acc_r[i][1] = metrics.accuracy_score(yhat_nn3, y_test)\n",
      "    acc_r[i][2] = metrics.accuracy_score(yhat_svc, y_test)\n",
      "    acc_r[i][3] = metrics.accuracy_score(yhat_dt, y_test)\n",
      "\n",
      "\n",
      "plt.boxplot(acc_r);\n",
      "for i in xrange(4):\n",
      "    xderiv = (i+1)*np.ones(acc_r[:,i].shape)+(np.random.rand(10,)-0.5)*0.1\n",
      "    plt.plot(xderiv,acc_r[:,i],'ro',alpha=0.3)\n",
      "    \n",
      "ax = plt.gca()\n",
      "ax.set_xticklabels(['1-NN','3-NN','SVM','Decission Tree'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This process is one particular form of a general model selection technique called **cross-validation**. There are other kinds of cross-validation, such as **leave-one-out** or **K-fold cross-validation**. \n",
      "\n",
      "+ In leave-one-out, given $N$ samples the model is trained with $N-1$ samples and tested with the remaining one. This is repeated $N$ times, once per training sample and the result is averaged. \n",
      "\n",
      "+ In K-fold cross-validation the training set is divided in K non-overlapping splits. K-1 splits are used for training and the remaining one used for assessing the mean. This process is repeated $K$ times leaving one split out each time. The results are averaged. We can compute an approximation to the confidence interval using this method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "#Create a 10-fold cross validation set\n",
      "acc = np.zeros((10,4))\n",
      "kf=cross_validation.KFold(n=y.shape[0], n_folds=10, indices=True, shuffle=True, random_state=0)\n",
      "i=0\n",
      "for train_index, test_index in kf:\n",
      "    X_train, X_test = X[train_index], X[test_index]\n",
      "    y_train, y_test = y[train_index], y[test_index]\n",
      "    \n",
      "    nn1 = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
      "    nn3 = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
      "    svc = svm.SVC()\n",
      "    dt = tree.DecisionTreeClassifier()\n",
      "    \n",
      "    nn1.fit(X_train,y_train)\n",
      "    nn3.fit(X_train,y_train)\n",
      "    svc.fit(X_train,y_train)\n",
      "    dt.fit(X_train,y_train)\n",
      "    \n",
      "    yhat_nn1=nn1.predict(X_test)\n",
      "    yhat_nn3=nn3.predict(X_test)\n",
      "    yhat_svc=svc.predict(X_test)\n",
      "    yhat_dt=dt.predict(X_test)\n",
      "    \n",
      "    acc[i][0] = metrics.accuracy_score(yhat_nn1, y_test)\n",
      "    acc[i][1] = metrics.accuracy_score(yhat_nn3, y_test)\n",
      "    acc[i][2] = metrics.accuracy_score(yhat_svc, y_test)\n",
      "    acc[i][3] = metrics.accuracy_score(yhat_dt, y_test)\n",
      "    i=i+1\n",
      "    \n",
      "plt.boxplot(acc);\n",
      "for i in xrange(4):\n",
      "    xderiv = (i+1)*np.ones(acc[:,i].shape)+(np.random.rand(10,)-0.5)*0.1\n",
      "    plt.plot(xderiv,acc[:,i],'ro',alpha=0.3)\n",
      "    \n",
      "ax = plt.gca()\n",
      "ax.set_xticklabels(['1-NN','3-NN','SVM','Decission Tree'])    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Just for fun let us put both plots together\n",
      "fig = plt.figure()\n",
      "ax = plt.gca()\n",
      "for i in xrange(4):\n",
      "    plt.boxplot([acc[:,i], acc_r[:,i]], positions = [2*i+1,2*i+2], widths = 0.6)\n",
      "    xderiv = (2*i+1)*np.ones(acc[:,i].shape)+(np.random.rand(10,)-0.5)*0.1\n",
      "    plt.plot(xderiv,acc[:,i],'ro',alpha=0.3)\n",
      "    xderiv = (2*i+2)*np.ones(acc[:,i].shape)+(np.random.rand(10,)-0.5)*0.1\n",
      "    plt.plot(xderiv,acc_r[:,i],'bo',alpha=0.3)\n",
      "# set axes limits and labels\n",
      "plt.xlim(0,9)\n",
      "plt.ylim(0,0.4)\n",
      "ax.set_xticklabels(['1-NN','3-NN','SVM','Decission Tree'])\n",
      "ax.set_xticks([1.5, 3.5, 5.5, 7.5])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Summary: Scikit-learn's estimator interface\n",
      "\n",
      "Scikit-learn strives to have a uniform interface across all methods,\n",
      "and we'll see examples of these below. Given a scikit-learn *estimator*\n",
      "object named `model`, the following methods are available:\n",
      "\n",
      "- Available in **all Estimators**\n",
      "  + `model.fit()` : fit training data. For supervised learning applications,\n",
      "    this accepts two arguments: the data `X` and the labels `y` (e.g. `model.fit(X, y)`).\n",
      "    For unsupervised learning applications, this accepts only a single argument,\n",
      "    the data `X` (e.g. `model.fit(X)`).\n",
      "- Available in **supervised estimators**\n",
      "  + `model.predict()` : given a trained model, predict the label of a new set of data.\n",
      "    This method accepts one argument, the new data `X_new` (e.g. `model.predict(X_new)`),\n",
      "    and returns the learned label for each object in the array.\n",
      "  + `model.predict_proba()` : For classification problems, some estimators also provide\n",
      "    this method, which returns the probability that a new observation has each categorical label.\n",
      "    In this case, the label with the highest probability is returned by `model.predict()`.\n",
      "  + `model.score()` : for classification or regression problems, most (all?) estimators implement\n",
      "    a score method.  Scores are between 0 and 1, with a larger score indicating a better fit.\n",
      "- Available in **unsupervised estimators**\n",
      "  + `model.transform()` : given an unsupervised model, transform new data into the new basis.\n",
      "    This also accepts one argument `X_new`, and returns the new representation of the data based\n",
      "    on the unsupervised model.\n",
      "  + `model.fit_transform()` : some estimators implement this method,\n",
      "    which more efficiently performs a fit and a transform on the same input data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#PART 2: Learning concepts and theory"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 1. What is learning?\n",
      "\n",
      "Let us recall the two basic values defined in the last notebook. We talk about *training error* or *in-sample error*, $E_{\\text{in}}$, referring to the error measured over all the observed data samples in the training set. We talk about *testing error* or *generalization error*, $E_{\\text{out}}$, as the expected error on unseen data. \n",
      "\n",
      "We can empirically estimate the generalization error by means of cross-validation techniques and observe that\n",
      "\n",
      "$$E_{\\text{out}} \\geq E_{\\text{in}}.$$\n",
      "\n",
      "The goal of learning is to minimize the generalization error, but how can we guarantee this minimization only using training data?\n",
      "\n",
      "From the above inequality it is easy to derive a couple of very intuitive ideas:\n",
      "\n",
      "+ Because $E_{\\text{out}}$ is greater than or equal to $E_{\\text{in}}$, it is desirable to have $$E_{\\text{in}} \\rightarrow 0.$$\n",
      "\n",
      "+ Additionally, we also want the training error behavior to track the generalization error, i.e. $$E_{\\text{out}}\\approx E_{\\text{in}}.$$\n",
      "\n",
      "We can rewrite this second condition as \n",
      "$$E_{\\text{in}} \\leq E_{\\text{out}} \\leq E_{\\text{in}} + \\Omega,$$\n",
      "with $\\Omega \\rightarrow 0$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Probably approximately correct learning.\n",
      "\n",
      "We would like to characterize $\\Omega$ in terms of our problem parameters, i.e. number of samples ($N$), dimensionality of the problem ($d$), etc. \n",
      "\n",
      "Statistic analysis offers an interesting characterization of this quantity \n",
      "\n",
      "$$E_{\\text{out}} \\leq E_{\\text{in}} + \\mathcal{O}\\big(\\sqrt{\\frac{\\log{C}}{N}}\\big),$$\n",
      "\n",
      "where $C$ is a measure of complexity of the model class we are using. Technically, we may refer to this model class also as the hypothesis space."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\">**QUIZ:** \n",
      "<li> Which will be the effect of having a large number of data? \n",
      "<li> Will selecting a model with small complexity reduce the out of sample error?\n",
      "</div>\n",
      "\n",
      "Before going further in this matter let us open the box of the learning process and observe which parts it is composed of."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#2. Inside the learning process\n",
      "\n",
      "Consider a simple two dimensional problem."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from sklearn import \n",
      "import numpy as np\n",
      "#Create some data\n",
      "X = np.concatenate([1.25*np.random.randn(40,2),5+1.5*np.random.randn(40,2)]) \n",
      "y = np.concatenate([np.ones((40,1)),-np.ones((40,1))])\n",
      "\n",
      "#Visualize\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X[0:40,0],X[0:40,1],color='r')\n",
      "plt.scatter(X[40:,0],X[40:,1],color='b')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to be able to learn, any algorithm has to define at least three components:\n",
      "\n",
      "+ **The model class/hypothesis space** defines the family of mathematical models that will be used. The target decision boundary will be approximated from one element of this space. For example, we can consider the class of linear models. In this case our decision boundary will be a line if the problem is defined in ${\\bf R}^2$ and the model class is the space of all posible lines in ${\\bf R}^2$. \n",
      "\n",
      "    Model classes define the geometric properties of the decision function. There are different taxonomies but the most well-known are the *families* of **linear** and **non-linear** models. These families usually depend on some parameters. And the solution to a learning problem is the selection of a particular set of parameters, i.e. the selection of an instance model from the model class space. The model class space is also called **hypothesis space**.\n",
      "\n",
      "    The selection of the best model will depend on our problem and what we want to obtain from the problem. The primary goal in learning is usually achieving the minimum error/maximum performance. But according to what else we want from the algorithm we will find different algorithms. Other common desirable properties are interpretability, behavior in front of missing data, fast training, etc.\n",
      "\n",
      "\n",
      "+ **The problem model** formalizes and encodes the desired properties of the solution. In many cases this formalization takes the form of an optimization problem. In it most basic instantiation, the problem model can be the **minimization of an error function**. The error function measures the difference between our model and the target one. Informally speaking, in a classification problem it measures how \"irritated\" we are when our model misses the right label of a training sample. For example, in classification the ideal error function is the **0-1 loss**. This function takes value $1$ when we incorrectly classify a training sample and zero otherwise. In this case, it can be intrepreted that one is only irritated by \"one unit of irritation\" when one sample is misclassified.\n",
      "\n",
      "    Problem model can also be used to impose other constraints on our solution, such as finding a smooth approximation, small complexity model, sparse solution, etc.\n",
      "    \n",
      "    \n",
      "+ **The learning algorithm** is an optimization/search method or algorithm that given a model class fits it to the training data according to the error function. According to the nature of our problem there are many different algorithms. In general, we are talking about finding the minimum error approximation or maximum probable model. In those cases, if the problem is convex/quasi-convex we will typically use first or second order methods (i.e. gradient descent, coordinate descent, Newton's method, Interior Point methods, etc). Other searching techniques such as genetic algorithms or monte-carlo techniques can be used if we do not have access to the derivatives of the objective function.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us use the \"human machine learning\" algorithm. You move the parameters until you feel the solution is correct."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from sklearn import \n",
      "import numpy as np\n",
      "#Create some data\n",
      "X = np.concatenate([1.25*np.random.randn(40,2),5+1.5*np.random.randn(40,2)]) \n",
      "y = np.concatenate([np.ones((40,1)),-np.ones((40,1))])\n",
      "\n",
      "#Visualize\n",
      "import matplotlib.pyplot as plt\n",
      "from IPython.html.widgets import interact\n",
      "\n",
      "def human_learning_algorithm(X,y):\n",
      "    \n",
      "    plt.scatter(X[0:40,0],X[0:40,1],color='r')\n",
      "    plt.scatter(X[40:,0],X[40:,1],color='b')\n",
      "    #plt.scatter(X[0:40,0],X[0:40,1],color='r')\n",
      "    #plt.scatter(X[40:,0],X[40:,1],color='b')    \n",
      "    delta = 0.025\n",
      "    xx = np.arange(-5.0, 10.0, delta)\n",
      "    yy = np.arange(-5.0, 10.0, delta)\n",
      "    XX, YY = np.meshgrid(xx, yy)\n",
      "    Xf = XX.flatten()\n",
      "    Yf = YY.flatten()\n",
      "    sz=XX.shape\n",
      "    data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
      "\n",
      "    def hml_display(w0,w1,offset):\n",
      "        w=np.array([w0,w1])\n",
      "        w.shape=(2,1)\n",
      "        #Evaluate the model for a given weight\n",
      "        Z = data.dot(w)+offset\n",
      "        Z.shape=sz\n",
      "        plt.scatter(X[0:40,0],X[0:40,1],color='r')\n",
      "        plt.scatter(X[40:,0],X[40:,1],color='b')\n",
      "        plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,10,-5,10),alpha=0.3, vmin=-15, vmax=15)\n",
      "        plt.contour(XX,YY,Z,[0])\n",
      "        fig = plt.gcf()\n",
      "        fig.set_size_inches(9,9)\n",
      "   \n",
      "    #Ipython 2.0      \n",
      "    interact(hml_display, w0=(-10.,10.), w1=(-10.,10.), offset=(-20.,40.));\n",
      "    #Ipython 1.1 back compatibility\n",
      "    #w0=-10. #-10.,10.\n",
      "    #w1=-10. #-10.,10.\n",
      "    #offset=-20. #-20.,40.\n",
      "    #hml_display(w0,w1,offset) \n",
      "\n",
      "    \n",
      "human_learning_algorithm(X,y)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\">**QUESTION:** Describe the process you used for fitting the classifier\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-info\">**Some notes on the learning process**\n",
      "<p>\n",
      "The main goal of any learning process is to achive the maximum predictive power (*accuracy*). This is minimize the error. However, there are three other important properties we usually desire our models to have:\n",
      "\n",
      "<p>\n",
      "<li> **Simplicity** - how much fiddling do we need for the method to work? Can I modify it to handle the particularities of my problem?\n",
      "<li> **Speed** - How long does it take to train a reliable model? (training time) Can I use it in embedded and real time applications? (testing time), How long do I have to wait for processing my 1YB (yottabyte - 1e24 Bytes) dataset?\n",
      "<li> **Interpretability** - Why did it make these predictions?\n",
      "\n",
      "<p>\n",
      "It happens that accuracy trades off with all the rest of the desirable properties. \n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#3. Learning curves and overfitting.\n",
      "\n",
      "Let us go back to PAC learnability."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reset -f\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from IPython.html.widgets import interact\n",
      "from sklearn import neighbors\n",
      "\n",
      "MAXC=50\n",
      "MAXN=1000\n",
      "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
      "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
      "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
      "y = np.concatenate([y,np.ones((MAXN,1))])\n",
      "perm = np.random.permutation(y.size)\n",
      "X = X[perm,:]\n",
      "y = y[perm]\n",
      "\n",
      "\n",
      "def complexity_number(C,N):\n",
      "    \n",
      "    Xr=X[:N,:]\n",
      "    yr=y[:N]\n",
      "    idxplus = yr==1\n",
      "    idxminus = yr==-1\n",
      "    idxplus = idxplus.flatten()\n",
      "    idxminus = idxminus.flatten()\n",
      "    plt.scatter(Xr[idxplus,0],Xr[idxplus,1],color='r')\n",
      "    plt.scatter(Xr[idxminus,0],Xr[idxminus,1],color='b')   \n",
      "    delta = 0.05\n",
      "    xx = np.arange(-5.0, 15.0, delta)\n",
      "    yy = np.arange(-5.0, 15.0, delta)\n",
      "    XX, YY = np.meshgrid(xx, yy)\n",
      "    Xf = XX.flatten()\n",
      "    Yf = YY.flatten()\n",
      "    sz=XX.shape\n",
      "    data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
      "    #Evaluate the model for a given weight\n",
      "    clf = neighbors.KNeighborsClassifier(MAXC-C+1)\n",
      "    clf.fit(Xr,yr.ravel())\n",
      "    Z=clf.predict(data)\n",
      "    Z.shape=sz\n",
      "    plt.scatter(Xr[idxplus,0],Xr[idxplus,1],color='r')\n",
      "    plt.scatter(Xr[idxminus,0],Xr[idxminus,1],color='b')\n",
      "    plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
      "    plt.contour(XX,YY,Z,[0])\n",
      "    fig = plt.gcf()\n",
      "    fig.set_size_inches(9,9)\n",
      "   \n",
      "#Ipython 2.0\n",
      "interact(complexity_number,  C=(1,MAXC), N = (20,MAXN));\n",
      "#Ipython 1.1.\n",
      "#C=1 #1...50\n",
      "#N = 20 #20...1000\n",
      "#complexity_number(C,N)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\">**EXERCISE: ** \n",
      "<li> Set the number of data samples per cluster $N$ to $100$ and the complexity value $C$ to $50$. Describe what you observe: Does the method missclassify any data sample? \n",
      "<li> Decrease the complexity value to $C = 20$. Describe the boundary: Does the method missclassify any data sample?\n",
      "<li> Which of the two settings do you think will perform the best in front of new data from the same distribution? Why?\n",
      "<li> Increase the number of data points to $N = 1000$ with $C=50$. Describe what you observe. Will the method perform better than the same method with $N=100$?\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3.1 Learning curves.\n",
      "\n",
      "Let us visualize the behavior observed. For this purpose we may draw a curve of the training error and test error as the number of training data increases for a given complexity. This curve is called **learning curve**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reset -f\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from IPython.html.widgets import interact\n",
      "from sklearn import metrics\n",
      "from sklearn import tree\n",
      "\n",
      "C=5\n",
      "MAXN=1000\n",
      "\n",
      "yhat_test=np.zeros((10,299,2))\n",
      "yhat_train=np.zeros((10,299,2))\n",
      "#Repeat ten times to get smooth curves\n",
      "for i in xrange(10):\n",
      "    X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
      "    X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
      "    y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
      "    y = np.concatenate([y,np.ones((MAXN,1))])\n",
      "    perm = np.random.permutation(y.size)\n",
      "    X = X[perm,:]\n",
      "    y = y[perm]\n",
      "\n",
      "    X_test = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
      "    X_test = np.concatenate([X_test,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
      "    y_test = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
      "    y_test = np.concatenate([y_test,np.ones((MAXN,1))])\n",
      "    j=0\n",
      "    for N in xrange(10,3000,10):\n",
      "        Xr=X[:N,:]\n",
      "        yr=y[:N]\n",
      "        idxplus = yr==1\n",
      "        idxminus = yr==-1\n",
      "        idxplus = idxplus.flatten()\n",
      "        idxminus = idxminus.flatten()\n",
      "        #Evaluate the model\n",
      "        clf = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=C)\n",
      "        clf.fit(Xr,yr.ravel())\n",
      "        yhat_test[i,j,0] = 1. - metrics.accuracy_score(clf.predict(X_test), y_test.ravel())\n",
      "        yhat_train[i,j,0] = 1. - metrics.accuracy_score(clf.predict(Xr), yr.ravel())\n",
      "        j=j+1\n",
      "\n",
      "plt.plot(np.mean(yhat_test[:,:,0].T,axis=1),'r')\n",
      "plt.plot(np.mean(yhat_train[:,:,0].T,axis=1),'b')\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(9,5)\n",
      "plt.xlabel('Number of samples x10')\n",
      "plt.ylabel('Error rate')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Observing the former plot we can see:\n",
      "\n",
      "+ As the number of training samples increase both errors tends to the same value, **bias**.\n",
      "+ When we have a little amount of training data, training error is very small but test error is very large.\n",
      "\n",
      "Check now the learning curve when the complexity is smaller."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "C=1\n",
      "MAXN=1000\n",
      "\n",
      "#Repeat ten times to get smooth curves\n",
      "for i in xrange(10):\n",
      "    X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
      "    X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
      "    y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
      "    y = np.concatenate([y,np.ones((MAXN,1))])\n",
      "    perm = np.random.permutation(y.size)\n",
      "    X = X[perm,:]\n",
      "    y = y[perm]\n",
      "\n",
      "    X_test = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
      "    X_test = np.concatenate([X_test,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
      "    y_test = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
      "    y_test = np.concatenate([y_test,np.ones((MAXN,1))])\n",
      "    j=0\n",
      "    for N in xrange(10,3000,10):\n",
      "        Xr=X[:N,:]\n",
      "        yr=y[:N]\n",
      "        idxplus = yr==1\n",
      "        idxminus = yr==-1\n",
      "        idxplus = idxplus.flatten()\n",
      "        idxminus = idxminus.flatten()\n",
      "        #Evaluate the model\n",
      "        clf = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=C)\n",
      "        clf.fit(Xr,yr.ravel())\n",
      "        yhat_test[i,j,1] = 1. - metrics.accuracy_score(clf.predict(X_test), y_test.ravel())\n",
      "        yhat_train[i,j,1] = 1. - metrics.accuracy_score(clf.predict(Xr), yr.ravel())\n",
      "        j=j+1\n",
      "\n",
      "plt.plot(np.mean(yhat_test[:,:,1].T,axis=1),'r')\n",
      "plt.plot(np.mean(yhat_train[:,:,1].T,axis=1),'b')\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(9,5)\n",
      "plt.xlabel('Number of samples x10')\n",
      "plt.ylabel('Error rate')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see a similar behavior in this second curve. Let us compare the two plots."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p1,=plt.plot(np.mean(yhat_test[:,:,0].T,axis=1),color='pink')\n",
      "p2,=plt.plot(np.mean(yhat_train[:,:,0].T,axis=1),'c')\n",
      "p3,=plt.plot(np.mean(yhat_test[:,:,1].T,axis=1),'r')\n",
      "p4,=plt.plot(np.mean(yhat_train[:,:,1].T,axis=1),'b')\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(9,5)\n",
      "plt.xlabel('Number of samples x10')\n",
      "plt.ylabel('Error rate')\n",
      "plt.legend([p1,p2,p3,p4],[\"Test C = 5\",\"Train C = 5\",\"Test C = 1\",\"Train C = 1\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Although both show a similar behavior we note several differences:\n",
      "\n",
      "+ With small complexity training and test errors converge sooner/with a smaller amount of data.\n",
      "+ However, with small complexity, the error of converge is larger than with increased complexity.\n",
      "\n",
      ">The value towards both errors converge is also called **bias**, and the difference between this value and the test error is called **variance**. The **bias/variance** decomposition of the learning curve is an alternative view to the training and generalization view."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3.2 Overfitting.\n",
      "\n",
      "Let us now plot the learning behavior for a fixed number of examples with respect to the complexity of the model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\">**QUESTION: ** What do you expect to happen?\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reset -f\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from IPython.html.widgets import interact\n",
      "from sklearn import metrics\n",
      "from sklearn import tree\n",
      "\n",
      "MAXC=20\n",
      "N=1000\n",
      "NTEST=4000\n",
      "ITERS=3\n",
      "\n",
      "yhat_test=np.zeros((ITERS,MAXC,2))\n",
      "yhat_train=np.zeros((ITERS,MAXC,2))\n",
      "#Repeat ten times to get smooth curves\n",
      "for i in xrange(ITERS):\n",
      "    X = np.concatenate([1.25*np.random.randn(N,2),5+1.5*np.random.randn(N,2)]) \n",
      "    X = np.concatenate([X,[8,5]+1.5*np.random.randn(N,2)])\n",
      "    y = np.concatenate([np.ones((N,1)),-np.ones((N,1))])\n",
      "    y = np.concatenate([y,np.ones((N,1))])\n",
      "    perm = np.random.permutation(y.size)\n",
      "    X = X[perm,:]\n",
      "    y = y[perm]\n",
      "\n",
      "    X_test = np.concatenate([1.25*np.random.randn(NTEST,2),5+1.5*np.random.randn(NTEST,2)]) \n",
      "    X_test = np.concatenate([X_test,[8,5]+1.5*np.random.randn(NTEST,2)])\n",
      "    y_test = np.concatenate([np.ones((NTEST,1)),-np.ones((NTEST,1))])\n",
      "    y_test = np.concatenate([y_test,np.ones((NTEST,1))])\n",
      "    \n",
      "    idxplus = y==1\n",
      "    idxminus = y==-1\n",
      "    idxplus = idxplus.flatten()\n",
      "    idxminus = idxminus.flatten()\n",
      "    j=0\n",
      "    for C in xrange(1,MAXC+1):\n",
      "        #Evaluate the model\n",
      "        clf = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=C)\n",
      "        clf.fit(X,y.ravel())\n",
      "        yhat_test[i,j,0] = 1. - metrics.accuracy_score(clf.predict(X_test), y_test.ravel())\n",
      "        yhat_train[i,j,0] = 1. - metrics.accuracy_score(clf.predict(X), y.ravel())\n",
      "        j=j+1\n",
      "\n",
      "p1, = plt.plot(np.mean(yhat_test[:,:,0].T,axis=1),'r')\n",
      "p2, = plt.plot(np.mean(yhat_train[:,:,0].T,axis=1),'b')\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(9,5)\n",
      "plt.xlabel('Complexity')\n",
      "plt.ylabel('Error rate')\n",
      "plt.legend([p1, p2], [\"Testing error\", \"Training error\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Observe that as the complexity increases the training error is reduced but above a certain complexity level the test error increases. This effect is called **overfitting**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\">**QUESTION: ** Let us go back to regression problems. Consider that we know the underlying model generating data samples. For example, data is generated by a 10th order polynomial. Mark all the correct answers:\n",
      "\n",
      "<li> I will use a 10th order polynomial as my model.\n",
      "<li> I will use a 2nd order polynomial as my model.\n",
      "<li> If I have a small number of data I have to use a small order polynomial.\n",
      "<li> If I have a large amount of data I may use a tenth order polynomial.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 4. Cures to overfitting.\n",
      "\n",
      "We may find three cures to overfitting:\n",
      "\n",
      "+ Observe that models are usually parameterized by some hyper-parameters. Selecting the complexity is usually governed by some of such parameters. Thus we are in front of a model selection problem. A good heuristic for selecting the model is to choose the value of the hyperparameters that yield the smallest estimated testing error. Remember that this can be done using **cross-validation**.\n",
      "\n",
      "+ We may also change the formulation of the objective function to penalize complex models. This is called **regularization**.\n",
      "\n",
      "+ We may use **ensembles**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 4.1 Cure I: Use model selection.\n",
      "\n",
      "We have seen model selection before when we wanted to compare the performance of different classifiers. In that case, our best bet was to select the classifier with the smallest ${E}_{\\text{out}}$. \n",
      "\n",
      "> **IDEA:** Analogous to model selection we may think of selecting the best hyper-parameters as choosing the classifier with parameters that performs the best. Thus, we may select a set of hyper-parameter values and use cross-validation to select the best configuration.\n",
      "\n",
      "The process of selecting the best hyper-parameters is called **validation**. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reset\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from IPython.html.widgets import interact\n",
      "from sklearn import metrics\n",
      "from sklearn import tree\n",
      "from sklearn import cross_validation\n",
      "\n",
      "#Create a toy problem\n",
      "N=500\n",
      "X = np.concatenate([1.25*np.random.randn(N,2),5+1.5*np.random.randn(N,2)]) \n",
      "X = np.concatenate([X,[8,5]+1.5*np.random.randn(N,2)])\n",
      "y = np.concatenate([np.ones((N,1)),-np.ones((N,1))])\n",
      "y = np.concatenate([y,np.ones((N,1))])\n",
      "\n",
      "#Create a 10-fold cross validation set\n",
      "kf=cross_validation.KFold(n=y.shape[0], n_folds=10, indices=True, shuffle=True, random_state=0)\n",
      "      \n",
      "#Search the parameter among the following\n",
      "C=np.arange(2,20)\n",
      "\n",
      "acc = np.zeros((10,18))\n",
      "i=0\n",
      "for train_index, val_index in kf:\n",
      "    X_train, X_val = X[train_index], X[val_index]\n",
      "    y_train, y_val = y[train_index], y[val_index]\n",
      "    j=0\n",
      "    for c in C:\n",
      "        dt = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=c)\n",
      "        dt.fit(X_train,y_train)\n",
      "        yhat = dt.predict(X_val)\n",
      "        acc[i][j] = metrics.accuracy_score(yhat, y_val)\n",
      "        j=j+1\n",
      "    i=i+1\n",
      "    \n",
      "plt.boxplot(acc);\n",
      "for i in xrange(18):\n",
      "    xderiv = (i+1)*np.ones(acc[:,i].shape)+(np.random.rand(10,)-0.5)*0.1\n",
      "    plt.plot(xderiv,acc[:,i],'ro',alpha=0.3)\n",
      "\n",
      "\n",
      "plt.ylim((0.80,0.95))\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(9,5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "macc= np.mean(acc,axis=0)\n",
      "idx=np.argmax(macc)\n",
      "print 'Complexity: ' + str(idx+1) + ' with accuracy: ' + str(macc[idx]) \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is the generalization error expected by selecting this method?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dt = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=idx+1)\n",
      "dt.fit(X_train,y_train)\n",
      "\n",
      "N=1000\n",
      "#Let us check it out by generating out of sample data\n",
      "X_test = np.concatenate([1.25*np.random.randn(N,2),5+1.5*np.random.randn(N,2)]) \n",
      "X_test = np.concatenate([X_test,[8,5]+1.5*np.random.randn(N,2)])\n",
      "y_test = np.concatenate([np.ones((N,1)),-np.ones((N,1))])\n",
      "y_test = np.concatenate([y_test,np.ones((N,1))])\n",
      "\n",
      "yhat = dt.predict(X_test)\n",
      "print metrics.accuracy_score(yhat, y_test)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.1.1 Train, Test and Validation.\n",
      "\n",
      "As we have seen the process of assessing the performance of the classifier by estimating the generalization error is called testing. And the process of selecting a model using the estimation of the generalization error is called validation. There is a subtle but critical difference in both and we have to be aware of it when dealing with our problem. \n",
      "\n",
      "+ Testing data is used only for assessing performance and will never be used in the learning process.\n",
      "+ Validation data is used to explicitly select the parameter with best performance according to an estimation of the generalization error. This is a form of learning. \n",
      "+ Training data is used for learning the model instance from a model class.\n",
      "\n",
      "In practice, we are given just training data, and in the most general case we have to explicitly tune some hyper-paramter. Thus, how do we select the different splits?\n",
      "\n",
      "It will depend on the questions about the method we want to answer:\n",
      "\n",
      "+ Let us say that our customer ask us to hand her a classifier for a given problem. If we just want to give him the best model then we may use cross-validation on our training data set and select the best performant model. In this scenario, when we return the trained classifier to our customer, we know that it is the one that achieves the best performance. But if the customer asks about the expected performance we can not say anything.\n",
      "\n",
      "> **A practical issue:** once selected the model we use the complete training set to train the final model.\n",
      "\n",
      "+ If we want to know the performance of our model we have to use unseen data. Thus, we may proceed in the following way:\n",
      "\n",
      "    + Split the training set in training and testing data. For example, use $30\\%$ of the training set for testing purposes. This data is hold out and will only be used to assess the performance of the method.\n",
      "\n",
      "    + Use the remanining training data for selecting the hyper-parameters by means of cross-validation.\n",
      "    \n",
      "    + Train the model with the selected parameter and assess the performance using the testing data set.\n",
      "\n",
      "> **A practical issue:** Observe that by splitting in three sets the classifier is trained with a smaller fraction of the data.\n",
      "\n",
      "+ If we want to make a good comparison of classifiers but we do not care about the best parameters, we may use nested cross-validation. The external cross-validation is used for assessing the performance of the classifier and in each loop of the external cross-validation another cross-validation is run with the remaining training set for selecting the best parameters."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 4.2 Cure II: Use regularization.\n",
      "\n",
      "Regularization accounts for estimating the value of $\\Omega$ in our out-of-sample inequality. In other words, it models the complexity of the technique. This usually becomes implicit in the algorithm but has huge consequences in real applications. There are two kinds of standard regularization strategies:\n",
      "\n",
      "+ L2 regularization: Intuitively, L2 regularization is in many cases a surrogate of the notion of smoothness. In this sense, low complexity means smooth models.\n",
      "\n",
      "+ L1 regularization: L1 regularization force sparse solution. This is useful for interpretability or when the number of parameters is so large that we only want a few active ones for computational issues. \n",
      "\n",
      "Although they are used to deal with overfitting, they trade-off with the error function in the objective and are governed by a hyper-parameter. Thus, we still have to select this parameter by means of model selection."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 4.3 Cure III: Ensemble\n",
      "\n",
      "A third cure to overfitting is to use ensemble techniques. The most well known are **Bagging** and **Boosting**. We will cover **Bagging** in another module."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#5. What to do when ..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "... our algorithm shows high **bias**.\n",
      "\n",
      "- **Add more features** If we are able to engineer discriminant features, this might help the classifier to reduce its bias.\n",
      "- **Use a more sophisticated model**. High bias usually means poor performance. If we are using a very simple model class, this might indicate that this model class is not able to properly fit training data. If our problem uses regularization for modeling complexity we might adjust the hyper-parameter to decrease the complexity.\n",
      "- **Use fewer samples**. Although this will not improve the results, we might as well train the model with fewer data points just for improving training time without worsening the performance.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "... our algorithm shows **high variance**.\n",
      "\n",
      "- **Use fewer features**. Using a feature selection or dimensionality reduction techniques may be useful, and decrease the over-fitting of the estimator.\n",
      "- **Use a simpler model**. High variance is usually synonim of closely modeling the training set. This can result in overfitting. In this case we can either use a simpler model class or adjust the regularization hyper-parameter to decrease complexity.\n",
      "- **Use more training samples**. \n",
      "- **Use ensemble techniques**. Some ensemble techniques such as *bootstraping aggregation* are specifically designed to reduce classification variance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# PART 3: Models in Machine Learning\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#1. Generative and discriminative models.\n",
      "\n",
      "In literature we will find two complementary visions of the learning problem according to the problem they solve, generative vs discriminative models. \n",
      "\n",
      "+ **Generative models** goal is modeling the data. This consists of estimating the joint probability density function of $x,y$, $P(x,y)$. With this description, the problem of classification is selecting the model that maximizes the posterior probability of the labels given the data, $P(y|x)$. This is done by applying Bayes rule to relate the posterior distribution to the likelihood and the priors.\n",
      "\n",
      "$$\\underset{w}{\\text{maximize}}\\; P_w(y|x)$$\n",
      "\n",
      "+ **Discriminative models** are concerned in finding a good approximation of the decision function even if it means losing the information about the concrete description of the data. In this setting we may find Maximum Likelihood Estimated methods such as logistic regression and other explicit function models such as SVM."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook we will introduce two very simple examples of each of those families. Both models are linear in the parameters and yield very simple decision boundaries, linear or quadratic boundaries.\n",
      "\n",
      "Linear models are defined as $h: {\\bf R}^n \\rightarrow {\\bf R}$ such that $h(x)=a^T x + b$, where $a,x \\in {\\bf R}^n$ and $b\u00a0\\in {\\bf R}$. In this section we will review two very important models that are modeled as linear and apply them to a couple of problems."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#2 Naive Bayes\n",
      "\n",
      "In this section, the Naive Bayes classifier is introduced by studying the case of document classification as a simple instance of a Natural Language Processing problem. \n",
      "\n",
      "But before that, let us apply our programatic knowledge and check what the boundary using Naive Bayes looks like."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reset\n",
      "import numpy as np\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "import matplotlib.pyplot as plt\n",
      "#Create some data\n",
      "X = np.concatenate([1.25*np.random.randn(40,2),5+1.5*np.random.randn(40,2)]) \n",
      "y = np.concatenate([np.ones((40,1)),-np.ones((40,1))])\n",
      "\n",
      "nb = GaussianNB()\n",
      "nb.fit(X,y.ravel())\n",
      "\n",
      "#Create grid data for visualization purposes\n",
      "delta = 0.025\n",
      "xx = np.arange(-5.0, 10.0, delta)\n",
      "yy = np.arange(-5.0, 10.0, delta)\n",
      "XX, YY = np.meshgrid(xx, yy)\n",
      "\n",
      "Z=nb.predict_proba(np.c_[XX.ravel(), YY.ravel()])\n",
      "Z = Z[:,1].reshape(XX.shape)\n",
      "\n",
      "plt.figure()\n",
      "idxplus = y==1\n",
      "idxminus = y==-1\n",
      "idxplus = idxplus.flatten()\n",
      "idxminus = idxminus.flatten()\n",
      "plt.scatter(X[idxplus,0],X[idxplus,1],color='r',alpha=0.4)\n",
      "plt.scatter(X[idxminus,0],X[idxminus,1],color='b',alpha=0.4)\n",
      "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,10,-5,10),alpha=0.3, vmin=0, vmax=1)\n",
      "plt.contour(XX,YY,Z,[0.5])\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(9,9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">If the boundary is not linear why is it considered a linear model? It is not a linear model, though it is an affine model with respect to the weights. In the particular case of text classification we will use certain probability density functions that will make the model linear."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2.1 Basic document representation\n",
      "\n",
      "In text classification, we are given a description $x \\in {\\bf R}^d$ of a document $\\delta$ and a fixed set of classes $y \\in \\{c_1, \\dots, c_K\\}$, for example the document topic. Given a new document, our goal is to predict the most probable class.\n",
      "\n",
      "A very simple description of a document is the **bag-of-words** description. This representation transforms a complete text to a vector of $d$ predefined words. The set of predefined words is selected by the practicioner. For example, the list can consist of the set of all words in a given language. \n",
      "\n",
      "<b>Example 1:</b>\n",
      "Suppose we are given four different documents belonging to the topics $y=\\{\\text{'economics'},\\text{'technology'}\\}$ and we select as our representation the following bag-of-words $x = \\{\\text{'market'}, \\text{'stock'}, \\text{'price'}, \\text{'application'}, \\text{'mobile'}, \\text{'google'}\\}$. We can count the number of times a certain term appears in that document and expect that this description is discriminative enough for identifying the document topic. Check the following example:\n",
      "\n",
      "<table border=\"1\">\n",
      "<tr>\n",
      "<td></td>\n",
      "<td>market</td>\n",
      "<td>stock</td>\n",
      "<td>price</td>\n",
      "<td>application</td>\n",
      "<td>mobile</td>\n",
      "<td>google</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>document 1($\\text{'economics'}$)</td>\n",
      "<td>1</td>\n",
      "<td>2</td>\n",
      "<td>3</td>\n",
      "<td>0</td>\n",
      "<td>0</td>\n",
      "<td>0</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>document 2($\\text{'economics'}$)</td>\n",
      "<td>0</td>\n",
      "<td>1</td>\n",
      "<td>2</td>\n",
      "<td>0</td>\n",
      "<td>0</td>\n",
      "<td>1</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>document 3($\\text{'technology'}$)</td>\n",
      "<td>0</td>\n",
      "<td>0</td>\n",
      "<td>0</td>\n",
      "<td>2</td>\n",
      "<td>3</td>\n",
      "<td>1</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>document 4($\\text{'technology'}$)</td>\n",
      "<td>1</td>\n",
      "<td>0</td>\n",
      "<td>1</td>\n",
      "<td>2</td>\n",
      "<td>3</td>\n",
      "<td>0</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "In this representation, document 2 is represented by the vector (0,1,2,0,0,1). We can alternatively use a binary value representing whether a term appears or not in the document. In this last case document would be represesnted by (0,1,1,0,0,1).\n",
      "\n",
      "Observe that this is a context free representation, i.e. the order of the words is not considered. Consider the sentences \"Google reduces the prices of applications in App market\" and \"The number of aplications in Google App market with cheap prices is reduced by 20%\". The representation for both sentences is the same, though the exact meaning of both sentences is completely different. However, this kind of representation may be enough for identifying that both refers to $\\text{'technology'}$.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2.2 The Naive Bayes classifier\n",
      "\n",
      "Naive Bayes is an instance of a Bayessian classifier. In this framework, the problem of classification consists of selecting the class with Maximum A Posteriori (MAP) probability, i.e. $$\\hat{y} = \\arg\\max_y p(y|x).$$\n",
      "\n",
      "In order to find this quantity we use the Bayes equation,\n",
      "\n",
      "$$ p(x,y) = p(x|y)p(y) = p(y|x)p(x),$$\n",
      "\n",
      "and\n",
      "\n",
      "$$ p(y|x) = \\frac{p(x|y)p(y)}{p(x)}.$$\n",
      "\n",
      "In order to compute the MAP the quantities $p(x|y)$, $p(y)$, $p(x)$ have to be estimated from observed data.\n",
      "\n",
      "In the problem of document classification, our goal is to select the class with MAP probability. For example, we will select the cathegory $\\text{'economics'}$ for a text with description (1,1,1,0,0,0) only if $P(y = \\text{'economics'}|x = (1,1,1,0,0,0)) > P(y = \\text{'technology'}|x = (1,1,1,0,0,0))$. \n",
      "\n",
      "Note that $p(x)$ is a constant value and it does not affect the decision, thus we just need to compute\n",
      "\n",
      "$$P(y|x) \\propto P(y)P(x|y)$$\n",
      "\n",
      "Estimating the likelihood term, $P(x|y)$, accounts for computing the probability of certain description vector in a given class, e.g. the probability of a text in $\\text{'economics'}$ having a description $x = (1,1,1,0,0,0)$ (the value of the probability that a description x = (1,1,1,0,0,0) has inside the category $\\text{'economics'}$), $p(x = (1,1,1,0,0,0)|y = \\text{'economics'})$\n",
      "\n",
      "Up to this point, the description of the classifier is general for any Bayessian classifier. *Naive Bayes additionally assumes that $x$ is composed of a set of $d$ independent variables.* This allows to rewrite the likelihood term as\n",
      "$$p(x_1,x_2,...,x_N | y) = p(x_1|y)p(x_2|y)...p(x_N|y) = \\prod\\limits_{i=1}^N p(x_i|y)$$\n",
      "\n",
      "For example, in our case \n",
      "$$P(x = (1,1,1,0,0,0)|y = \\text{'tech'}) = P(x_1=1|y = \\text{'tech'})P(x_2=1|y = \\text{'tech'})P(x_3=1|y = \\text{'tech'})P(x_4=0|y = \\text{'tech'})P(x_5=0|y = \\text{'tech'})P(x_6=0|y = \\text{'tech'})$$\n",
      "\n",
      "This is understood as the fact that the probability of a document described as x = (1,1,1,0,0,0) is described by the product of the probilities that the first to the third word are present, and the fourth to the sixth word are not.\n",
      "\n",
      "In the end, the Naive Bayes classifier has the following form,\n",
      "$$p(y|x) \\propto p(y)\\prod\\limits_{i=1}^N p(x_i|y)$$\n",
      "\n",
      "\n",
      "In many cases the prior $p(y)$ is unknown or simply we prefer to use a non-informative prior (all documents have the same probability of appearance in our context ($p(y)$)). In that case the formulation is simplified to the Maximum Likelihood Estimate."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2.3 Estimating conditioned probabilities \n",
      "\n",
      "The last remaining step is the estimation of the individual conditional probabilities. There are two classical variants the **Multinomial Naive Bayes** and the **Bernoulli Naive Bayes**. The difference between both lies in the goal of what they are modeling. **In Multinomial NB we compute the probability of generating the observed document.** In this sense, we multiply the conditional probability of each word in the document for all words present in the document. An alternative view is the *Bernoulli model*. **In the Bernoulli Naive Bayes we compute the probability of the binary bag-of-words descriptor.** Observe that in the Bernouilli Naive Bayes the final probability depends on the words that appear in the document but also on the words that do not appear while in the multinomial NB it only depends on the words that appear. On the contrary, multinomial naive bayes takes into account the multiplicity of the words in the document while Bernoulli does not. Let us consider in this example the *Bernoulli model* that is consistent with our representation where a zero indicates a word is not present in the document and a one represents it is present. In order to estimate this probability we can use a frequentist approximation to probability, i.e. we will estimate the probability as the frequency of appearance of each term in each category. This computation divides the number documents where the word appears over the total number of documents. \n",
      "\n",
      "In our previous example, $p(x_3=1 (\\text{the word 'price' appears})|y =\\text{'tech'}) = 1/2$ and $p(x_3=1 (\\text{the word 'price' appears})|y =\\text{'eco'}) = 2/2$. This is computed by dividing the number of documents where the word price appear in a given category over the number of documents of that category.\n",
      "\n",
      "### 2.3.1 The zero probability effect\n",
      "In the former example the probability $p(x_5=1|y=\\text{'eco'}) = 0$. This implies that if the word 'mobile' appears the document can not belong to the class $\\text{'economy'}$. It is unreasonable to completely penalize a whole class by the appearance or not appearance of a single word. It is customary to assign to those cases a very low probability value instead. One well known approach to correct this effect is the so called **Laplace correction**. It is computed as follows,\n",
      "\n",
      "$$p(x_i=1 | y=c_k ) = \\frac{\\text{# of documents of class } c_k \\text{ where word } x_i \\text{ appears} + 1}{\\text{# of documents of class } c_k + M}$$\n",
      "\n",
      "where $M$ is the amount of words in the description. \n",
      "\n",
      "### 2.3.2 Underflow effect\n",
      "\n",
      "As the number of words in the description increase there is a higher probability that many of those words will not be present in the document. The product of many very small values may lead to floating point underflow effects. For this reason it is usual to use the log probability instead. This transformation does not change the decision boundary. In our simplified case\n",
      "\n",
      "$$\\log p(x|y) = \\sum\\limits_{i=1}^N \\log p(x_i|y)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2.2 Applying Naive Bayes to text classification\n",
      "\n",
      "<center><img src=\"files/The_New_York_Times_logo.png\"></center>\n",
      "In this example, our goal is to automatically categorize news according to their title into twenty-eight standard topics. In this problem we will deal with every New York Times front page story from 1996 to 2006, coded according to the Policy Agendas (http://www.policyagendas.org). This collection of data has been compiled by Amber E. Boydstun.\n",
      "\n",
      "Specifically, we are interested in classifying news from The New York Times in the following macro-topics according to its title:\n",
      "\n",
      "\n",
      "\n",
      "<table border=\"1\">\n",
      "<tr>\n",
      "<td>\n",
      "1 \n",
      "<td>\n",
      "Macroeconomics\n",
      "<tr>\n",
      "<td>\n",
      "2 \n",
      "<td>\n",
      "Civil Rights, Minority Issues, and Civil Liberties \n",
      "<tr>\n",
      "<td>\n",
      "3\n",
      "<td>\n",
      "Health\n",
      "<tr>\n",
      "<td>\n",
      "4 \n",
      "<td>Agriculture\n",
      "<tr>\n",
      "<td>\n",
      "5 \n",
      "<td>Labor, Employment, and Immigration\n",
      "<tr>\n",
      "<td>\n",
      "6 \n",
      "<td> Education\n",
      "<tr>\n",
      "<td>\n",
      "7\n",
      "<td>Environment\n",
      "<tr>\n",
      "<td>\n",
      "8\n",
      "<td>Energy\n",
      "<tr>\n",
      "<td>\n",
      "10 \n",
      "<td>Transportation\n",
      "<tr>\n",
      "<td>\n",
      "12 \n",
      "<td>Law, Crime, and Family Issues\n",
      "<tr>\n",
      "<td>\n",
      "13 \n",
      "<td>Social Welfare\n",
      "<tr>\n",
      "<td>\n",
      "14 \n",
      "<td>Community Development and Housing Issues\n",
      "<tr>\n",
      "<td>\n",
      "15 \n",
      "<td>Banking, Finance, and Domestic Commerce\n",
      "<tr>\n",
      "<td>\n",
      "16 \n",
      "<td>Defense\n",
      "<tr>\n",
      "<td>\n",
      "17 \n",
      "<td>Space, Science, Technology and Communications\n",
      "<tr>\n",
      "<td>\n",
      "18 \n",
      "<td>Foreign Trade\n",
      "<tr>\n",
      "<td>\n",
      "19 \n",
      "<td>International Affairs and Foreign Aid\n",
      "<tr>\n",
      "<td>\n",
      "20 \n",
      "<td>Government Operations\n",
      "<tr>\n",
      "<td>\n",
      "21 \n",
      "<td>Public Lands and Water Management\n",
      "<tr>\n",
      "<td>\n",
      "24 \n",
      "<td>State and Local Government Administration\n",
      "<tr>\n",
      "<td>\n",
      "26 \n",
      "<td>Weather and Natural Disasters\n",
      "<tr>\n",
      "<td>\n",
      "27 \n",
      "<td>Fires\n",
      "<tr>\n",
      "<td>\n",
      "28 \n",
      "<td>Arts and Entertainment\n",
      "<tr>\n",
      "<td>\n",
      "29 \n",
      "<td>Sports and Recreation\n",
      "<tr>\n",
      "<td>\n",
      "30 \n",
      "<td>Death Notices\n",
      "<tr>\n",
      "<td>\n",
      "31 \n",
      "<td>Churches and Religion\n",
      "<tr>\n",
      "<td>\n",
      "99 \n",
      "<td>Other, Miscellaneous, and Human Interest\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reset -f\n",
      "#load data\n",
      "import pandas as pd\n",
      "data=pd.read_csv('./files/Boydstun_NYT_FrontPage_Dataset_1996-2006_0.csv')\n",
      "data.head()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us split the data set in two set: \n",
      "    \n",
      "+ We will train the classifier with news up to 2004.\n",
      "+ We will test the classifier in news from 2005 and 2006."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "#Let us train the classifier with data up to 1/1/2004 and test its performnace in data from 2004-2006\n",
      "split = pd.to_datetime(pd.Series(data['Date']))<pd.datetime(2004, 1, 1)\n",
      "raw_data = data['Title']\n",
      "raw_train = raw_data[split]\n",
      "raw_test = raw_data[np.logical_not(split)]\n",
      "y = data['Topic_2digit']\n",
      "y_train = y[split]\n",
      "y_test = y[np.logical_not(split)]\n",
      "print 'Check the split sizes, train, test and total amount of data:'\n",
      "print raw_train.shape, raw_test.shape, raw_data.shape\n",
      "print 'Display the labels:'\n",
      "print np.unique(y)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let us tokenize the data\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# We use the count number of instances considering that a word has a minimum support of two documents\n",
      "vectorizer = CountVectorizer(min_df=2, \n",
      "# stop words such as 'and', 'the', 'of' are removed                             \n",
      " stop_words='english', \n",
      " strip_accents='unicode')\n",
      "\n",
      "#example of the tokenization\n",
      "test_string = unicode(raw_train[0])\n",
      "print \"Example: \" + test_string +\"\\n\"\n",
      "print \"Preprocessed: \" + vectorizer.build_preprocessor()(test_string)+\"\\n\"\n",
      "print \"Tokenized:\" + str(vectorizer.build_tokenizer()(test_string))+\"\\n\"\n",
      "print \"Analyzed data string:\" + str(vectorizer.build_analyzer()(test_string))+\"\\n\"\n",
      "\n",
      "\n",
      "#Process and convert data\n",
      "X_train = vectorizer.fit_transform(raw_train)\n",
      "X_test = vectorizer.transform(raw_test)\n",
      "\n",
      "print \"Number of tokens: \" + str(len(vectorizer.get_feature_names())) +\"\\n\"\n",
      "print \"Extract of tokens:\"\n",
      "print vectorizer.get_feature_names()[1000:1100]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Fit a Bernoulli Naive Bayes\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "nb = BernoulliNB()\n",
      "nb.fit(X_train,y_train)\n",
      "\n",
      "y_hat = nb.predict(X_test)\n",
      "from sklearn import metrics\n",
      "import matplotlib.pyplot as plt\n",
      "def plot_confusion_matrix(y_pred, y):\n",
      "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest')\n",
      "    plt.colorbar()\n",
      "    plt.ylabel('true value')\n",
      "    plt.xlabel('predicted value')\n",
      "    fig = plt.gcf()\n",
      "    fig.set_size_inches(9,9)    \n",
      "    \n",
      "print \"classification accuracy:\", metrics.accuracy_score(y_hat, y_test)\n",
      "plot_confusion_matrix(y_hat, y_test)\n",
      "print \"Classification Report:\"\n",
      "print metrics.classification_report(y_hat,np.array(y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\">**QUESTION:** Identify the three most simple classes.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Save data for future use.\n",
      "import pickle\n",
      "ofname = open('NYT_data.pkl', 'wb')\n",
      "s = pickle.dump([X_train,y_train,X_test,y_test],ofname)\n",
      "ofname.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#What are the top N most predictive features per class?\n",
      "N = 5\n",
      "voc = vectorizer.get_feature_names()\n",
      "for i, label in enumerate(np.unique(y)):\n",
      "    topN = np.argsort(nb.coef_[i])[-N:]\n",
      "    print 'Code: '+ str(label) + ' Terms : '+ str([voc[i] for i in topN])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us check what would happen if we enrich the data set with the summary of the article."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_data = data['Title']+data['Summary']\n",
      "raw_train = raw_data[split]\n",
      "raw_test = raw_data[np.logical_not(split)]\n",
      "y = data['Topic_2digit']\n",
      "y_train = y[split]\n",
      "y_test = y[np.logical_not(split)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let us tokenize the data\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "vectorizer = CountVectorizer(min_df=2, \n",
      " stop_words='english', \n",
      " strip_accents='unicode')\n",
      "\n",
      "#example\n",
      "test_string = unicode(raw_train[0])\n",
      "print \"Example: \" + test_string +\"\\n\"\n",
      "print \"Preprocessed: \" + vectorizer.build_preprocessor()(test_string)+\"\\n\"\n",
      "print \"Tokenized:\" + str(vectorizer.build_tokenizer()(test_string))+\"\\n\"\n",
      "print \"Analyzed data string:\" + str(vectorizer.build_analyzer()(test_string))+\"\\n\"\n",
      "\n",
      "\n",
      "#Fit and convert data\n",
      "X_train = vectorizer.fit_transform(raw_train)\n",
      "X_test = vectorizer.transform(raw_test)\n",
      "\n",
      "print \"\\n\"\n",
      "print \"Number of tokens: \" + str(len(vectorizer.get_feature_names())) +\"\\n\"\n",
      "print \"Extract of tokes:\"\n",
      "print vectorizer.get_feature_names()[1000:1100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import BernoulliNB\n",
      "nb = BernoulliNB()\n",
      "nb.fit(X_train,y_train)\n",
      "\n",
      "y_hat = nb.predict(X_test)\n",
      "from sklearn import metrics\n",
      "import matplotlib.pyplot as plt\n",
      "def plot_confusion_matrix(y_pred, y):\n",
      "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest')\n",
      "    plt.colorbar()\n",
      "    plt.ylabel('true value')\n",
      "    plt.xlabel('predicted value')\n",
      "    fig = plt.gcf()\n",
      "    fig.set_size_inches(9,9)    \n",
      "    \n",
      "print \"classification accuracy:\", metrics.accuracy_score(y_hat, y_test)\n",
      "plot_confusion_matrix(y_hat, y_test)\n",
      "print \"Classification Report:\"\n",
      "print metrics.classification_report(y_hat,np.array(y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Save data for future use.\n",
      "import pickle\n",
      "ofname = open('NYT_context_data.pkl', 'wb')\n",
      "s = pickle.dump([X_train,y_train,X_test,y_test],ofname)\n",
      "ofname.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#What are the top N most predictive features per class?\n",
      "N = 5\n",
      "voc = vectorizer.get_feature_names()\n",
      "for i, label in enumerate(np.unique(y)):\n",
      "    topN = np.argsort(nb.coef_[i])[-N:]\n",
      "    print 'Code: '+ str(label) + ' Terms : '+ str([voc[i] for i in topN])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Observe that adding the small summary improves the recognition rate by $10\\%$. \n",
      "\n",
      "As a side note, Naive Bayes with these models creates a linear decision boundary. For this reason, sometimes NB is called a linear classifier."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 3. Support Vector Machines\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Support Vector Machines (SVM) is a prototypical example of discriminative learning. In this setting one explicitly assumes a function model class of the boundary. The classical model for SVM is a linear model. SVM is not the only discriminative linear model, e.g. perceptron, logistic classifier, etc. But, it is probably the most complete problem formulation.\n",
      "\n",
      "Let us first check the intuition behind SVM,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "%reset\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from IPython.html.widgets import interact\n",
      "\n",
      "class HLA():\n",
      "    def __init__(self):\n",
      "        np.random.seed(1)\n",
      "        self.X = np.concatenate([1.25*np.random.randn(40,2),5+1.5*np.random.randn(40,2)]) \n",
      "        self.y = np.concatenate([np.ones((40,1)),-np.ones((40,1))])\n",
      "        plt.scatter(self.X[0:40,0],self.X[0:40,1],color='r')\n",
      "        plt.scatter(self.X[40:,0],self.X[40:,1],color='b') \n",
      "        delta = 0.025\n",
      "        xx = np.arange(-5.0, 10.0, delta)\n",
      "        yy = np.arange(-5.0, 10.0, delta)\n",
      "        XX, YY = np.meshgrid(xx, yy)\n",
      "        Xf = XX.flatten()\n",
      "        Yf = YY.flatten()\n",
      "        self.sz=XX.shape\n",
      "        self.data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
      "\n",
      "    def run(self,w0,w1,offset):\n",
      "        w=np.array([w0,w1])\n",
      "        w.shape=(2,1)\n",
      "        Z = self.data.dot(w)+offset\n",
      "        Z.shape=self.sz\n",
      "        plt.scatter(self.X[0:40,0],self.X[0:40,1],color='r')\n",
      "        plt.scatter(self.X[40:,0],self.X[40:,1],color='b')\n",
      "        plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,10,-5,10),alpha=0.3, vmin=-30, vmax=30)\n",
      "        XX = self.data[:,0].reshape(self.sz)\n",
      "        YY = self.data[:,1].reshape(self.sz)\n",
      "        plt.contour(XX,YY,Z,[0])\n",
      "        fig = plt.gcf()\n",
      "        fig.set_size_inches(9,9)\n",
      "   \n",
      "\n",
      "def decorator(w0,w1,offset):\n",
      "    widget_hla.run(w0,w1,offset)\n",
      "    \n",
      "widget_hla = HLA()\n",
      "interact(decorator, w0=(-10.,10.), w1=(-10.,10.), offset=(-20.,40.));\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\">**QUESTION:** Using the former widget, check manually the following configurations:\n",
      "\n",
      "<li> $(w_0,w_1,\\text{offset}) = (-1.7, -3.1, 10)$\n",
      "<li> $(w_0,w_1,\\text{offset}) = (-3.7, -0.5, 10.3)$\n",
      "<li> $(w_0,w_1,\\text{offset}) = (-7.5, -3.2, 28.8)$\n",
      "<p>\n",
      "Which one of those configuration do you think yields a better boundary? Why?\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-info\">\n",
      "**INTUITION:** The Support Vector Machine classifer finds the boundary with maximum distance/**margin** to both classes.</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Observations:\n",
      "- It implicitly models the notion of noise. One expects that the boundary with maximum margin will be robust to small perturbations in the data.\n",
      "- A maximum margin classifier has a unique solution in the separable case."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us check the result of fitting a SVM classifier using sklearn:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "%reset\n",
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from IPython.html.widgets import interact\n",
      "from sklearn import svm\n",
      "\n",
      "class svm_example():\n",
      "    def __init__(self):\n",
      "        '''Data creation'''\n",
      "        np.random.seed(1)\n",
      "        self.X = np.concatenate([1.25*np.random.randn(40,2),5+1.5*np.random.randn(40,2)]) \n",
      "        self.y = np.concatenate([np.ones((40,1)),-np.ones((40,1))])\n",
      "\n",
      "    def run(self):\n",
      "        '''Fit a linear SVM'''\n",
      "        self.clf = svm.SVC(kernel='linear')\n",
      "        self.clf.fit(self.X,self.y.ravel())\n",
      "        \n",
      "    def display(self):\n",
      "        '''Display stuff'''\n",
      "        delta = 0.25\n",
      "        xx = np.arange(-5.0, 10.0, delta)\n",
      "        yy = np.arange(-5.0, 10.0, delta)\n",
      "        XX, YY = np.meshgrid(xx, yy)\n",
      "        Xf = XX.flatten()\n",
      "        Yf = YY.flatten()\n",
      "        sz=XX.shape\n",
      "        data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
      "        Z=self.clf.decision_function(data)\n",
      "        Z.shape=sz\n",
      "        plt.scatter(self.X[0:40,0],self.X[0:40,1],color='r')\n",
      "        plt.scatter(self.X[40:,0],self.X[40:,1],color='b')\n",
      "        plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,10,-5,10),alpha=0.3, vmin=-3, vmax=3)\n",
      "        XX = data[:,0].reshape(sz)\n",
      "        YY = data[:,1].reshape(sz)\n",
      "        plt.contour(XX,YY,Z,[-1,0,1],colors=['b','k','r'])\n",
      "        fig = plt.gcf()\n",
      "        fig.set_size_inches(9,9)\n",
      "        print 'Number of support vectors: ' + str(np.sum(self.clf.n_support_))\n",
      "        plt.scatter(self.clf.support_vectors_[:, 0], \n",
      "           self.clf.support_vectors_[:, 1], \n",
      "           s=120, \n",
      "           facecolors='none', \n",
      "           linewidths=2,\n",
      "           zorder=10)\n",
      "        print '(w0,w1) = ' + str(10*self.clf.coef_[0])\n",
      "        print 'offset = ' + str(10*self.clf.intercept_[0])\n",
      "        return XX,YY,Z\n",
      "\n",
      "\n",
      "\n",
      "c = svm_example()\n",
      "c.run()\n",
      "XX,YY,Z=c.display()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Observe that there is a critical subset of data points. These are called **Support Vectors**. If any of those points disappear the boundary changes.  The decision boundary depends on the support vectors, thus we have to store them in our model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check the intuition in 3D:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib import cm\n",
      "np.random.seed(1)\n",
      "X = np.concatenate([1.25*np.random.randn(40,2),5+1.5*np.random.randn(40,2)]) \n",
      "y = np.concatenate([np.ones((40,1)),-np.ones((40,1))])\n",
      "def control3D(elevation,azimuth):\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111, projection='3d')\n",
      "    fig.set_size_inches(12,12)\n",
      "    ax.plot_surface(XX,YY,Z,cmap=cm.coolwarm,alpha=0.3,linewidth=0)\n",
      "    ax.scatter(X[0:40,0],X[0:40,1],1,color='r')\n",
      "    ax.scatter(X[40:,0],X[40:,1],-1,color='b')\n",
      "    ax.contour(XX,YY,Z,[-1,0,1],colors=['b','k','r'])\n",
      "    ax.view_init(elev=elevation, azim=azimuth)\n",
      "\n",
      "#Ipython 2.0\n",
      "interact(control3D,elevation=(0.,90.),azimuth=(0,360))\n",
      "#Ipython 1.1\n",
      "#elevation = 45\n",
      "#azimuth = 180\n",
      "#control3D(elevation,azimuth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\">**QUESTIONS: **\n",
      "<li> Set the azimuth to $113$ and elevation to $0$. Observe the data points and the relative position of the hyperplane. \n",
      "<li> Change the elevation to $90$. Describe this projection.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3.1 Modeling the Support Vector Machine."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.1.1 Geometry of the hyperplane\n",
      "A hyperplane in ${\\bf R}^d$ is defined as an affine combination of the variables: $\\pi\\equiv a^Tx + b = 0$. \n",
      "\n",
      "Features:\n",
      "\n",
      "+ A hyperplane splits the space in two half-spaces. The evaluation of the equation of the hyperplane on any element of one of the half-space is a positive value. It is a negative value for all the elements in the other half-space.\n",
      "+ The distance of a point $x \\in{\\bf R}^d$ to the hyperplane $\\pi$ is \n",
      "$$d(x,\\pi)=\\frac{a^Tx+b}{\\|a\\|_2}$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.1.2 Modeling the separating hyperplane\n",
      "Given a binary classification problem with training data $\\mathcal{D}=\\{(x_i,y_i)\\},\\; i=1\\dots N, \\; y_i\\in\\{+1,-1\\} $. Consider $\\mathcal{S} \\subseteq \\mathcal{D}$ the subset of all data points belonging to class $+1$, $\\mathcal{S}=\\{x_i | y_i=+1\\}$, and $\\mathcal{R}=\\{x_i | y_i=-1\\}$ its complement. \n",
      "\n",
      "Then the problem of finding a separating hyperplane consists of fulfilling the following constraints\n",
      "\n",
      "$$a^Ts_i+b>0\\; \\text{and}\\; a^Tr_i+b<0 \\quad \\forall s_i\\in\\mathcal{S}, r_i\\in\\mathcal{R}.$$\n",
      "\n",
      "Note the strict inequalities in the formulation. Informally, we can consider the smallest satisfied constraint. And observe that the rest must be satisfied with a larger value. Thus, we can arbitrarily set that value to 1 and rewrite the problem as $$a^Ts_i+b\\geq 1\\; \\text{and}\\; a^Tr_i+b\\leq -1.$$\n",
      "\n",
      "This is a *feasibility problem* and it is usually written in the following way in optimization standard notation\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\text{minimize} & 1\\\\\n",
      "\\text{subject to} & a^T r_i + b \\leq -1,\\; \\forall r_i \\in \\mathcal{R}\\\\\n",
      "& a^T s_i + b \\geq 1\\; \\forall s_i \\in \\mathcal{S}\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "or in a compact way\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\text{minimize} & 1\\\\\n",
      "\\text{subject to} & y_i (a^T x_i + b) \\geq 1,\\; \\forall x_i \\in \\mathcal{D}\\\\\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "The solution of this problem is not unique, e.g. remember all the parameters of the 'Human Learning Algorithm'.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.1.3 The maximum margin hyperplane\n",
      "\n",
      "Selecting the maximum margin hyperplane requires to add a new constraint to our problem. Remember from the geometry of the hyperplane that the distance of any point to a hyperplane is given by $d(x,\\pi)=\\frac{a^Tx+b}{\\|a\\|_2}$. \n",
      "\n",
      "Recall that we want positive data to be beyond value 1 and negative data below -1. Thus, what is the distance value we want to maximize?\n",
      "\n",
      "The positive point closest to the boundary is at $1/\\|a\\|_2$ and the negative point closest to the boundary data point is also at $1/\\|a\\|_2$. Thus data points from different classes are at least $2/\\|a\\|_2$ apart. \n",
      "\n",
      "Recall that our goal is to find the separating hyperplane with maximum margin, i.e. with maximum distance among elements from different classes. Thus, we can complete the former formulation with our last requirement as follows\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\text{maximize} & 2/\\|a\\|_2 \\\\\n",
      "\\text{subject to} & y_i (a^T x_i + b) \\geq 1,\\; \\forall x_i \\in \\mathcal{D}\\\\\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "or equivalently,\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\text{minimize} & \\|a\\|_2/2 \\\\\n",
      "\\text{subject to} & y_i (a^T x_i + b) \\geq 1,\\; \\forall x_i \\in \\mathcal{D}\\\\\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "This formulation has a solution as long as the problem is linearly separable."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.1.4 Dealing with the non-separable case\n",
      "\n",
      "In order to deal with misclassifications, we are going to introduce a new set of variables $\\xi_i$, that represents the amount of violation in the $i-th$ constraint. If the constraint is already satisfied, then $\\xi_i=0$, and $\\xi_i>0$ otherwise. Because $\\xi_i$ is related to the errors, we would like to keep this amount as close so zero as possible. This makes us introduce a element in the objective trading-off with the maximum margin.\n",
      "\n",
      "The new model becomes\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\text{minimize} & \\|a\\|_2/2 + C \\sum\\limits_{i=1}^N \\xi_i\\\\\n",
      "\\text{subject to} & y_i (a^T x_i + b) \\geq 1 - \\xi_i,\\; i=1\\dots N\\\\\n",
      "& \\xi_i\\geq 0\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "where $C$ is the trade-off parameter that roughly balances margin and misclassification rate. This formulation is also called **soft-margin SVM**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-info\">**Take home ideas:**\n",
      "<ul>\n",
      "<li> Classical SVM fits a hyperplane separating boundary. </li>\n",
      "<li> The hyperplane is defined to achieve the maximum margin. </li>\n",
      "<li> If the problem is not linearly separable a new term related to the misclassification performance is introduced that trades-off with the margin. This trade-off is governed by parameter $C$ (or $\\nu$ in $\\nu$-SVM). </li>\n",
      "</ul>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.1.5 The New York Times problem again\n",
      "\n",
      "Let us now apply our knowledge to the New York Times headlines topic prediction. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Recover NTY data\n",
      "import pickle\n",
      "fname = open('NYT_data.pkl','rb')\n",
      "data = pickle.load(fname)\n",
      "X_train = data[0]\n",
      "y_train = data[1]\n",
      "X_test = data[2]\n",
      "y_test = data[3]\n",
      "print 'Loading ok.'\n",
      "\n",
      "from sklearn import svm\n",
      "clf = svm.LinearSVC()\n",
      "clf.fit(X_train,y_train)\n",
      "\n",
      "y_hat = clf.predict(X_test)\n",
      "from sklearn import metrics\n",
      "import matplotlib.pyplot as plt\n",
      "def plot_confusion_matrix(y_pred, y):\n",
      "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest')\n",
      "    plt.colorbar()\n",
      "    plt.ylabel('true value')\n",
      "    plt.xlabel('predicted value')\n",
      "    fig = plt.gcf()\n",
      "    fig.set_size_inches(9,9)    \n",
      "    \n",
      "print \"classification accuracy:\", metrics.accuracy_score(y_hat, y_test)\n",
      "plot_confusion_matrix(y_hat, y_test)\n",
      "print \"Classification Report:\"\n",
      "print metrics.classification_report(y_hat,np.array(y_test))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Recover NTY data\n",
      "import pickle\n",
      "fname = open('NYT_context_data.pkl','rb')\n",
      "data = pickle.load(fname)\n",
      "X_train = data[0]\n",
      "y_train = data[1]\n",
      "X_test = data[2]\n",
      "y_test = data[3]\n",
      "print 'Loading ok.'\n",
      "\n",
      "from sklearn import svm\n",
      "clf = svm.LinearSVC()\n",
      "clf.fit(X_train,y_train)\n",
      "\n",
      "y_hat = clf.predict(X_test)\n",
      "from sklearn import metrics\n",
      "import matplotlib.pyplot as plt\n",
      "def plot_confusion_matrix(y_pred, y):\n",
      "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest')\n",
      "    plt.colorbar()\n",
      "    plt.ylabel('true value')\n",
      "    plt.xlabel('predicted value')\n",
      "    fig = plt.gcf()\n",
      "    fig.set_size_inches(9,9)    \n",
      "    \n",
      "print \"classification accuracy:\", metrics.accuracy_score(y_hat, y_test)\n",
      "plot_confusion_matrix(y_hat, y_test)\n",
      "print \"Classification Report:\"\n",
      "print metrics.classification_report(y_hat,np.array(y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the default parameters we can improve the recognition rate by $10\\%$. However we can not check the most important words. Can we find a better trade-off?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Let us check a cross-validation grid search\n",
      "\n",
      "from sklearn import grid_search\n",
      "parameters = {'C':[0.01, 0.05, 0.1, 0.5, 1, 10]}\n",
      "svc= svm.LinearSVC()\n",
      "clf = grid_search.GridSearchCV(svc, parameters)\n",
      "clf.fit(X_train,y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'The best parameterization is ' + str(clf.best_params_)\n",
      "print 'The achieved score is ' + str(clf.best_score_)\n",
      "\n",
      "print 'Checking the rest of the scores \\n'\n",
      "import matplotlib.pyplot as plt\n",
      "d = [clf.grid_scores_[i][1] for i in xrange(6)]\n",
      "print d\n",
      "plt.plot(d,'r',marker='o')\n",
      "ax = plt.gca()\n",
      "ax.set_xticklabels([0.01, 0.05, 0.1, 0.5, 1, 10])    \n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_hat = clf.predict(X_test)\n",
      "from sklearn import metrics\n",
      "import matplotlib.pyplot as plt\n",
      "def plot_confusion_matrix(y_pred, y):\n",
      "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest')\n",
      "    plt.colorbar()\n",
      "    plt.ylabel('true value')\n",
      "    plt.xlabel('predicted value')\n",
      "    fig = plt.gcf()\n",
      "    fig.set_size_inches(9,9)    \n",
      "    \n",
      "print \"classification accuracy:\", metrics.accuracy_score(y_hat, y_test)\n",
      "plot_confusion_matrix(y_hat, y_test)\n",
      "print \"Classification Report:\"\n",
      "print metrics.classification_report(y_hat,np.array(y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}