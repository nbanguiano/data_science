{
 "metadata": {
  "name": "",
  "signature": "sha256:1399d18b39b84535ff556e4f566acf7cf1b9e579238a8bca793d2d8a2827126d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>May 2014 - This notebook was created by [Oriol Pujol Vila](http://www.maia.ub.es/~oriol). Source and license info are in the folder.</i></small>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Machine Learning 2: Non-linear models and ensemble learning\n",
      "\n",
      "1. Non-linear models\n",
      "    * Nearest neighbor rule.\n",
      "    * Decision Trees.\n",
      "    * Kernel learning.\n",
      "\n",
      "2. Ensemble learning\n",
      "    * Introduction to ensemble learning\n",
      "    * Bagging and Random Forest\n",
      "    * Introduction to the multiclass problem and Error Correcting Output Coding"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#1. Non-linear models in Machine Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In our last case study, we shown that linear models can achive very good results. This is particularly true in datasets with large dimensionality. In those scenarios, the ratio between data and number of dimensions can be quite small. As a result we have a set of data spread out and it is very likely a linear model will be enough. However, it we have large amounts of data, this will not be enough. In those scenarios we may have to use non-linear models. \n",
      "\n",
      "In this notebook, we will introduce two concepts non-linearity and multi-class. Three models will be covered, Nearest Neighbors, Decision Trees and Kernel learning. We will apply these models to the predict **Customer Churn**. \n",
      "\n",
      "Let us first introduce the case study"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#CASE STUDY: Customer Churn Analysis\n",
      "\n",
      "Modeling churn means to understand what keeps the customer engaged to our product. Its analysis goal is to predict or describe the **churn rate** i.e. the rate at which customer leave or cease the subscription to a service. Its value lies in the fact that engaging new customers is often more costly than retaining existing ones. For that reason subscription business-based companies usually have proactive policies towards customer retention.\n",
      "\n",
      "In this case study, we aim at building a machine learning based model for customer churn prediction on data from a Telecom company. Each row on the dataset represents a subscribing telephone customer. Each column contains customer attributes such as phone number, call minutes used during different times of day, charges incurred for services, lifetime account duration, and whether or not the customer is still a customer.\n",
      "\n",
      "This case is partially inspired in Eric Chiang's analysis of churn rate. Data is available from the University of California Irvine machine learning repositories data set.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The complete set of attributes is the following:\n",
      "\n",
      "+ State: categorical, for the 50 states and the District of Columbia\n",
      "+ Account length: integer-valued, how long an account has been active \n",
      "+ Area code: categorical\n",
      "+ Phone number: customer ID\n",
      "+ International Plan: binary feature, yes or no\n",
      "+ VoiceMail Plan: binary feature, yes or no\n",
      "+ Number of voice mail messages: integer-valued\n",
      "+ Total day minutes: continuous, minutes customer used service during the day\n",
      "+ Total day calls: integer-valued\n",
      "+ Total day charge: continuous\n",
      "+ Total evening minutes: continuous, minutes customer used service during the evening\n",
      "+ Total evening calls: integer-valued\n",
      "+ Total evening charge: continuous\n",
      "+ Total night minutes: continuous, minutes customer used service during the night\n",
      "+ Total night calls: integer-valued\n",
      "+ Total night charge: continuous\n",
      "+ Total international minutes: continuous, minutes customer used service to make international calls\n",
      "+ Total international calls: integer-valued\n",
      "+ Total international charge: continuous\n",
      "+ Number of calls to customer service: integer-valued"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "churn_df = pd.read_csv('files/churn.csv')\n",
      "col_names = churn_df.columns.tolist()\n",
      "\n",
      "print \"Column names:\"\n",
      "print col_names\n",
      "\n",
      "to_show = col_names[:6] + col_names[-6:]\n",
      "\n",
      "print \"\\nSample data:\"\n",
      "churn_df[to_show].head(6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Isolate target data\n",
      "churn_result = churn_df['Churn?']\n",
      "y = np.where(churn_result == 'True.',1,0)\n",
      "\n",
      "# We don't need these columns\n",
      "to_drop = ['State','Phone','Churn?']\n",
      "churn_feat_space = churn_df.drop(to_drop,axis=1)\n",
      "\n",
      "# 'yes'/'no' has to be converted to boolean values\n",
      "# NumPy converts these from boolean to 1. and 0. later\n",
      "yes_no_cols = [\"Int'l Plan\",\"VMail Plan\"]\n",
      "churn_feat_space[yes_no_cols] = churn_feat_space[yes_no_cols] == 'yes'\n",
      "\n",
      "# Pull out features for future use\n",
      "features = churn_feat_space.columns\n",
      "\n",
      "X = churn_feat_space.as_matrix().astype(np.float)\n",
      "\n",
      "print \"Feature space holds %d observations and %d features\" % X.shape\n",
      "print \"Unique target labels:\", np.unique(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "plt.pie(np.c_[len(y)-np.sum(y),np.sum(y)][0],labels=['No Churn','Churn'],colors=['r','g'],shadow=True,autopct ='%.2f' )\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(6,6)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Save data for future use.\n",
      "import pickle\n",
      "ofname = open('churn_data.pkl', 'wb')\n",
      "s = pickle.dump([X,y,features],ofname)\n",
      "ofname.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\">**QUESTION: ** This kind of datasets are called **unbalanced** datasets. Name a trivial classifier with \"good\" accuracy in this data set?\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-info\">\n",
      "**Unbalanced datasets**\n",
      "<p>\n",
      "The unbalanced term describes the condition of the data where the ratio between the sizes of the positive and negative is a small value. In those scenarios, always predicting the majority class usually yields good accuracy performance, though it is ill informative. This kind of problems is very common when we want to model unusual events such as rare diseases, the occurrence of a failure in machinery, credit card fraud operations, etc. In those scenarios gathering data from usual events is very easy but collecting data from unusual events is difficult and results in a comparatively small size data set. In order to measure the performance on those data sets one has to use other performance metrics, such as specificity or positive predictive value on the minority class. In the end, the value of a misclassification of a sample depends on the application and the user. For example, in cancer detection because the cost of missing one patient in a trial is very large, we want the predictor to have very large sensitivity (we do not accept false negatives) though it means accepting more false positives. These false positives can be discarded in subsequent tests. \n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#2. Nearest Neighbors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nearest neighbors is a member of the instance based learning and lazy Learning families. Instance based models base the model on the evaluation of a function that depends on the point we are querying and training data. Nearest Neighbors is the **simplest** of these techniques. The rationale behind this model is as follows: Each training data set can be seen as a solved case/problem. Thus, given a new problem instance we may retrieve the most *similar* case in our data set and apply the same solution. In the case of classification, this means that we select the label of the most similar data example in our training set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Let's see what the boundary looks like in a toy problem.\n",
      "\n",
      "MAXN=10\n",
      "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
      "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
      "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
      "y = np.concatenate([y,np.ones((MAXN,1))])\n",
      "idxplus = y==1\n",
      "idxminus = y==-1\n",
      "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
      "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
      "\n",
      "from sklearn import cross_validation\n",
      "from sklearn import neighbors\n",
      "from sklearn import metrics\n",
      "\n",
      "delta = 0.05\n",
      "xx = np.arange(-5.0, 15.0, delta)\n",
      "yy = np.arange(-5.0, 15.0, delta)\n",
      "XX, YY = np.meshgrid(xx, yy)\n",
      "Xf = XX.flatten()\n",
      "Yf = YY.flatten()\n",
      "sz=XX.shape\n",
      "data = np.c_[Xf[:,np.newaxis],Yf[:,np.newaxis]];\n",
      "\n",
      "#Evaluate the model for a given weight\n",
      "clf = neighbors.KNeighborsClassifier(1)\n",
      "clf.fit(X,y.ravel())\n",
      "Z=clf.predict(data)\n",
      "Z.shape=sz\n",
      "\n",
      "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
      "plt.contour(XX,YY,Z,[0])\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(9,9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Observations:\n",
      "\n",
      "+ The boundary is piece-wise linear. It is composed of edges of the Voronoi diagram.\n",
      "+ Observe that the classifier perfectly fits the training data. Adding or removing one data point can largely change the boundary. This implies that the complexity of the method is large.\n",
      "+ The key component of the nearest neigbors classifier is the notion of similarity/distance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember that regularization explicitly models complexity. Regularization is usually a penalty term. In nearest neighbors we can penalize solutions with small \"support\" by using a majority voting on the $k$ closests data samples to the query sample."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Let's see what the boundary looks like in a toy problem.\n",
      "\n",
      "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
      "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
      "\n",
      "clf = neighbors.KNeighborsClassifier(3)\n",
      "clf.fit(X,y.ravel())\n",
      "Z2=clf.predict(data)\n",
      "Z2.shape=sz\n",
      "\n",
      "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.4, vmin=-1, vmax=1)\n",
      "plt.imshow(Z2, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.2, vmin=-1, vmax=1)\n",
      "\n",
      "plt.contour(XX,YY,Z2,[0])\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(9,9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2.1 Churn classification with nearest neighbors."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So let us head back to analyzing the problem of customer churn prediction. We may fit a 1-Nearest Neighbor classifier and check the result."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Recover Churn data\n",
      "import pickle\n",
      "fname = open('churn_data.pkl','rb')\n",
      "data = pickle.load(fname)\n",
      "X = data[0]\n",
      "y = data[1]\n",
      "print 'Loading ok.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "from sklearn import neighbors\n",
      "from sklearn import metrics\n",
      "acc = np.zeros((5,))\n",
      "i=0\n",
      "kf=cross_validation.KFold(n=y.shape[0], n_folds=5, indices=True, shuffle=False, random_state=0)\n",
      "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
      "yhat = y.copy()\n",
      "for train_index, test_index in kf:\n",
      "    X_train, X_test = X[train_index], X[test_index]\n",
      "    y_train, y_test = y[train_index], y[test_index]\n",
      "    dt = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
      "    dt.fit(X_train,y_train)\n",
      "    yhat[test_index] = dt.predict(X_test)\n",
      "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
      "    i=i+1\n",
      "print 'Mean accuracy: '+ str(np.mean(acc))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def draw_confusion(y,yhat,labels):\n",
      "    cm = metrics.confusion_matrix(y, yhat)\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "    ax.matshow(cm)\n",
      "    plt.title('Confusion matrix',size=20)\n",
      "    ax.set_xticklabels([''] + labels, size=20)\n",
      "    ax.set_yticklabels([''] + labels, size=20)\n",
      "    plt.ylabel('Predicted',size=20)\n",
      "    plt.xlabel('True',size=20)\n",
      "    for i in xrange(2):\n",
      "        for j in xrange(2):\n",
      "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
      "    fig.set_size_inches(7,7)\n",
      "    plt.show()\n",
      "\n",
      "draw_confusion(y,yhat,['no churn', 'churn'])\n",
      "print metrics.classification_report(y,yhat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is quite a bad result. Remember that by always selecting class 'no churn' we should get around $85\\%$ of accuracy. As it was noticed before the definition of distance is critical. In NN we are using Euclidean distance. Distances assume that all variables operate at the same scale, i.e. all are commensurable. A change in one unit in one of the variables is equivalent to/as important as a change of 1 unit in the other. In this data set, this does not happen. For example, area codes values are around 400 while whether the customer enjoys an international plan take values 0 and 1. Thus, we may account for these changes by scaling the features. The most standard way of doing this is feature normalization or standarization. In this preprocessing technique each feature is scaled to have zero mean and unit standard deviation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Standarize\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "X = scaler.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "acc_snooping = np.zeros((5,))\n",
      "i=0\n",
      "kf=cross_validation.KFold(n=y.shape[0], n_folds=5, indices=True, shuffle=False, random_state=0)\n",
      "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
      "yhat = y.copy()\n",
      "for train_index, test_index in kf:\n",
      "    X_train, X_test = X[train_index], X[test_index]\n",
      "    y_train, y_test = y[train_index], y[test_index]\n",
      "    dt = neighbors.KNeighborsClassifier(3)\n",
      "    dt.fit(X_train,y_train)\n",
      "    yhat[test_index] = dt.predict(X_test)\n",
      "    acc_snooping[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
      "    i=i+1\n",
      "print 'Mean accuracy: '+ str(np.mean(acc_snooping))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-error\"> **QUESTION:** In the former process we have accidentally snooped into the data and the result is contaminated. Where?</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#NO SNOOPING\n",
      "acc = np.zeros((5,))\n",
      "i=0\n",
      "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
      "yhat = y.copy()\n",
      "for train_index, test_index in kf:\n",
      "    X_train, X_test = X[train_index], X[test_index]\n",
      "    y_train, y_test = y[train_index], y[test_index]\n",
      "    scaler = StandardScaler()\n",
      "    X_train = scaler.fit_transform(X_train)\n",
      "    dt = neighbors.KNeighborsClassifier(3)\n",
      "    dt.fit(X_train,y_train)\n",
      "    X_test = scaler.transform(X_test)\n",
      "    yhat[test_index] = dt.predict(X_test)\n",
      "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
      "    i=i+1\n",
      "print 'Mean accuracy: '+ str(np.mean(acc))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "acct=np.c_[acc_snooping,acc]\n",
      "plt.boxplot(acct);\n",
      "for i in xrange(2):\n",
      "    xderiv = (i+1)*np.ones(acct[:,i].shape)+(np.random.rand(5,)-0.5)*0.1\n",
      "    plt.plot(xderiv,acct[:,i],'ro',alpha=0.3)\n",
      "ax = plt.gca()\n",
      "ax.set_xticklabels(['snooping', 'no snooping'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def draw_confusion(y,yhat,labels):\n",
      "    cm = metrics.confusion_matrix(y, yhat)\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "    ax.matshow(cm)\n",
      "    plt.title('Confusion matrix',size=20)\n",
      "    ax.set_xticklabels([''] + labels, size=20)\n",
      "    ax.set_yticklabels([''] + labels, size=20)\n",
      "    plt.ylabel('Predicted',size=20)\n",
      "    plt.xlabel('True',size=20)\n",
      "    for i in xrange(2):\n",
      "        for j in xrange(2):\n",
      "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
      "    fig.set_size_inches(7,7)\n",
      "    plt.show()\n",
      "\n",
      "draw_confusion(y,yhat,['no churn', 'churn'])\n",
      "print metrics.classification_report(y,yhat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2.1.1 A brief description of the confusion matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This result is much better. As we have seen accuracy can be a little informative in some problems. For this reason we may use other performance measures. Classic performance measures can be derived from the confusion matrix. Consider the following confusion matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def draw_confusion(y,yhat,labels):\n",
      "    cm = metrics.confusion_matrix(y, yhat)\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "    ax.matshow(cm)\n",
      "    plt.title('Confusion matrix',size=20)\n",
      "    ax.set_xticklabels([''] + labels, size=20)\n",
      "    ax.set_yticklabels([''] + labels, size=20)\n",
      "    plt.ylabel('Predicted',size=20)\n",
      "    plt.xlabel('True',size=20)\n",
      "    ax.text(0, 0, 'TP', va='center', ha='center',color='white',size=20)\n",
      "    ax.text(0, 1, 'FN', va='center', ha='center',color='white',size=20)\n",
      "    ax.text(1, 0, 'FP', va='center', ha='center',color='white',size=20)\n",
      "    ax.text(1, 1, 'TN', va='center', ha='center',color='white',size=20)            \n",
      "    fig.set_size_inches(7,7)\n",
      "    plt.show()\n",
      "\n",
      "draw_confusion(y,yhat,['positive', 'negative'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The matrix is divided in four quarters and contains\n",
      "\n",
      "+ True Positives (TP): Positive samples predicted as such.\n",
      "+ True Negatives (TN): Negative samples predicted as such.\n",
      "+ False Positives (FP): Negative samples predicted as positive.\n",
      "+ False Negatives (FN): Positive samples predicted as negative.\n",
      "\n",
      "The combination of these elements allows to define several performance metrics:\n",
      "\n",
      "+ Accuracy: \n",
      "\n",
      "$$\\text{accuracy}=\\frac{\\text{TP}+\\text{TN}}{\\text{TP}+\\text{TN}+\\text{FP}+\\text{FN}}$$\n",
      "\n",
      "Column-wise we find these two partial performance metrics:\n",
      "\n",
      "+ Sensitivity or Recall: \n",
      "\n",
      "$$\\text{sensitivity}=\\frac{\\text{TP}}{\\text{Real Positives}}=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$$\n",
      "\n",
      "+ Specificity:\n",
      "\n",
      "$$\\text{specificity}=\\frac{\\text{TN}}{\\text{Real Negatives}}=\\frac{\\text{TN}}{\\text{TN}+\\text{FP}}$$\n",
      "\n",
      "Row-wise we find these two partial performance metrics:\n",
      "\n",
      "+ Precision or Positive Predictive Value:\n",
      "\n",
      "$$\\text{precision}=\\frac{\\text{TP}}{\\text{Predicted Positives}}=\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$$\n",
      "\n",
      "+ Negative predictive value:\n",
      "\n",
      "$$\\text{NPV}=\\frac{\\text{TN}}{\\text{Predicted Negative}}=\\frac{\\text{TN}}{\\text{TN}+\\text{FN}}$$\n",
      "\n",
      "The concept of positive and negative samples is purely arbitrary, thus we really have to remember the concepts of precision/positive predictive value and sensitivity/recall. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let us check the concepts with churn as the positive class\n",
      "TP = np.sum(np.logical_and(yhat==1,y==1))\n",
      "TN = np.sum(np.logical_and(yhat==0,y==0))\n",
      "FP = np.sum(np.logical_and(yhat==1,y==0))\n",
      "FN = np.sum(np.logical_and(yhat==0,y==1))\n",
      "\n",
      "print 'TP: ' + str(TP)\n",
      "print 'TN: ' + str(TN)\n",
      "print 'FP: ' + str(FP)\n",
      "print 'FN: ' + str(FN)\n",
      "print 'sensitivity/recall: '+ str(TP/(TP+FN))\n",
      "print 'precision: '+ str(TP/(TP+FP))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2.1.2 Analyzing the confusion matrix in our problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our goal is to predict custormer churn, thus we may ask how often the classifier correctly predicts it. We will consider \"churn\" as the positive class. The question we are wondering about is the ratio between the $TP$ and all the $\\text{Real Positives}$. This is the *sensitivity* or *recall*. We are able to correctly predict $187/(187+296) = 0.39$ of the customers that cease the service. Observe that this value is consistent with the classification report when checking recall for class $1$.\n",
      "\n",
      "However, we have to trade-off this value with *precision*. Precision answers the question, from all the customers we predict will churn, which is the ratio of those that actually churn? This effectively tells us the price we are paying in terms of how many non-churn customers are being predicted as quitters. If we check this value, we can see it is $187/(187+59) = 76\\%$. This means that about 1 out 4 customers predicted as churn are not quitting the service."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-info\"> **Nearest Neighbors**\n",
      "<p>\n",
      "<ul>\n",
      "<li> One of the simplest classifiers.\n",
      "<li> Smoothness of the model is governed by the number of the neighbors.\n",
      "<li> Hyper-parameter $k$ or $p$ in the $\\ell_p$ norm are set by cross-validation.\n",
      "</ul>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#3. Decision trees"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Decision trees are another kind of intutive classification strategy based on the divide and conquer paradigm. \n",
      "\n",
      "The basic **idea** in decision trees is to partition the space in patches and fit a model in that patch. There are two questions to answer in order to implement this solution:\n",
      "\n",
      "+ How do we partition the space?\n",
      "+ What model to use in each patch?\n",
      "\n",
      "In classification trees the second question is straight forward, each patch is given the value of a label and all data falling in that part of the space will be predicted as such."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3.1 Decision tree modeling\n",
      "\n",
      "Elements:\n",
      "\n",
      "- Splits using axis-orthogonal hyperplanes. This is the key that allows interpretability of the results.\n",
      "\n",
      "- At each internal node we test a value of a feature. A feature and a threshold are stored for each internal node.  \n",
      "\n",
      "- Leaves makes the class prediction. If leaves are pure, we have to store the class label. If leaves are impure, then the fraction of samples for each class is stored and its frequency is returned when queried.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3.1.1 Building our intuition on decision trees\n",
      "\n",
      "Let us build up our intuition with a simple example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "#Let's see what the boundary looks like in a toy problem.\n",
      "%reset\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "MAXN=10\n",
      "np.random.seed(2)\n",
      "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
      "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
      "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
      "y = np.concatenate([y,np.ones((MAXN,1))])\n",
      "idxplus = y==1\n",
      "idxminus = y==-1\n",
      "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
      "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
      "\n",
      "from sklearn import tree\n",
      "from sklearn import metrics\n",
      "\n",
      "delta = 0.05\n",
      "xx = np.arange(-5.0, 15.0, delta)\n",
      "yy = np.arange(-5.0, 15.0, delta)\n",
      "XX, YY = np.meshgrid(xx, yy)\n",
      "Xf = XX.flatten()\n",
      "Yf = YY.flatten()\n",
      "sz=XX.shape\n",
      "data = np.c_[Xf[:,np.newaxis],Yf[:,np.newaxis]];\n",
      "clf = tree.DecisionTreeClassifier(random_state=0)\n",
      "clf.fit(X,y.ravel())\n",
      "Z=clf.predict(data)\n",
      "Z.shape=sz\n",
      "\n",
      "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
      "plt.contour(XX,YY,Z,[0])\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(9,9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Export Tree\n",
      "import os\n",
      "dotfile = tree.export_graphviz(clf, out_file = \"toy_tree.dot\")\n",
      "\n",
      "os.system(\"dot -Tpng toy_tree.dot -o toy_tree.png\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import Image\n",
      "Image(\"toy_tree.png\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us check the meaning of the tree. The first node splits the training set using feature $1$ by applying the threshold $\\leq 3.04$. As a result we are able to correctly classify eleven of the thirty data points. Let us see the boundary in that case."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = tree.DecisionTreeClassifier(random_state=0,max_depth=1)\n",
      "clf.fit(X,y.ravel())\n",
      "Z=clf.predict(data)\n",
      "Z.shape=sz\n",
      "\n",
      "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
      "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
      "\n",
      "\n",
      "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
      "plt.contour(XX,YY,Z,[0])\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(9,9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The second node splits the training set using feature $0$ by applying the threshold $\\leq 6.25$. Note that this only is used in the part of the space where feature $1$ is greater  than $3.04$. Observe that the remaining blue space is characterized by the following logical function: $(x_1>3.04) \\wedge (x_0\\leq 6.25)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = tree.DecisionTreeClassifier(random_state=0, max_depth=2)\n",
      "clf.fit(X,y.ravel())\n",
      "Z=clf.predict(data)\n",
      "Z.shape=sz\n",
      "\n",
      "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
      "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
      "\n",
      "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
      "plt.contour(XX,YY,Z,[0])\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(9,9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is great about decision trees?\n",
      "\n",
      "+ Trees are easy for humans to interpret. It can be seen as a set of rules. Each path from root to one leaf of the tree is an AND combination of the thresholded features.\n",
      "+ Given a finite data set, decision trees can express any function of the input attributes. In ${\\bf R}^d$ we can isolate every point in the data set by constructing a box around each of them.\n",
      "+ There can be more than one tree that fits the same data. From all of them we would like a tree with minimum number of nodes. But the problem is NP."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3.1.2 Learning the tree\n",
      "\n",
      "Because the problem is NP we can resort to a greedy construction algorithm. Greedy algorithms choose the current best binary partition without taking into account its impact on the quality of subsequent splits.\n",
      "\n",
      "The algorithm idea is as follows:\n",
      "\n",
      "+ Initialize the algorithm with a node associated to the full data set. \n",
      "\n",
      "**while** the list is not empty\n",
      "1. Retrieve the first node from the list.\n",
      "2. Find the data associated to that node.\n",
      "3. Find a splitting point.\n",
      "4. If the node is splittable, create the nodes linked to the parent node and put them in the exploration list.\n",
      "\n",
      "#### The splitting criterion\n",
      "\n",
      "There are many different splitting criteria. The most common ones are:\n",
      "\n",
      "+ Misclassification error\n",
      "+ Gini index\n",
      "+ Cross-entropy/Information gain/Mutual information\n",
      "\n",
      "Withouth going into details, misclassification error splits greedily select the split that corrects more data at each point. Gini index and cross-entropy probabilistically model the notion of impurity of a node. The split is chosen so that the average purity of the new nodes is maximized. Observe that as we descend in the tree the purity increases and eventually converge to pure leaves. A nice way of thinking about entropy is Pedro Domingos' simile with surprise. Entropy measures the average surprise/information a probabilistic result yields. In a binary variable, the maximum surprise occurs when both outcomes are equally probable, one has the maximum uncertainty on the result. Otherwise, the surprise decreases. This behavior is also display in Gini's index."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "entropy = lambda p: -np.sum(p * np.log2(p)) if not 0 in p else 0\n",
      "gini = lambda p: 1. - (np.array(p)**2).sum()\n",
      "pvals = np.linspace(0, 1)        \n",
      "plt.plot(pvals, [entropy([p,1-p])/2. for p in pvals], label='Entropy')\n",
      "plt.plot(pvals, [gini([p,1-p]) for p in pvals], label='Gini')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3.1.3 Trees and overfitting\n",
      "\n",
      "Because trees are very expressive models they can model any training set perfectly and easily overfit.\n",
      "\n",
      "There are two ways of avoiding overfitting in trees:\n",
      "\n",
      "+ Stop growing the tree when the split is not statistically significant.\n",
      "+ Grow a full tree and post-prune.\n",
      "\n",
      "One of the simplest ways of post pruning is \"reduced error prunning\". It goes like this,\n",
      "\n",
      "1. Split data into training and validation\n",
      "2. Create a candidate tree on the training set\n",
      "3. Do until further pruning is harmful\n",
      "    1. Evaluate impact on the validation set of removing each posible node (with descendants)\n",
      "    2. Greedily remove the node that improves the performance the most.\n",
      "    \n",
      "Pruning is not implemented in sklearn at this moment. However let us check what happens in our customer churn prediction problem when we use a decision tree."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reset -f\n",
      "#Recover Churn data\n",
      "import pickle\n",
      "fname = open('churn_data.pkl','rb')\n",
      "data = pickle.load(fname)\n",
      "X = data[0]\n",
      "y = data[1]\n",
      "features = data[2]\n",
      "print 'Loading ok.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#NO SNOOPING\n",
      "import numpy as np\n",
      "from sklearn import cross_validation\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn import tree\n",
      "from sklearn import metrics\n",
      "\n",
      "\n",
      "kf=cross_validation.KFold(n=y.shape[0], n_folds=5, indices=True, shuffle=False, random_state=0)\n",
      "\n",
      "acc = np.zeros((5,))\n",
      "i=0\n",
      "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
      "yhat = y.copy()\n",
      "for train_index, test_index in kf:\n",
      "    X_train, X_test = X[train_index], X[test_index]\n",
      "    y_train, y_test = y[train_index], y[test_index]\n",
      "    scaler = StandardScaler()\n",
      "    X_train = scaler.fit_transform(X_train)\n",
      "    dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
      "    dt.fit(X_train,y_train)\n",
      "    X_test = scaler.transform(X_test)\n",
      "    yhat[test_index] = dt.predict(X_test)\n",
      "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
      "    i=i+1\n",
      "print 'Mean accuracy: '+ str(np.mean(acc))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "def draw_confusion(y,yhat,labels):\n",
      "    cm = metrics.confusion_matrix(y, yhat)\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "    ax.matshow(cm)\n",
      "    plt.title('Confusion matrix',size=20)\n",
      "    ax.set_xticklabels([''] + labels, size=20)\n",
      "    ax.set_yticklabels([''] + labels, size=20)\n",
      "    plt.ylabel('Predicted',size=20)\n",
      "    plt.xlabel('True',size=20)\n",
      "    for i in xrange(2):\n",
      "        for j in xrange(2):\n",
      "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
      "    fig.set_size_inches(7,7)\n",
      "    plt.show()\n",
      "\n",
      "draw_confusion(y,yhat,['no churn', 'churn'])\n",
      "print metrics.classification_report(y,yhat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let us check the concepts with churn as the positive class\n",
      "TP = np.sum(np.logical_and(yhat==1,y==1))\n",
      "TN = np.sum(np.logical_and(yhat==0,y==0))\n",
      "FP = np.sum(np.logical_and(yhat==1,y==0))\n",
      "FN = np.sum(np.logical_and(yhat==0,y==1))\n",
      "\n",
      "print 'TP: ' + str(TP)\n",
      "print 'TN: ' + str(TN)\n",
      "print 'FP: ' + str(FP)\n",
      "print 'FN: ' + str(FN)\n",
      "print 'sensitivity/recall: '+ str(TP/(TP+FN))\n",
      "print 'precision: '+ str(TP/(TP+FP))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Observe that by using a decision tree, the recall increased by $30\\%$ while having the precision at a simliar level than nearest neighbors. Let us check the first levels of the tree."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "#Let us check the the first three levels of the tree. GraphViz and PyDot are needed.\n",
      "dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
      "scaler = StandardScaler()\n",
      "Xs = scaler.fit_transform(X)\n",
      "dt.fit(Xs,y)\n",
      "\n",
      "#Export Tree\n",
      "\n",
      "dotfile = tree.export_graphviz(dt, out_file = \"churn.dot\", feature_names = features)\n",
      "\n",
      "\n",
      "os.system(\"dot -Tpng churn.dot -o churn.png\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import Image\n",
      "Image(\"churn.png\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Observe the first feature split and the values of the entropy according to the split."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "entropy = lambda p: -np.sum(p * np.log2(p)) if not 0 in p else 0\n",
      "\n",
      "#Let us check the entropy on the root node\n",
      "#There are 2850 samples of customers that stay in the company. The frequency is\n",
      "proot = 2850/3333\n",
      "#And the entropy value is\n",
      "print 'Root node entropy: '+ str(entropy([proot,1-proot]))\n",
      "\n",
      "#After the split we have the following frequencies for the left and right children\n",
      "pleft = np.sum([2476,166,13,111])/3122 #Frequency of label 0 on the left node\n",
      "pright = np.sum([32,5,44,3])/211 #Frequency of label 0 on the right node\n",
      "\n",
      "print 'Left node entropy: '+ str(entropy([pleft,1-pleft]))\n",
      "print 'Right node entropy: '+ str(entropy([pright,1-pright]))\n",
      "\n",
      "#Information gain computes the difference between the entropy of the parent and the weighed entropies of the children\n",
      "# I = H_root - \\sum freq_i * H_i\n",
      "\n",
      "I = entropy([proot,1-proot]) - 3122/3333*entropy([pleft,1-pleft])+211/3333*entropy([pright,1-pright])\n",
      "\n",
      "print 'Information gain: '+ str(I)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The split reduces the average entropy of the children, thus the splits are more pure.\n",
      "\n",
      "Observations:\n",
      "\n",
      "+ Observe that because we have restricted (for visualization purposes) the maximum depth of the tree, the leaves are not pure. \n",
      "+ Analyzing the leaves we can see that most of the clients that are hooked to the plan share the following conditions:\n",
      "\n",
      "$$(\\text{Day Charge} \\leq 1.5) \\wedge (\\text{Customer Service Calls} \\leq 1.47) \\wedge (\\text{International Plan} \\leq 1.36)$$\n",
      "\n",
      "Note that these values are preprocessed and as such they convey little interpretable information. Thus, in order to make sense of the former conditions we have to invert the transformation. We can do it in several ways, one way is to use interpolation and query for the feature of interest."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import interpolate\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "idx_global = features.tolist()\n",
      "\n",
      "#Day Charge\n",
      "plt.subplot(1,3,1)\n",
      "idx = idx_global.index('Day Charge')\n",
      "selection = X[:,idx]\n",
      "idx_sort = np.argsort(selection)\n",
      "plt.plot(Xs[idx_sort,idx], X[idx_sort,idx])\n",
      "spline = interpolate.UnivariateSpline(x=Xs[idx_sort,idx], y=X[idx_sort,idx])\n",
      "plt.plot(Xs[idx_sort,idx],spline(Xs[idx_sort,idx]),'r')\n",
      "plt.title('Day Charge transformation')\n",
      "ax = plt.gca()\n",
      "ax.set_xlabel('Transformed data')\n",
      "ax.set_ylabel('Original data')\n",
      "print 'Day Charge < '+str(spline([1.55])[0])\n",
      "\n",
      "\n",
      "#CustServ Calls\n",
      "plt.subplot(1,3,2)\n",
      "idx = idx_global.index('CustServ Calls')\n",
      "selection = X[:,idx]\n",
      "idx_sort = np.argsort(selection)\n",
      "plt.plot(Xs[idx_sort,idx], X[idx_sort,idx],'o')\n",
      "spline = interpolate.UnivariateSpline(x=Xs[idx_sort,idx], y=X[idx_sort,idx])\n",
      "plt.plot(Xs[idx_sort,idx],spline(Xs[idx_sort,idx]),'r')\n",
      "plt.title('Customer Service Calls transformation')\n",
      "ax = plt.gca()\n",
      "ax.set_xlabel('Transformed data')\n",
      "ax.set_ylabel('Original data')\n",
      "print 'Customer service calls <' + str(spline([1.47])[0])\n",
      "\n",
      "#International Plan\n",
      "plt.subplot(1,3,3)\n",
      "idx = idx_global.index(\"Int'l Plan\")\n",
      "selection = X[:,idx]\n",
      "idx_sort = np.argsort(selection)\n",
      "plt.plot(Xs[idx_sort,idx], X[idx_sort,idx],'o')\n",
      "spline = interpolate.UnivariateSpline(x=Xs[idx_sort,idx], y=X[idx_sort,idx])\n",
      "plt.plot(Xs[idx_sort,idx],spline(Xs[idx_sort,idx]),'r')\n",
      "plt.title('International Plan transformation')\n",
      "ax = plt.gca()\n",
      "ax.set_xlabel('Transformed data')\n",
      "ax.set_ylabel('Original data')\n",
      "print 'International Plan < ' + str(spline([1.36])[0])\n",
      "\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(16,4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thus the final profile for the 2476 customers that do not churn has in common:\n",
      "\n",
      "$$(\\text{Day Charge} \\leq 45) \\wedge (\\text{Customer Service Calls} < 4) \\wedge (\\text{International Plan}= \\text{NO})$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 4. Extending Support Vector Machines to the Non-Linear Case. A very brief introduction to kernels."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have seen that a linear model in the parameters can model non-linear boundaries provided we **explicitly** map original data non-linearly. For example, we can create a linear model with features squared. This will lead to a quadratic boundary with respect to the original space. There is another way of **implicitly** encoding non-linearities by means of **kernels**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The kernel encodes the notion of similarity between two data points. \n",
      "\n",
      "The change in the formulation involve the introduction of several concepts from mathematical analysis. For the sake of simplicity, we will skip the details (read \"The story of a kernel\" in the following paragraph or ask any detail if you are curious).\n",
      "\n",
      "As a result, any regularized cost function optimization problem such as SVM has a solution of following form,\n",
      "\n",
      "$$f(x) = \\sum\\limits_{i=1}^N \\alpha_i k(x_i,x)$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-info\">**The story of a kernel**\n",
      "<p>\n",
      "*Disclaimer: This is  a highly mathematically non rigorous story of how kernels come into play in machine learning.*\n",
      "<p>\n",
      "At some point we talked about keeping complexity in check. For that purpose we need to measure complexity. And we saw that certain models, such as linear models can control it using the norm of weights. To the fact of adding this penalty to the objective function we are optimizing is called *regularization*.\n",
      "<p>\n",
      "However, it would be great to be able to measure the complexity of any function. To address this issue we have to resort to functional analysis. Functional analysis is a brach of mathematical analyisis that deals with spaces of functions. For that purpose a Hilbert space must be introduced so that similarity and distance among functions can be measured. A Hilbert space is a complete vector space with inner product. Intuitively, it generalizes the classical Euclidean space to infinite dimensions, and thus, to functional spaces.\n",
      "<p>\n",
      "One particular functional space is the Reproducing Kernel Hilbert Space (RKHS). In this space a function evaluated on a point $x$ is defined by the inner product of the function and the kernel evaluated on that point, i.e. $f(x) = \\langle f(\\cdot),K(x,\\cdot) \\rangle$, where $K$ is the kernel. This is called Riesz representation and it is the key for showing the most important result for our problems, *The Representer's theorem*.\n",
      "<p>\n",
      "The Respresenter's theorem states that the solution of any problem with the following form \n",
      "$$\n",
      "\tf^*=\\underset{f\\in \\mathcal{H}}{\\operatorname{arg\\,min}}\\frac{1}{n}\\sum_i{\\mathcal{L}(f(x_i),y_i)}+\\lambda\\|f\\|_{\\mathcal{H}}^2\n",
      "\t$$\n",
      "is given by\n",
      "$$f(x) = \\sum\\limits_{i=1}^N \\alpha_i k(x_i,x)$$\n",
      "where $x_i$ are our samples. \n",
      "</div>\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The kernel has to be a positive semi-definite function, such as:\n",
      "\n",
      "+ Linear kernel: $$k(x_i,x_j) = x_i^Tx_j$$\n",
      "+ Polynomial kernel: $$k(x_i,x_j) = (1+ x_i^Tx_j)^p$$\n",
      "+ Radial Basis Function kernel $$k(x_i,x_j) = e^{-\\frac{\\|x_i-x_j\\|^2}{2\\sigma^2}}$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-error\"> On a practical side you can define a kernel by using your favorite distance $d(x_i,x_j)$ and defining the kernel as\n",
      "$$k(x_i,x_j) = e^{-\\gamma d(x_i,x_j)}, \\quad \\gamma>0$$\n",
      "where $\\gamma$ is a hyper-parameter that controls the decay of the exponential (we will tune it using cross-validation). Observe that RBF is an instantiation of this more general rule.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As comented before, kernels implicitly encode a non-linear transformation and remember that SVM finds the optimal hyperplane. By combining both concepts we have a linear method applied on a data on a transformed space, but we do not have to provide the explicit transformation.\n",
      "\n",
      "This becomes incredibly useful when one realizes that the feature mapping from a radial basis function kernel is a maping into a $\\infty$-dimensional space."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Le us build our intuition about how kernels work with the following video:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import YouTubeVideo\n",
      "YouTubeVideo('3liCbRZPrZA', size=700)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us try it in a toy problem."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Let's see what the boundary looks like in a toy problem.\n",
      "%reset\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "MAXN=10\n",
      "np.random.seed(2)\n",
      "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
      "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
      "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
      "y = np.concatenate([y,np.ones((MAXN,1))])\n",
      "idxplus = y==1\n",
      "idxminus = y==-1\n",
      "\n",
      "from sklearn import svm\n",
      "from sklearn import metrics\n",
      "\n",
      "delta = 0.05\n",
      "xx = np.arange(-5.0, 15.0, delta)\n",
      "yy = np.arange(-5.0, 15.0, delta)\n",
      "XX, YY = np.meshgrid(xx, yy)\n",
      "Xf = XX.flatten()\n",
      "Yf = YY.flatten()\n",
      "sz=XX.shape\n",
      "data = np.c_[Xf[:,np.newaxis],Yf[:,np.newaxis]];\n",
      "\n",
      "def SVC_gamma(gamma, C):\n",
      "    clf = svm.SVC(kernel = 'rbf', gamma = gamma, C = C)\n",
      "    clf.fit(X,y.ravel())\n",
      "    Z=clf.decision_function(data)\n",
      "    Z.shape=sz\n",
      "    plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
      "    plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
      "    plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
      "    plt.contour(XX,YY,Z,[0])\n",
      "    fig = plt.gcf()\n",
      "    fig.set_size_inches(9,9)\n",
      "\n",
      "from IPython.html.widgets import interact    \n",
      "interact(SVC_gamma, gamma=(0.011,5.,0.01), C = (0.01,2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#4.1 Application to customer churn prediction\n",
      "\n",
      "Let us apply the RBF kernel SVM to the customer churn prediction. Usually, discriminant classifiers are not affine invariant and we have to consider some feature normalization process. For the sake of fairness, we will use the same standarization method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reset\n",
      "#Recover Churn data\n",
      "import pickle\n",
      "fname = open('churn_data.pkl','rb')\n",
      "data = pickle.load(fname)\n",
      "X = data[0]\n",
      "y = data[1]\n",
      "features = data[2]\n",
      "print 'Loading ok.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#NO SNOOPING\n",
      "import numpy as np\n",
      "from sklearn import cross_validation\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn import svm\n",
      "from sklearn import metrics\n",
      "\n",
      "\n",
      "kf=cross_validation.KFold(n=y.shape[0], n_folds=5, indices=True, shuffle=False, random_state=0)\n",
      "\n",
      "acc = np.zeros((5,))\n",
      "i=0\n",
      "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
      "yhat = y.copy()\n",
      "for train_index, test_index in kf:\n",
      "    X_train, X_test = X[train_index], X[test_index]\n",
      "    y_train, y_test = y[train_index], y[test_index]\n",
      "    scaler = StandardScaler()\n",
      "    X_train = scaler.fit_transform(X_train)\n",
      "    #Standard parameters\n",
      "    clf = svm.SVC(kernel='rbf', gamma = 0.051, C = 1)\n",
      "    clf.fit(X_train,y_train.ravel())\n",
      "    X_test = scaler.transform(X_test)\n",
      "    yhat[test_index] = clf.predict(X_test)\n",
      "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
      "    i=i+1\n",
      "print 'Mean accuracy: '+ str(np.mean(acc))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "def draw_confusion(y,yhat,labels):\n",
      "    cm = metrics.confusion_matrix(y, yhat)\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "    ax.matshow(cm)\n",
      "    plt.title('Confusion matrix',size=20)\n",
      "    ax.set_xticklabels([''] + labels, size=20)\n",
      "    ax.set_yticklabels([''] + labels, size=20)\n",
      "    plt.ylabel('Predicted',size=20)\n",
      "    plt.xlabel('True',size=20)\n",
      "    for i in xrange(2):\n",
      "        for j in xrange(2):\n",
      "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
      "    fig.set_size_inches(7,7)\n",
      "    plt.show()\n",
      "\n",
      "draw_confusion(y,yhat,['no churn', 'churn'])\n",
      "print metrics.classification_report(y,yhat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us cross-validate the parameters and check if we can do better."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import grid_search\n",
      "parameters = {'C':[ 2,4,8],'gamma':[0.02, 0.05, 0.1], 'class_weight':[{0:0.5},{0:1},{0:2}]}\n",
      "\n",
      "kf=cross_validation.KFold(n=y.shape[0], n_folds=5, indices=True, shuffle=False, random_state=0)\n",
      "\n",
      "acc = np.zeros((5,))\n",
      "i=0\n",
      "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
      "yhat = y.copy()\n",
      "for train_index, test_index in kf:\n",
      "    X_train, X_test = X[train_index], X[test_index]\n",
      "    y_train, y_test = y[train_index], y[test_index]\n",
      "    scaler = StandardScaler()\n",
      "    X_train = scaler.fit_transform(X_train)\n",
      "    #Standard parameters\n",
      "    clf = svm.SVC(kernel='rbf', class_weight={0:1,1:10})\n",
      "    # We can change the scoring \"average_precision\", \"recall\", \"f1\"\n",
      "    clf = grid_search.GridSearchCV(clf, parameters, scoring='average_precision')\n",
      "    clf.fit(X_train,y_train.ravel())\n",
      "    X_test = scaler.transform(X_test)\n",
      "    yhat[test_index] = clf.predict(X_test)\n",
      "    #recall, f1, precision\n",
      "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
      "    print str(clf.best_params_)\n",
      "    i=i+1\n",
      "print 'Mean accuracy: '+ str(np.mean(acc))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "def draw_confusion(y,yhat,labels):\n",
      "    cm = metrics.confusion_matrix(y, yhat)\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "    ax.matshow(cm)\n",
      "    plt.title('Confusion matrix',size=20)\n",
      "    ax.set_xticklabels([''] + labels, size=20)\n",
      "    ax.set_yticklabels([''] + labels, size=20)\n",
      "    plt.ylabel('Predicted',size=20)\n",
      "    plt.xlabel('True',size=20)\n",
      "    for i in xrange(2):\n",
      "        for j in xrange(2):\n",
      "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
      "    fig.set_size_inches(7,7)\n",
      "    plt.show()\n",
      "\n",
      "draw_confusion(y,yhat,['no churn', 'churn'])\n",
      "print metrics.classification_report(y,yhat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class = \"alert alert-success\"> **QUESTION:** We are dealing with an unbalanced problem. Change the code to bias the model to deal with unbalancing.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#PART 2. Introduction to ensemble learning\n",
      "\n",
      "When we want to purchase a product we usually read user's reviews. Before undergoing a major surjery procedure we seek the opinion of different experts. Ensemble learning mimicks one of the human uncertainty reduction mechanism, seeking additional opinions before making a major decision.\n",
      "\n",
      "Ensemble learning is divided in two steps:\n",
      "\n",
      "1. Train a set of classifiers\n",
      "2. Aggregate their results\n",
      "\n",
      "There are different reasons for using ensemble learning in practice:\n",
      "\n",
      "1. **Statistical reasons:** The combination of outputs of different classifiers may reduce the risk of an unfortunate selection of a poorly performing classifier.\n",
      "2. **Large scale data sets:** It makes little sense to only have one classifier on very large sets of data. Partition data in smaller subsets and aggregate seems like a good idea.\n",
      "3. **Divide and conquer:** Some problems too difficult for a single classifier to solve. The decision boundary may be too complex or lie outside the space of functions of the classifier.\n",
      "4. **Data fusion:** Different source fusion is usually a problem. One usually faces data coming from heterogeneous sources and the question is how to fuse these data. One solution is to train one classifier per source and the fuse the decision of those experts."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1.1 Diversity\n",
      "\n",
      "One condition required for the system to work is that errors on different classifiers should be made on different samples in order for the strategic combination of the classifiers to correct possible errors in the judgement of the class o a particular instance. This effect has been called **diversity**.\n",
      "\n",
      "Diversity can be obtained in different ways:\n",
      "\n",
      "+ Using different training sets. Use resampling strategies to obtain different optimal classifiers. This effect is correlated with the notion of stability of the classifier and the concept of bias and variance of the classifier.\n",
      "+ Using different training parameters for different classifiers\n",
      "+ Combining different architectures. (i.e. svm, decission trees, ...)\n",
      "+ Training on different features. (i.e. random subspaces or random projections)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1.2 Bootstrapping aggregation \n",
      "\n",
      "Bootstrapping means resampling the training data set with replacement. Usually the same number of data as the original data set is used.\n",
      "\n",
      "**Bootstrapping aggreagtion (aka. Bagging)** is a ensemble technique that uses multiple bootstrapped copies of the training set to build a set of classifiers. One classifier for each bootstrapped training copy. And then, use a combination technique, such as majority voting, in order to take the final decision."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us check, how it works."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Let us train an overfitted classifier. For example an SVC.\n",
      "%reset\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "MAXN=60\n",
      "np.random.seed(2)\n",
      "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
      "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
      "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
      "y = np.concatenate([y,np.ones((MAXN,1))])\n",
      "idxplus = y==1\n",
      "idxminus = y==-1\n",
      "\n",
      "from sklearn import tree\n",
      "from sklearn import metrics\n",
      "\n",
      "x = np.linspace(-5,15,200)\n",
      "XX,YY = np.meshgrid(x,x)\n",
      "sz=XX.shape\n",
      "data=np.c_[XX.ravel(),YY.ravel()]\n",
      "\n",
      "\n",
      "clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
      "clf.fit(X,y.ravel())\n",
      "Z=clf.predict(data)\n",
      "mx= np.max(Z)\n",
      "mn= np.min(Z)\n",
      "Z.shape=sz\n",
      "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
      "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
      "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=mn, vmax=mx)\n",
      "plt.colorbar()\n",
      "plt.contour(XX,YY,Z,[0])\n",
      "\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(9,9)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train_bagged_tree(X,y,C):\n",
      "    clf_list=[]\n",
      "    for i in xrange(C):\n",
      "        np.random.seed(None)\n",
      "        idx=np.random.randint(0,y.shape[0],y.shape[0])\n",
      "        clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
      "        Xr=X[idx,:]\n",
      "        yr=y[idx]\n",
      "        clf_list.append((clf.fit(Xr,yr.ravel()),idx))  #Add the indices for visualization purposes in test\n",
      "    return clf_list\n",
      "\n",
      "\n",
      "def visualize_bagged_tree(X,y,clf_list):\n",
      "    C = len(clf_list)\n",
      "    x = np.linspace(-5,15,200)\n",
      "    XX,YY = np.meshgrid(x,x)\n",
      "    sz=XX.shape\n",
      "    data=np.c_[XX.ravel(),YY.ravel()]\n",
      "    yhat=np.zeros((data.shape[0],len(clf_list)))\n",
      "    i=0\n",
      "    for dt,idx in clf_list:\n",
      "        yhat[:,i]=dt.predict(data)\n",
      "        Xr=X[idx,:]\n",
      "        yr=y[idx]\n",
      "        mx= np.max(yhat[:,i])\n",
      "        mn= np.min(yhat[:,i])\n",
      "        plt.subplot(int(np.floor(C/4))+1,4,i+1)\n",
      "        plt.scatter(Xr[(yr==1).ravel(),0],Xr[(yr==1).ravel(),1],color='r')\n",
      "        plt.scatter(Xr[(yr==-1).ravel(),0],Xr[(yr==-1).ravel(),1],color='b')\n",
      "        plt.imshow(yhat[:,i].reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=mn, vmax=mx)\n",
      "        plt.contour(XX,YY,yhat[:,i].reshape(sz),[0])\n",
      "        i=i+1\n",
      "    fig = plt.gcf()\n",
      "    fig.set_size_inches(20,7*int(np.floor(C/4))+1)\n",
      "    return yhat\n",
      "\n",
      "\n",
      "clf_list=train_bagged_tree(X,y,16)\n",
      "y_pred=visualize_bagged_tree(X,y,clf_list)\n",
      "y_pred = np.sum(y_pred,axis=1)\n",
      "\n",
      "print 'Process finnished.'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mx= np.max(y_pred)\n",
      "mn= np.min(y_pred)\n",
      "plt.figure()\n",
      "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
      "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
      "plt.imshow(y_pred.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=mn, vmax=mx)\n",
      "plt.colorbar()\n",
      "plt.contour(XX,YY,y_pred.reshape(sz),[0])\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(10,10) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Zb = y_pred.reshape(sz)\n",
      "plt.subplot(1,2,1)\n",
      "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
      "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
      "plt.imshow(Zb, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=mn, vmax=mx)\n",
      "plt.contour(XX,YY,Zb,[0])\n",
      "plt.subplot(1,2,2)\n",
      "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
      "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
      "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
      "plt.contour(XX,YY,Z,[0])\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(16,9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###1.2.1 Application to customer churn prediction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us check this approach in the Churn problem. Recall that a single decision tree achieved an accuracy of $91.7\\%$, precision of $71\\%$ and recall of $72\\%$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reset\n",
      "#Recover Churn data\n",
      "import pickle\n",
      "fname = open('churn_data.pkl','rb')\n",
      "data = pickle.load(fname)\n",
      "X = data[0]\n",
      "y = 2*data[1]-1\n",
      "features = data[2]\n",
      "print 'Loading ok.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train_bagged_tree(X,y,C):\n",
      "    clf_list=[]\n",
      "    for i in xrange(C):\n",
      "        np.random.seed(None)\n",
      "        idx=np.random.randint(0,y.shape[0],y.shape[0])\n",
      "        clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
      "        Xr=X[idx,:]\n",
      "        yr=y[idx]\n",
      "        clf_list.append(clf.fit(Xr,yr.ravel()))  \n",
      "    return clf_list\n",
      "\n",
      "\n",
      "def test_bagged_tree(X,clf_list):\n",
      "    yhat=np.zeros((X.shape[0],len(clf_list)))\n",
      "    i=0\n",
      "    for dt in clf_list:\n",
      "        yhat[:,i]=dt.predict(X)\n",
      "        i=i+1\n",
      "    return np.sign(np.mean(yhat,axis=1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#NO SNOOPING\n",
      "import numpy as np\n",
      "from sklearn import cross_validation\n",
      "from sklearn import tree\n",
      "from sklearn import metrics\n",
      "\n",
      "kf=cross_validation.KFold(n=y.shape[0], n_folds=5, indices=True, shuffle=False, random_state=0)\n",
      "\n",
      "acc = np.zeros((5,))\n",
      "i=0\n",
      "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
      "yhat = y.copy()\n",
      "for train_index, test_index in kf:\n",
      "    X_train, X_test = X[train_index], X[test_index]\n",
      "    y_train, y_test = y[train_index], y[test_index]\n",
      "    clf_list = train_bagged_tree(X_train,y_train.ravel(),51)\n",
      "    yhat[test_index]=test_bagged_tree(X_test,clf_list) \n",
      "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
      "    i=i+1\n",
      "print 'Mean accuracy: '+ str(np.mean(acc))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "def draw_confusion(y,yhat,labels):\n",
      "    cm = metrics.confusion_matrix(y, yhat)\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "    ax.matshow(cm.T)\n",
      "    plt.title('Confusion matrix',size=20)\n",
      "    ax.set_xticklabels([''] + labels, size=20)\n",
      "    ax.set_yticklabels([''] + labels, size=20)\n",
      "    plt.ylabel('Predicted',size=20)\n",
      "    plt.xlabel('True',size=20)\n",
      "    for i in xrange(2):\n",
      "        for j in xrange(2):\n",
      "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
      "    fig.set_size_inches(7,7)\n",
      "    plt.show()\n",
      "\n",
      "draw_confusion(y,yhat,['no churn', 'churn'])\n",
      "print metrics.classification_report(y,yhat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Observe that the solution accuracy increases by about $5\\%$, recall goes upt to $75\\%$ and precision also increases up to $91\\%$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-info\"> **Bagging** performance improvement is due to the reduction of the variance of the classifier while mantaining its bias.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1.3. Random Forest"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Random Forest technique introduces a randomization over the feature selected for building  each tree in the ensemble in order to improve diversity in an attempt to reduce variance evan more. Let us code this variant of bagging."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reset\n",
      "from sklearn import tree\n",
      "import numpy as np\n",
      "def train_random_forest(X,y,C):\n",
      "    F=int(np.ceil(np.sqrt(X.shape[1])))\n",
      "    clf_list=[]\n",
      "    for i in xrange(C):\n",
      "        np.random.seed(None)\n",
      "        idx=np.random.randint(0,y.shape[0],y.shape[0])\n",
      "        feat_idx=np.random.permutation(np.arange(X.shape[1]))[:10]\n",
      "        clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
      "        Xr=X[idx,:].copy()\n",
      "        Xr=Xr[:,feat_idx]\n",
      "        yr=y[idx]\n",
      "        clf_list.append((clf.fit(Xr,yr.ravel()),feat_idx))\n",
      "    return clf_list\n",
      "\n",
      "\n",
      "def test_random_forest(X,clf_list):\n",
      "    yhat=np.zeros((X.shape[0],len(clf_list)))\n",
      "    i=0\n",
      "    for dt,feat_idx in clf_list:\n",
      "        yhat[:,i]=dt.predict(X[:,feat_idx])\n",
      "        i=i+1\n",
      "    return np.sign(np.mean(yhat,axis=1)),yhat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Recover Churn data\n",
      "import pickle\n",
      "fname = open('churn_data.pkl','rb')\n",
      "data = pickle.load(fname)\n",
      "X = data[0]\n",
      "y = 2*data[1]-1\n",
      "print 'Labels: '+ str(np.unique(y))\n",
      "features = data[2]\n",
      "print 'Loading ok.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "clf_list = train_random_forest(X,y,201)\n",
      "yhat,yk = test_random_forest(X,clf_list)\n",
      "acc = metrics.accuracy_score(yhat, y)\n",
      "print yk.shape\n",
      "print np.sum(np.mean(yk,axis=1)>0)\n",
      "print acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#NO SNOOPING\n",
      "from sklearn import cross_validation\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn import ensemble\n",
      "from sklearn import metrics\n",
      "\n",
      "\n",
      "kf=cross_validation.KFold(n=y.shape[0], n_folds=5, indices=True, shuffle=False, random_state=0)\n",
      "\n",
      "acc = np.zeros((5,))\n",
      "i=0\n",
      "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
      "yhat = y.copy()\n",
      "for train_index, test_index in kf:\n",
      "    X_train, X_test = X[train_index], X[test_index]\n",
      "    y_train, y_test = y[train_index], y[test_index]\n",
      "    #dt = ensemble.RandomForestClassifier(n_estimators=51)\n",
      "    #dt.fit(X_train,y_train)\n",
      "    #yhat[test_index]=dt.predict(X_test)\n",
      "    clf_list = train_random_forest(X_train,y_train,201)\n",
      "    yhat[test_index],yk = test_random_forest(X_test,clf_list)\n",
      "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
      "    i=i+1\n",
      "print acc\n",
      "print np.unique(yhat)\n",
      "print np.unique(y_test)\n",
      "print 'Mean accuracy: '+ str(np.mean(acc))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "def draw_confusion(y,yhat,labels):\n",
      "    cm = metrics.confusion_matrix(y, yhat)\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "    ax.matshow(cm.T)\n",
      "    plt.title('Confusion matrix',size=20)\n",
      "    ax.set_xticklabels([''] + labels, size=20)\n",
      "    ax.set_yticklabels([''] + labels, size=20)\n",
      "    plt.ylabel('Predicted',size=20)\n",
      "    plt.xlabel('True',size=20)\n",
      "    for i in xrange(2):\n",
      "        for j in xrange(2):\n",
      "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
      "    fig.set_size_inches(7,7)\n",
      "    plt.show()\n",
      "draw_confusion(y,yhat,['no churn', 'churn'])\n",
      "print metrics.classification_report(y,yhat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#3. Reductionist frameworks for the multi-class problems "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Up to this moment we have applied several models in multi-class problems but we have barely talked about the problem of multiple classes in learning. \n",
      "\n",
      "First of all, there are very few models that intrinsically handle the multi-class case. By intrinsically handling the multi-class problem I am referring to methods in which we do not have to worry about how many classes our problem has. The two big families of models that can deal with the problem are\n",
      "\n",
      "+ Decision trees: the leaves encode the class.\n",
      "+ Nearest Neighbors: we only care about class labels of instances close to my query sample.\n",
      "\n",
      "What about the rest of the models? Did not Bayesian models or Neural Networks also handle this problem? Yes, they work in the multi-class case. But we have to worry about how many classes there are. In particular we have to build a model for each class and then take a maximum score/probability/confidence among the predictions. This way of addressing the multiclass problem is also known as **one-against-all** because we consider one model for each class while the samples from the rest of the classes are considered as negative samples. This is the first example of a reductionist framework.\n",
      "\n",
      "The reductionist framework refers to those ensemble methods that allows to reduce the multi-class problem to a set of binary problems. In a $K$ class problem, the two most common approaches in this framework are:\n",
      "\n",
      "+ **one-against-all:** We consider $K$ partitions of the problem, corresponding to setting one class as positive class and the rest as negative. \n",
      "+ **one-against-one:** We consider all posible pairs of classes and build a model for each subproblem. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reset\n",
      "#Create a multiclass toy problem\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn import svm\n",
      "MAXN=5\n",
      "np.random.seed(0)\n",
      "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.25*np.random.randn(MAXN,2)]) \n",
      "X = np.concatenate([X,[8,-2]+1.25*np.random.randn(MAXN,2)])\n",
      "y = np.concatenate([np.ones((MAXN,1)),2*np.ones((MAXN,1))])\n",
      "y = np.concatenate([y,3*np.ones((MAXN,1))])\n",
      "\n",
      "#Display data\n",
      "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r',label='class 1')\n",
      "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b',label='class 2')\n",
      "plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g',label='class 3')\n",
      "plt.legend()\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(9,9)\n",
      "\n",
      "#Train a LinearSVC in one-vs-all fashion\n",
      "clf_list=[]\n",
      "for i in xrange(3):\n",
      "    clf = svm.LinearSVC()\n",
      "    y_meta = y.copy()\n",
      "    #Create a binary problem with one class at +1 and the rest at -1\n",
      "    y_meta=np.where(y_meta == i+1 ,1,-1)\n",
      "    clf_list.append(clf.fit(X,y_meta.ravel()))\n",
      "\n",
      "#Test each classifier\n",
      "plt.figure()\n",
      "x = np.linspace(-5,15,200)\n",
      "XX,YY = np.meshgrid(x,x)\n",
      "sz=XX.shape\n",
      "data=np.c_[XX.ravel(),YY.ravel()]\n",
      "i=1\n",
      "yhat_d=np.empty((data.shape[0],3))\n",
      "for c in clf_list:\n",
      "    yhat=c.predict(data)\n",
      "    #Visualization of each boundary\n",
      "    yhat_d[:,i-1]=c.decision_function(data)\n",
      "    mn = np.min(yhat)\n",
      "    mx = np.max(yhat)\n",
      "    plt.subplot(1,3,i)\n",
      "    plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='b',label='class 1')\n",
      "    plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='g',label='class 2')\n",
      "    plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='r',label='class 3')\n",
      "    plt.imshow(yhat.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=mn, vmax=mx)\n",
      "    plt.contour(XX,YY,yhat.reshape(sz),[0])\n",
      "    i=i+1\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(16,9)\n",
      "\n",
      "y_final=np.argmax(yhat_d,axis=1)\n",
      "plt.figure()\n",
      "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='b',label='class 1')\n",
      "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='g',label='class 2')\n",
      "plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='r',label='class 3')\n",
      "plt.imshow(y_final.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=2)\n",
      "plt.contour(XX,YY,y_final.reshape(sz),[0, 1])\n",
      "plt.title(\"Final decision boundary\")\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(16,8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3.1 Error correcting output coding\n",
      "\n",
      "Error correcting output coding is a generalization of the methods shown before. In the most general case each class is assigned a ternary code $c_i \\in \\{+1,0,-1\\}^l$ with length $l$. This step is called **coding**. In testing a new sample will be given a test code and this will be compared according to some distance to the class codewords. The class with the closest codeword will be selected as the predicted class. This step is called **decoding**.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3.1.1 Understanding the coding step\n",
      "If we arrange the codewords as rows in a matrix we obtain the coding matrix $M \\in \\{+1,0,-1\\}^{K\\times l}$. Consider the following example with four classes and code length $l=3$:\n",
      "\n",
      "<table>\n",
      "  <tr>\n",
      "    <th></th>\n",
      "    <th>$h_1$</th>\n",
      "    <th>$h_2$</th>\n",
      "    <th>$h_3$</th>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <th>$y_1$</th>\n",
      "    <th>$1$</th>\n",
      "    <th>$1$</th>\n",
      "    <th>$1$</th>\n",
      "  </tr>\n",
      "    <tr>\n",
      "    <th>$y_2$</th>\n",
      "    <th>$1$</th>\n",
      "    <th>$-1$</th>\n",
      "    <th>$0$</th>\n",
      "  </tr>\n",
      "    <tr>\n",
      "    <th>$y_3$</th>\n",
      "    <th>$-1$</th>\n",
      "    <th>$0$</th>\n",
      "    <th>$1$</th>\n",
      "  </tr>\n",
      "    <tr>\n",
      "    <th>$y_4$</th>\n",
      "    <th>$-1$</th>\n",
      "    <th>$0$</th>\n",
      "    <th>$-1$</th>\n",
      "  </tr>\n",
      "</table>\n",
      "\n",
      "The first class, $y_1$ is coded as $(1,1,1)$, the second $y_2$ is coded as $(1,-1,0)$, and so on.\n",
      "\n",
      "Note that the columns of the matrix define a binary problem involving all the classes in the following way: in same column, all classes with code $+1$ belongs to the same meta-class, all classes with code $-1$ to the other meta-class, and all classes with code $0$ are not considered in that particular problem. In our example, the first column defines a binary problem involving the discrimination of all samples from classes $y_1,y_2$ (coded as $+1$) against all the samples of $y_3,y_4$ (coded as $-1$). The second column only considers the samples of class $y_1$ against the samples of class $y_2$. Note that all the zero coded classes are not considered. \n",
      "\n",
      "Given a coding matrix a classifier is trained for each column according to the column defined binary problem. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-success\">**EXERCISE:** Which are the coding matrix of one-vs-one and one-vs-all?\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3.1.2 Understanding the decoding step\n",
      "\n",
      "Given a set of classifiers trained according to the problems defined by the columns of the coding matrix, in the prediction step all the classifiers are applyed to the testing sample. As a result a binary code $t$ is obtained. This coded is compared to all the class codes according to some decoding/distance metric. The most common ones are:\n",
      "\n",
      "+ Hamming decoding/$\\ell_1$-decoding\n",
      "$$d(a,b) = \\frac{1}{2}\\sum\\limits_{i=1}^l |a_i-b_i|$$\n",
      "\n",
      "+ Euclidean decoding\n",
      "$$d(a,b) = \\sqrt{\\sum\\limits_{i=1}^l (a_i-b_i)^2}$$\n",
      "\n",
      "For example, consider that $t=(-1,-1,-1)$. Note that there is no exact code in the coding matrix, thus we have to check for the closest one. If we apply Hamming decoding we obtain\n",
      "\n",
      "<table>\n",
      "  <tr>\n",
      "    <th></th>\n",
      "    <th>Hamming</th>\n",
      "    <th>Euclidean</th>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <th>$y_1$</th>\n",
      "    <th>$3$</th>\n",
      "    <th>$\\sqrt{12}$</th>\n",
      "  </tr>\n",
      "    <tr>\n",
      "    <th>$y_2$</th>\n",
      "    <th>$\\frac{3}{2}$</th>\n",
      "    <th>$\\sqrt{5}$</th>\n",
      "  </tr>\n",
      "    <tr>\n",
      "    <th>$y_3$</th>\n",
      "    <th>$\\frac{3}{2}$</th>\n",
      "    <th>$\\sqrt{5}$</th>\n",
      "  </tr>\n",
      "    <tr>\n",
      "    <th>$y_4$</th>\n",
      "    <th>$\\frac{1}{2}$</th>\n",
      "    <th>$1$</th>\n",
      "  </tr>\n",
      "</table>\n",
      "\n",
      "Observe that in both cases the sample will be predicted as class $y_4$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us apply this framework to the former problem using a one-vs-all approach:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reset\n",
      "#Create a multiclass toy problem\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn import svm\n",
      "MAXN=5\n",
      "np.random.seed(0)\n",
      "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.25*np.random.randn(MAXN,2)]) \n",
      "X = np.concatenate([X,[8,-2]+1.25*np.random.randn(MAXN,2)])\n",
      "y = np.concatenate([0*np.ones((MAXN,1)),1*np.ones((MAXN,1))])\n",
      "y = np.concatenate([y,2*np.ones((MAXN,1))])\n",
      "x = np.linspace(-5,15,200)\n",
      "XX,YY = np.meshgrid(x,x)\n",
      "sz=XX.shape\n",
      "data=np.c_[XX.ravel(),YY.ravel()]\n",
      "\n",
      "#Define the coding matrix\n",
      "M = np.array([[1, -1, -1],[-1, 1, -1],[-1, -1, 1]]) #1vsAll\n",
      "#M = np.array([[1, 1, 0 ],[-1, 0, 1],[0, -1, -1]]) #1vs1\n",
      "\n",
      "print 'Coding matrix M = \\n' + str(M)\n",
      "def inset(a,b): \n",
      "    return [item in b for item in a]\n",
      "\n",
      "def fit_ECOC(X, y, M):\n",
      "    clf_list=[]\n",
      "    for i in xrange(M.shape[1]): #For each column\n",
      "        y_meta=y.copy()\n",
      "        idx_c1 = np.where(inset(y, np.where(M[:,i]==1)[0]))[0]\n",
      "        idx_c2 = np.where(inset(y, np.where(M[:,i]==-1)[0]))[0]\n",
      "        clf = svm.LinearSVC()\n",
      "        clf_list.append(clf.fit(np.r_['0',X[idx_c1,:],X[idx_c2,:]],np.r_['0',np.ones((idx_c1.shape[0],1)),-np.ones((idx_c2.shape[0],1))].ravel()))\n",
      "    return clf_list\n",
      "\n",
      "def predict_ECOC(X,M, clf_list):\n",
      "    #Test codes\n",
      "    c = np.zeros((X.shape[0],M.shape[1]))\n",
      "    for i in xrange(M.shape[1]):\n",
      "        c[:,i]=clf_list[i].decision_function(X) #SOFT CODES\n",
      "        #c[:,i]=clf_list[i].predict(X) #HARD CODES \n",
      "    #Use Euclidean distance\n",
      "    i=0\n",
      "    d = np.zeros((X.shape[0],M.shape[0]))\n",
      "    for code in M:\n",
      "        d[:,i]=np.sum(np.power((c-code),2),axis=1)\n",
      "        i=i+1\n",
      "    return np.argmin(d,axis=1)    \n",
      "\n",
      "\n",
      "clf_list=fit_ECOC(X,y,M)\n",
      "y_final = predict_ECOC(data,M,clf_list)\n",
      "\n",
      "plt.scatter(X[(y==0).ravel(),0],X[(y==0).ravel(),1],color='b',label='class 1')\n",
      "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='g',label='class 2')\n",
      "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='r',label='class 3')\n",
      "plt.imshow(y_final.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=2)\n",
      "plt.contour(XX,YY,y_final.reshape(sz),[0, 1])\n",
      "plt.title(\"Final decision boundary\")\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(16,8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-success\">\n",
      "**EXERCISE:** Replace the hard codes prediction by the soft coding by changing in the ECOC_predict() function the \".predict\" by \".decision_function\". Run the algorithm with one-against-all approach. What do you observe?\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-success\">\n",
      "**EXERCISE:** Replace the coding matrix by one-vs-one.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M = np.array([[1, 1, 0 ],[-1, 0, 1],[0, -1, -1]])\n",
      "clf_list=fit_ECOC(X,y,M)\n",
      "y_final2 = predict_ECOC(data,M,clf_list)\n",
      "\n",
      "plt.scatter(X[(y==0).ravel(),0],X[(y==0).ravel(),1],color='b',label='class 1')\n",
      "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='g',label='class 2')\n",
      "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='r',label='class 3')\n",
      "plt.imshow(y_final2.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=2)\n",
      "\n",
      "plt.imshow(y_final.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=2)\n",
      "plt.contour(XX,YY,y_final2.reshape(sz),[0, 1])\n",
      "plt.title(\"Final decision boundary\")\n",
      "fig = plt.gcf()\n",
      "fig.set_size_inches(16,8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}